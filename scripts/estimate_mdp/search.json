[
  {
    "objectID": "estimate_mdp.html",
    "href": "estimate_mdp.html",
    "title": "Estimating Markov Decision Process Parameters",
    "section": "",
    "text": "import sys\nsys.path.insert(0, '..')\nsys.path.insert(0, '../../src')\n\nimport json\nfrom pathlib import Path\nimport numpy as np\nimport torch\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Import MDP modules\nfrom mdp_estimator import EstimateMDP, EstimateCCP\nfrom mdp_solver import MonotonicNetwork, ComputeChoiceProbability, GenerateStateGrid, SolveValueIteration\nfrom config_mdp import get_estimator_config\n\n# Load simulation results and configuration\nsim_dir = Path('../../output/simulate_mdp')\nif not sim_dir.exists():\n    raise FileNotFoundError(\n        f\"Expected simulation results in {sim_dir}. Run simulate_mdp.qmd first.\"\n    )\n\n# Load configuration from simulator output (as data from previous step)\nwith open(sim_dir / 'config.json', 'r', encoding='utf-8') as f:\n    config = json.load(f)\n\nmodel_config = config['model']\nsimulator_config = config['simulator']\n\n# Extract model parameters (true values for comparison)\nbeta_true = model_config['beta']\ngamma_true = model_config['gamma']\ndelta = model_config['delta']\ngamma_E = model_config['gamma_E']\n\n# Extract simulator parameters\nM = simulator_config['M']\nT = simulator_config['T']\n\n# Load simulated data\nsim_data = np.load(sim_dir / 'simulation_results.npz')\nstates = sim_data['states']\nactions = sim_data['actions']\nrewards = sim_data['rewards']\nstate_range = tuple(sim_data['state_range'])\n\nprint(f\"Loaded simulation data: M={M}, T={T}, delta={delta}\")\nprint(f\"True parameters: beta={beta_true}, gamma={gamma_true}\")\n\nLoaded simulation data: M=1000, T=100, delta=0.95\nTrue parameters: beta=1.0, gamma=0.1"
  },
  {
    "objectID": "estimate_mdp.html#overview",
    "href": "estimate_mdp.html#overview",
    "title": "Estimating Markov Decision Process Parameters",
    "section": "",
    "text": "This document estimates the structural parameters of the Markov Decision Process from simulated data. We observe state-action pairs \\((s_t^{(m)}, a_t^{(m)})\\) for \\(m = 1, \\ldots, M\\) paths over \\(t = 1, \\ldots, T\\) periods, and estimate the parameters \\(\\beta\\) and \\(\\gamma\\) while treating the discount factor \\(\\delta\\) as known."
  },
  {
    "objectID": "estimate_mdp.html#model-specification",
    "href": "estimate_mdp.html#model-specification",
    "title": "Estimating Markov Decision Process Parameters",
    "section": "Model Specification",
    "text": "Model Specification\nThe MDP model has the following structure (from solve_mdp.qmd):\nMean Reward Function: \\[\\bar{r}(s, a) = \\beta \\log(1 + s) - a\\]\nState Transition: \\[s_{t+1} = (1 - \\gamma) s_t + a_t\\]\nParameters to Estimate:\n\n\\(\\beta\\): Weight on state value in the reward function\n\\(\\gamma\\): State depreciation rate in the transition function\n\nKnown Parameter:\n\n\\(\\delta\\): Discount factor (assumed known)"
  },
  {
    "objectID": "estimate_mdp.html#load-simulation-data",
    "href": "estimate_mdp.html#load-simulation-data",
    "title": "Estimating Markov Decision Process Parameters",
    "section": "Load Simulation Data",
    "text": "Load Simulation Data\n\nimport sys\nsys.path.insert(0, '..')\nsys.path.insert(0, '../../src')\n\nimport json\nfrom pathlib import Path\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom config_mdp import get_solver_config\n\n# Load simulation results\nsim_dir = Path('../../output/simulate_mdp')\nif not sim_dir.exists():\n    raise FileNotFoundError(\n        f\"Expected simulation results in {sim_dir}. Run simulate_mdp.qmd first.\"\n    )\n\n# Load configuration\nwith open(sim_dir / 'simulation_config.json', 'r', encoding='utf-8') as f:\n    sim_config = json.load(f)\n\n# Load simulated data\nsim_data = np.load(sim_dir / 'simulation_results.npz')\nstates = sim_data['states']  # Shape: (M, T)\nactions = sim_data['actions']  # Shape: (M, T)\nrewards = sim_data['rewards']  # Shape: (M, T)\n\nM = sim_config['M']\nT = sim_config['T']\ndelta = sim_config['delta']  # Known discount factor\nstate_range = tuple(sim_config['state_range'])\n\n# True parameter values (for comparison)\nbeta_true = sim_config['beta']\ngamma_true = sim_config['gamma']\n\nprint(f\"Loaded simulation data:\")\nprint(f\"  Number of paths (M): {M}\")\nprint(f\"  Time periods (T): {T}\")\nprint(f\"  Total observations: {M * T:,}\")\nprint(f\"  State range: {state_range}\")\nprint(f\"\\nKnown parameter:\")\nprint(f\"  delta (discount factor): {delta}\")\nprint(f\"\\nTrue parameter values (for comparison):\")\nprint(f\"  beta: {beta_true}\")\nprint(f\"  gamma: {gamma_true}\")\n\nLoaded simulation data:\n  Number of paths (M): 100\n  Time periods (T): 100\n  Total observations: 10,000\n  State range: (0.0, 10.0)\n\nKnown parameter:\n  delta (discount factor): 0.95\n\nTrue parameter values (for comparison):\n  beta: 1.0\n  gamma: 0.1"
  },
  {
    "objectID": "estimate_mdp.html#data-summary",
    "href": "estimate_mdp.html#data-summary",
    "title": "Estimating Markov Decision Process Parameters",
    "section": "Data Summary",
    "text": "Data Summary\n\n# Summary statistics for states and actions\nprint(\"\\nState Statistics:\")\nprint(f\"  Min: {states.min():.3f}\")\nprint(f\"  Max: {states.max():.3f}\")\nprint(f\"  Mean: {states.mean():.3f}\")\nprint(f\"  Std: {states.std():.3f}\")\n\nprint(\"\\nAction Statistics:\")\nprint(f\"  Fraction a=0: {(actions == 0).mean():.3f}\")\nprint(f\"  Fraction a=1: {(actions == 1).mean():.3f}\")\n\nprint(\"\\nReward Statistics:\")\nprint(f\"  Min: {rewards.min():.3f}\")\nprint(f\"  Max: {rewards.max():.3f}\")\nprint(f\"  Mean: {rewards.mean():.3f}\")\nprint(f\"  Std: {rewards.std():.3f}\")\n\n# Visualize data distribution\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\n# State distribution\nax = axes[0]\nax.hist(states.flatten(), bins=50, alpha=0.7, edgecolor='black')\nax.set_xlabel('State (s)')\nax.set_ylabel('Frequency')\nax.set_title('Distribution of States')\nax.grid(True, alpha=0.3)\n\n# Action distribution\nax = axes[1]\naction_counts = [(actions == 0).sum(), (actions == 1).sum()]\nax.bar([0, 1], action_counts, alpha=0.7, edgecolor='black')\nax.set_xlabel('Action (a)')\nax.set_ylabel('Frequency')\nax.set_title('Distribution of Actions')\nax.set_xticks([0, 1])\nax.grid(True, alpha=0.3, axis='y')\n\n# Reward distribution\nax = axes[2]\nax.hist(rewards.flatten(), bins=50, alpha=0.7, edgecolor='black')\nax.set_xlabel('Reward (r)')\nax.set_ylabel('Frequency')\nax.set_title('Distribution of Rewards')\nax.grid(True, alpha=0.3)\n\nfig.tight_layout()\nplt.show()\n\n\nState Statistics:\n  Min: 0.076\n  Max: 9.957\n  Mean: 5.136\n  Std: 1.097\n\nAction Statistics:\n  Fraction a=0: 0.484\n  Fraction a=1: 0.516\n\nReward Statistics:\n  Min: -0.927\n  Max: 2.394\n  Mean: 1.281\n  Std: 0.549"
  },
  {
    "objectID": "estimate_mdp.html#estimation-setup",
    "href": "estimate_mdp.html#estimation-setup",
    "title": "Estimating Markov Decision Process Parameters",
    "section": "Estimation Setup",
    "text": "Estimation Setup\nThe goal is to estimate \\(\\theta = (\\beta, \\gamma)\\) from the observed data \\(\\{(s_t^{(m)}, a_t^{(m)}, s_{t+1}^{(m)})\\}_{m=1,t=1}^{M,T-1}\\).\nKey observations:\n\nState transition provides direct information about \\(\\gamma\\): From \\(s_{t+1} = (1 - \\gamma) s_t + a_t\\), we can write: \\[\\gamma = 1 - \\frac{s_{t+1} - a_t}{s_t}\\]\nReward function provides information about \\(\\beta\\): From \\(\\bar{r}(s, a) = \\beta \\log(1 + s) - a\\), the expected reward conditional on state and action depends on \\(\\beta\\).\nJoint estimation via likelihood: The choice probabilities depend on the value functions, which depend on both \\(\\beta\\) and \\(\\gamma\\), allowing joint estimation through the likelihood of observed actions.\n\nEstimation approach: We will use the structure of the model to estimate the parameters. Details of the estimation procedure will be added in subsequent sections."
  },
  {
    "objectID": "estimate_mdp.html#load-configuration-and-simulation-results",
    "href": "estimate_mdp.html#load-configuration-and-simulation-results",
    "title": "Estimating Markov Decision Process Parameters",
    "section": "",
    "text": "import sys\nsys.path.insert(0, '..')\nsys.path.insert(0, '../../src')\n\nimport json\nfrom pathlib import Path\nimport numpy as np\nimport torch\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Import MDP modules\nfrom mdp_estimator import EstimateMDP, EstimateCCP\nfrom mdp_solver import MonotonicNetwork, ComputeChoiceProbability, GenerateStateGrid, SolveValueIteration\nfrom config_mdp import get_estimator_config\n\n# Load simulation results and configuration\nsim_dir = Path('../../output/simulate_mdp')\nif not sim_dir.exists():\n    raise FileNotFoundError(\n        f\"Expected simulation results in {sim_dir}. Run simulate_mdp.qmd first.\"\n    )\n\n# Load configuration from simulator output (as data from previous step)\nwith open(sim_dir / 'config.json', 'r', encoding='utf-8') as f:\n    config = json.load(f)\n\nmodel_config = config['model']\nsimulator_config = config['simulator']\n\n# Extract model parameters (true values for comparison)\nbeta_true = model_config['beta']\ngamma_true = model_config['gamma']\ndelta = model_config['delta']\ngamma_E = model_config['gamma_E']\n\n# Extract simulator parameters\nM = simulator_config['M']\nT = simulator_config['T']\n\n# Load simulated data\nsim_data = np.load(sim_dir / 'simulation_results.npz')\nstates = sim_data['states']\nactions = sim_data['actions']\nrewards = sim_data['rewards']\nstate_range = tuple(sim_data['state_range'])\n\nprint(f\"Loaded simulation data: M={M}, T={T}, delta={delta}\")\nprint(f\"True parameters: beta={beta_true}, gamma={gamma_true}\")\n\nLoaded simulation data: M=1000, T=100, delta=0.95\nTrue parameters: beta=1.0, gamma=0.1"
  },
  {
    "objectID": "estimate_mdp.html#estimation-strategy",
    "href": "estimate_mdp.html#estimation-strategy",
    "title": "Estimating Markov Decision Process Parameters",
    "section": "Estimation Strategy",
    "text": "Estimation Strategy\nWe estimate the structural parameters \\(\\beta\\) and \\(\\gamma\\) using a two-step approach. The key insight is that in real data, we typically observe only states and actions - rewards are not observed.\nObservable data: \\(\\{(s_t^{(m)}, a_t^{(m)})\\}_{m=1,t=0}^{M,T-1}\\)\nKnown parameter: \\(\\delta\\) (discount factor)\nParameters to estimate: \\(\\beta\\) (reward weight), \\(\\gamma\\) (depreciation rate)\n\nStep 1: Estimate State Depreciation Parameter \\(\\gamma\\)\nThe state transition is deterministic: \\[s_{t+1} = (1 - \\gamma) s_t + a_t\\]\nRearranging: \\[\\gamma = 1 - \\frac{s_{t+1} - a_t}{s_t}\\]\nWe can directly estimate \\(\\gamma\\) from observed state transitions and actions using ordinary least squares or method of moments.\n\n\nStep 2: Estimate Reward Parameter \\(\\beta\\) via Revealed Preference\nThe reward function is: \\[\\bar{r}(s, a) = \\beta \\log(1 + s) - a\\]\nSince rewards are not observed, we use a revealed preference approach based on the optimality of observed actions. This follows an actor-critic style method:\n\nStep 2a: Estimate Conditional Choice Probability (CCP)\nEstimate the choice probability as a function of state:\n\\[\\hat{P}(a=1|s) = \\sigma(g_\\theta(s))\\]\nwhere \\(g_\\theta\\) is a standard neural network: - Continuous in \\(s\\) - No monotonicity constraints (unconstrained weights) - Flexibly fits the observed choice pattern from data - Output layer applies sigmoid: \\(\\hat{P}(a=1|s) = \\sigma(g_\\theta(s))\\), output \\(\\in [0,1]\\)\nThis is estimated by maximum likelihood using the observed state-action pairs.\n\n\nStep 2b: Solve for Value Functions Given CCP and Candidate \\(\\beta\\)\nFor a candidate value of \\(\\beta\\), solve for value functions \\(v^{(0)}(s)\\) and \\(v^{(1)}(s)\\) that are consistent with the estimated CCP \\(\\hat{P}(a|s)\\) from Step 2a.\nThe value functions satisfy the Bellman equation under the estimated policy: \\[v^{(a)}(s_i) = \\bar{r}(s_i, a) + \\delta \\left[(1 - \\hat{P}(a=1|s'_i)) v^{(0)}(s'_i) + \\hat{P}(a=1|s'_i) v^{(1)}(s'_i)\\right] + \\delta \\gamma_E\\]\nwhere \\(s'_i = (1-\\hat{\\gamma})s_i + a\\) using the estimated \\(\\hat{\\gamma}\\) from Step 1.\nEfficient Solution via Linear System:\nSince we evaluate on a discrete state grid \\(\\{s_1, ..., s_N\\}\\), the Bellman equation forms a linear system in the value functions. We can solve it directly:\n\nSolve linear system: For each action \\(a \\in \\{0,1\\}\\), solve \\((I - \\delta T^{(a)}) v^{(a)} = r^{(a)}\\) where \\(T^{(a)}\\) is the transition matrix under the estimated policy\nFit neural networks: Train networks \\(v_\\theta^{(0)}, v_\\theta^{(1)}\\) to approximate the solved grid values via supervised learning\n\nThis is much faster than iterative value function training, as it requires only one linear solve and one supervised learning step per \\(\\beta\\) candidate.\n\n\nStep 2c: Compute Updated CCP from Value Functions\nFrom the trained value functions, compute the implied choice probabilities using the logit formula: \\[P^{\\text{updated}}(a=1|s) = \\frac{\\exp(v^{(1)}(s))}{\\exp(v^{(0)}(s)) + \\exp(v^{(1)}(s))}\\]\n\n\nStep 2d: Find \\(\\beta\\) that Minimizes Distance\nFind the value of \\(\\beta\\) that minimizes the distance between the estimated CCP network and the updated CCP: \\[\\hat{\\beta} = \\arg\\min_\\beta \\sum_{s \\in \\mathcal{S}} \\left[\\hat{P}(a=1|s) - P^{\\text{updated}}(a=1|s; \\beta)\\right]^2\\]\nwhere \\(\\mathcal{S}\\) is a grid of state values spanning the observed state range.\nThis is a nested fixed-point problem: - Outer loop: Search over candidate values of \\(\\beta\\) in parallel - Inner loop: For each \\(\\beta\\), solve for value functions consistent with estimated CCP, then compute updated CCP\nThe optimal \\(\\beta\\) is the one where the estimated policy (from data) is consistent with the optimal policy (from the model).\nParallelization: Since the evaluation of each \\(\\beta\\) candidate is independent, we can parallelize the search across multiple workers, significantly reducing computation time."
  },
  {
    "objectID": "estimate_mdp.html#pseudo-code",
    "href": "estimate_mdp.html#pseudo-code",
    "title": "Estimating Markov Decision Process Parameters",
    "section": "Pseudo Code",
    "text": "Pseudo Code\n\n\n\n\n\n\nAlgorithm: Two-Step Estimation of MDP Parameters\n\n\n\nDesign Principles:\nThis pseudo code is designed for direct implementation:\n\nNo placeholders: Every operation corresponds to a concrete function or method call\nExplicit parameters: All inputs are passed as parameters, no implicit/global state\nReuses shared functions: Uses ComputeNextState, ComputeMeanReward, ComputeExpectedValue, ComputeChoiceProbability from solve_mdp\nNo hard-coding: All values are parameterized\n\nInput:\n\nObserved data: states: Array[\\(M \\times T\\)], actions: Array[\\(M \\times T\\)]\nKnown parameter: \\(\\delta\\): float (discount factor)\nState grid for evaluation: \\(N\\): int, state_range: tuple[float, float]\nNetwork hyperparameters: hyperparameters: dict (containing hidden_sizes: list[int])\nTraining parameters: num_epochs: int, learning_rate: float\nSearch grid for \\(\\beta\\): beta_grid: Array[\\(K\\)] (candidate values of \\(\\beta\\))\nTolerance: \\(\\epsilon_{\\text{tol}}\\): float, max_iter: int\nOther constants: \\(\\gamma_E\\): float (Euler’s constant)\n\nOutput:\n\nEstimated parameters: \\(\\hat{\\gamma}\\): float, \\(\\hat{\\beta}\\): float\nBeta search grid: beta_grid: Array[\\(K\\)]\nDistances for each beta candidate: distances: Array[\\(K\\)]\n\n\n\nMain Algorithm\nProcedure EstimateMDP(states: Array[\\(M \\times T\\)], actions: Array[\\(M \\times T\\)], \\(\\delta\\): float, \\(N\\): int, state_range: tuple[float, float], hyperparameters: dict, num_epochs: int, learning_rate: float, beta_grid: Array[\\(K\\)], \\(\\epsilon_{\\text{tol}}\\): float, max_iter: int, \\(\\gamma_E\\): float) \\(\\to\\) (float, float, Array[\\(K\\)], Array[\\(K\\)])\n\n\\(\\hat{\\gamma}\\): float \\(\\leftarrow\\) EstimateGamma(states, actions)\n\\(\\hat{\\beta}\\): float, distances: Array[\\(K\\)] \\(\\leftarrow\\) EstimateBeta(states, actions, \\(\\hat{\\gamma}\\), \\(\\delta\\), \\(N\\), state_range, hyperparameters, num_epochs, learning_rate, beta_grid, \\(\\epsilon_{\\text{tol}}\\), max_iter, \\(\\gamma_E\\))\nReturn \\(\\hat{\\gamma}\\), \\(\\hat{\\beta}\\), beta_grid, distances\n\n\n\n\nStep 1: Estimate Gamma\nProcedure EstimateGamma(states: Array[\\(M \\times T\\)], actions: Array[\\(M \\times T\\)]) \\(\\to\\) float\n\ngamma_estimates: Array \\(\\leftarrow\\) Empty array\nFor \\(m\\): int = 1 to \\(M\\):\n\nFor \\(t\\): int = 1 to \\(T-1\\):\n\n    i. \\(s_t\\): float \\(\\leftarrow\\) states[\\(m\\), \\(t\\)]\n    ii. \\(s_{t+1}\\): float \\(\\leftarrow\\) states[\\(m\\), \\(t+1\\)]\n    iii. \\(a_t\\): int \\(\\leftarrow\\) actions[\\(m\\), \\(t\\)]\n    iv. If \\(s_t \\neq 0\\):\n        \\(\\gamma_{mt}\\): float \\(\\leftarrow 1 - \\frac{s_{t+1} - a_t}{s_t}\\)\n        Append \\(\\gamma_{mt}\\) to gamma_estimates\n\\(\\hat{\\gamma}\\): float \\(\\leftarrow\\) Mean(gamma_estimates)\nReturn \\(\\hat{\\gamma}\\)\n\n\n\n\nStep 2: Estimate Beta via Revealed Preference\nProcedure EstimateBeta(states: Array[\\(M \\times T\\)], actions: Array[\\(M \\times T\\)], \\(\\hat{\\gamma}\\): float, \\(\\delta\\): float, \\(N\\): int, state_range: tuple[float, float], hyperparameters: dict, num_epochs: int, learning_rate: float, beta_grid: Array[\\(K\\)], \\(\\epsilon_{\\text{tol}}\\): float, max_iter: int, \\(\\gamma_E\\): float) \\(\\to\\) (float, Array[\\(K\\)])\n\n\\(\\hat{P}\\): Network \\(\\leftarrow\\) EstimateCCP(states, actions, hyperparameters, num_epochs, learning_rate)     // Returns network that predicts \\(P(a=1|s)\\)\n\\(S\\): Tensor[\\(N \\times 1\\)] \\(\\leftarrow\\) GenerateStateGrid(\\(N\\), state_range)\n\\(\\hat{P}_{\\text{eval}}\\): Tensor[\\(N \\times 1\\)] \\(\\leftarrow \\hat{P}(S)\\)     // Pre-compute \\(P(a=1|s)\\) on grid once\ndistances: Array[\\(K\\)] \\(\\leftarrow\\) Empty array\nParallelFor \\(k\\): int = 1 to \\(K\\) do in parallel:     // Evaluate beta candidates in parallel\n\n\\(\\beta_k\\): float \\(\\leftarrow\\) beta_grid[\\(k\\)]\ndistance: float \\(\\leftarrow\\) EvaluateBetaCandidate(\\(\\beta_k\\), \\(\\hat{P}\\), \\(\\hat{P}_{\\text{eval}}\\), \\(\\hat{\\gamma}\\), \\(\\delta\\), \\(\\gamma_E\\), hyperparameters, \\(S\\), max_iter, \\(\\epsilon_{\\text{tol}}\\), num_epochs, learning_rate)\ndistances[\\(k\\)] \\(\\leftarrow\\) distance\n\n\\(k^*\\): int \\(\\leftarrow\\) ArgMin(distances)\n\\(\\hat{\\beta}\\): float \\(\\leftarrow\\) beta_grid[\\(k^*\\)]\nReturn \\(\\hat{\\beta}\\), distances\n\n\n\n\nSubroutine for Parallel Beta Evaluation\nProcedure EvaluateBetaCandidate(\\(\\beta_k\\): float, \\(\\hat{P}\\): Network, \\(\\hat{P}_{\\text{eval}}\\): Tensor[\\(N \\times 1\\)], \\(\\gamma\\): float, \\(\\delta\\): float, \\(\\gamma_E\\): float, hyperparameters: dict, \\(S\\): Tensor[\\(N \\times 1\\)], max_iter: int, \\(\\epsilon_{\\text{tol}}\\): float, num_epochs: int, learning_rate: float) \\(\\to\\) float\n\n\\(v_\\theta^{(0)}\\), \\(v_\\theta^{(1)}\\) \\(\\leftarrow\\) SolveValueFunctionGivenCCP(\\(\\hat{P}\\), \\(\\beta_k\\), \\(\\gamma\\), \\(\\delta\\), \\(\\gamma_E\\), hyperparameters, \\(S\\), max_iter, \\(\\epsilon_{\\text{tol}}\\), num_epochs, learning_rate)\n\\(P^{\\text{updated}}\\): Tensor[\\(N \\times 1\\)] \\(\\leftarrow\\) ComputeCCPFromValue(\\(S\\), \\(v_\\theta^{(0)}\\), \\(v_\\theta^{(1)}\\))     // Returns \\(P(a=1|s)\\)\ndistance: float \\(\\leftarrow\\) ComputeDistance(\\(\\hat{P}_{\\text{eval}}\\), \\(P^{\\text{updated}}\\))     // Both are \\(P(a=1|s)\\)\nReturn distance\n\n\n\n\nSubroutines for Step 2a: Estimate CCP\nProcedure EstimateCCP(states: Array[\\(M \\times T\\)], actions: Array[\\(M \\times T\\)], hyperparameters: dict, num_epochs: int, learning_rate: float) \\(\\to\\) Network\n\n\\(\\hat{P}\\): Network \\(\\leftarrow\\) InitializeCCPNetwork(hyperparameters)     // Standard network: unconstrained weights, sigmoid output \\(\\in [0,1]\\)\noptimizer: Optimizer \\(\\leftarrow\\) CreateOptimizer(\\(\\hat{P}\\), learning_rate)\nFor epoch: int = 1 to num_epochs:\n\noptimizer.zero_grad()\nFor \\(m\\): int = 1 to \\(M\\):\n\n    i. For \\(t\\): int = 1 to \\(T\\):\n        \\(s_{mt}\\): Tensor \\(\\leftarrow\\) states[\\(m\\), \\(t\\)]\n        \\(a_{mt}\\): int \\(\\leftarrow\\) actions[\\(m\\), \\(t\\)]\n        \\(\\hat{p}_{mt}\\): Tensor \\(\\leftarrow \\hat{P}(s_{mt})\\)     // Probability of \\(a=1\\)\n        loss: Tensor \\(\\leftarrow\\) ComputeBinaryCrossEntropy(\\(a_{mt}\\), \\(\\hat{p}_{mt}\\))\n\nloss.backward()\noptimizer.step()\n\nReturn \\(\\hat{P}\\)     // Returns network that predicts \\(P(a=1|s)\\)\n\n\n\n\nSubroutines for Step 2b: Solve Value Function Given CCP\nProcedure SolveValueFunctionGivenCCP(\\(\\hat{P}\\): Network, \\(\\beta\\): float, \\(\\gamma\\): float, \\(\\delta\\): float, \\(\\gamma_E\\): float, hyperparameters: dict, \\(S\\): Tensor[\\(N \\times 1\\)], num_epochs: int, learning_rate: float) \\(\\to\\) (Network, Network)\n\n\\(v^{(0)}\\): Array[\\(N\\)], \\(v^{(1)}\\): Array[\\(N\\)] \\(\\leftarrow\\) SolveLinearBellman(\\(\\hat{P}\\), \\(\\beta\\), \\(\\gamma\\), \\(\\delta\\), \\(\\gamma_E\\), \\(S\\))\n\\(v_\\theta^{(0)}\\), \\(v_\\theta^{(1)}\\) \\(\\leftarrow\\) FitNetworksToValues(\\(S\\), \\(v^{(0)}\\), \\(v^{(1)}\\), hyperparameters, num_epochs, learning_rate)\nReturn \\(v_\\theta^{(0)}\\), \\(v_\\theta^{(1)}\\)\n\n\nProcedure SolveLinearBellman(\\(\\hat{P}\\): Network, \\(\\beta\\): float, \\(\\gamma\\): float, \\(\\delta\\): float, \\(\\gamma_E\\): float, \\(S\\): Tensor[\\(N \\times 1\\)]) \\(\\to\\) (Array[\\(N\\)], Array[\\(N\\)])\n\n\\(N\\): int \\(\\leftarrow\\) Length(\\(S\\))\nFor each action \\(a\\): int \\(\\in \\{0, 1\\}\\):\n\n\\(r^{(a)}\\): Array[\\(N\\)] \\(\\leftarrow\\) Empty array\n\\(T^{(a)}\\): Matrix[\\(N \\times N\\)] \\(\\leftarrow\\) Zero matrix\nFor \\(i\\): int = 1 to \\(N\\):\n\n    i. \\(s_i\\): float \\(\\leftarrow S[i]\\)\n    ii. \\(r^{(a)}[i]\\) \\(\\leftarrow\\) ComputeMeanReward(\\(s_i\\), \\(a\\), \\(\\beta\\))\n    iii. \\(s'_i\\): float \\(\\leftarrow\\) ComputeNextState(\\(s_i\\), \\(a\\), \\(\\gamma\\))\n    iv. \\(\\hat{p}_i\\): float \\(\\leftarrow \\hat{P}(s'_i)\\)     // \\(P(a=1|s'_i)\\) from network\n    v. For \\(j\\): int = 1 to \\(N\\):     // Find which grid point \\(s'_i\\) maps to\n        If \\(s'_i \\approx S[j]\\):     // Interpolate or find nearest neighbor\n            \\(T^{(a)}[i, j]\\) \\(\\leftarrow (1 - \\hat{p}_i)\\)     // Transition to \\(v^{(0)}\\)\n            // Also add contribution to \\(v^{(1)}\\) via second matrix (or handle jointly)\n\nAdd \\(\\delta \\gamma_E\\) to \\(r^{(a)}\\)     // Constant term from Euler constant\n\\(v^{(a)}\\): Array[\\(N\\)] \\(\\leftarrow\\) SolveLinearSystem(\\((I - \\delta T^{(a)})\\), \\(r^{(a)}\\))\n\nReturn \\(v^{(0)}\\), \\(v^{(1)}\\)\n\n\nProcedure FitNetworksToValues(\\(S\\): Tensor[\\(N \\times 1\\)], \\(v^{(0)}\\): Array[\\(N\\)], \\(v^{(1)}\\): Array[\\(N\\)], hyperparameters: dict, num_epochs: int, learning_rate: float) \\(\\to\\) (Network, Network)\n\n\\(v_\\theta^{(0)}\\): Network, \\(v_\\theta^{(1)}\\): Network \\(\\leftarrow\\) InitializeNetworks(hyperparameters)\noptimizer: Optimizer \\(\\leftarrow\\) CreateOptimizer(\\([v_\\theta^{(0)}, v_\\theta^{(1)}]\\), learning_rate)\ntargets: Tensor[\\(N \\times 2\\)] \\(\\leftarrow\\) Stack(\\(v^{(0)}\\), \\(v^{(1)}\\))\n\\(v_\\theta^{(0)}\\), \\(v_\\theta^{(1)}\\) \\(\\leftarrow\\) UpdateNetworks(\\(S\\), targets, \\(v_\\theta^{(0)}\\), \\(v_\\theta^{(1)}\\), num_epochs, optimizer)\nReturn \\(v_\\theta^{(0)}\\), \\(v_\\theta^{(1)}\\)\n\n\n\n\nSubroutines for Step 2c: Compute CCP from Value Functions\nProcedure ComputeCCPFromValue(\\(S\\): Tensor[\\(N \\times 1\\)], \\(v_\\theta^{(0)}\\): Network, \\(v_\\theta^{(1)}\\): Network) \\(\\to\\) Tensor[\\(N \\times 1\\)]\n\n\\(v_0\\): Tensor[\\(N \\times 1\\)] \\(\\leftarrow v_\\theta^{(0)}(S)\\)\n\\(v_1\\): Tensor[\\(N \\times 1\\)] \\(\\leftarrow v_\\theta^{(1)}(S)\\)\n\\(P^{\\text{updated}}\\): Tensor[\\(N \\times 1\\)] \\(\\leftarrow \\frac{\\exp(v_1)}{\\exp(v_0) + \\exp(v_1)}\\)     // Probability of \\(a=1\\)\nReturn \\(P^{\\text{updated}}\\)\n\n\n\n\nSubroutines for Step 2d: Compute Distance\nProcedure ComputeDistance(\\(\\hat{P}_{\\text{eval}}\\): Tensor[\\(N \\times 1\\)], \\(P^{\\text{updated}}\\): Tensor[\\(N \\times 1\\)]) \\(\\to\\) float\n\ndiff: Tensor[\\(N \\times 1\\)] \\(\\leftarrow \\hat{P}_{\\text{eval}} - P^{\\text{updated}}\\)\nsquared_diff: Tensor[\\(N \\times 1\\)] \\(\\leftarrow\\) diff \\(\\odot\\) diff     // Element-wise square\ndistance: float \\(\\leftarrow\\) Sum(squared_diff)\nReturn distance\n\n\n\n\nShared Functions from solve_mdp\nThe following functions are imported from solve_mdp and reused directly:\nProcedure ComputeNextState(\\(s\\): Tensor, \\(a\\): int, \\(\\gamma\\): float) \\(\\to\\) Tensor\n\nReturns \\((1 - \\gamma) \\cdot s + a\\)\nSee solve_mdp documentation for implementation details\n\nProcedure ComputeMeanReward(\\(s\\): Tensor, \\(a\\): int, \\(\\beta\\): float) \\(\\to\\) Tensor\n\nReturns \\(\\beta \\cdot \\log(1 + s) - a\\)\nSee solve_mdp documentation for implementation details\n\nProcedure InitializeNetworks(hyperparameters: dict) \\(\\to\\) (Network, Network)\n\nSee solve_mdp documentation for implementation details\n\nProcedure UpdateNetworks(\\(S\\): Tensor, targets: Tensor, \\(v_\\theta^{(0)}\\): Network, \\(v_\\theta^{(1)}\\): Network, num_epochs: int, optimizer: Optimizer) \\(\\to\\) (Network, Network)\n\nSee solve_mdp documentation for implementation details\n\nProcedure CheckConvergence(\\(S\\): Tensor, targets: Tensor, \\(v_\\theta^{(0)}\\): Network, \\(v_\\theta^{(1)}\\): Network) \\(\\to\\) float\n\nSee solve_mdp documentation for implementation details\n\n\n\n\nKey Implementation Details:\n\nShared configuration: Load \\(\\delta\\), \\(\\gamma_E\\) from the same config used in solve_mdp\nStandard network for CCP: Use a standard neural network with unconstrained weights to flexibly fit \\(P(a=1|s)\\) from observed choices. Output layer applies sigmoid \\(\\sigma\\) to constrain to \\([0,1]\\). Train on action indicator \\(1\\{a=1\\}\\) using binary cross-entropy loss\nNested fixed-point: The outer loop searches over \\(\\beta\\) candidates, inner loop solves value functions for each \\(\\beta\\)\nBinary cross-entropy loss: For CCP estimation, use \\(-[a \\log(\\hat{p}) + (1-a) \\log(1-\\hat{p})]\\) where \\(a = 1\\{a=1\\}\\) and \\(\\hat{p} = \\hat{P}(s)\\)\nGrid search for \\(\\beta\\): Can be replaced with gradient-free optimization methods like Nelder-Mead"
  },
  {
    "objectID": "estimate_mdp.html#run-estimation",
    "href": "estimate_mdp.html#run-estimation",
    "title": "Estimating Markov Decision Process Parameters",
    "section": "Run Estimation",
    "text": "Run Estimation\n\n# Load solver configuration from solver output (for estimation hyperparameters)\nsolver_dir = Path('../../output/solve_mdp')\nwith open(solver_dir / 'config.json', 'r', encoding='utf-8') as f:\n    solver_output = json.load(f)\n\nsolver_config = solver_output['solver']\nhyperparameters = solver_config['hyperparameters']\nN = solver_config['N']\nepsilon_tol = solver_config['epsilon_tol']\nmax_iter = solver_config['max_iter']\nnum_epochs = solver_config['num_epochs']\nlearning_rate = solver_config['learning_rate']\n\n# Load estimator configuration (estimation method parameters)\nestimator_config = get_estimator_config()\nbeta_grid = np.linspace(\n    estimator_config['beta_grid_min'],\n    estimator_config['beta_grid_max'],\n    estimator_config['beta_grid_points']\n)\n\nprint(f\"\\nEstimation Configuration:\")\nprint(f\"  State grid size (N): {N}\")\nprint(f\"  Beta grid: {beta_grid}\")\nprint(f\"  Max iterations: {max_iter}\")\nprint(f\"  Tolerance: {epsilon_tol}\")\nprint(f\"  Training epochs per iteration: {num_epochs}\")\nprint(f\"  Learning rate: {learning_rate}\")\n\n# Run estimation\nprint(f\"\\nRunning estimation...\")\ngamma_hat, beta_hat, beta_grid_used, distances = EstimateMDP(\n    states=states,\n    actions=actions,\n    delta=delta,\n    N=N,\n    state_range=state_range,\n    hyperparameters=hyperparameters,\n    num_epochs=num_epochs,\n    learning_rate=learning_rate,\n    beta_grid=beta_grid,\n    epsilon_tol=epsilon_tol,\n    max_iter=max_iter,\n    gamma_E=gamma_E\n)\n\nprint(f\"\\nEstimation Results:\")\nprint(f\"  Estimated gamma: {gamma_hat:.4f}\")\nprint(f\"  True gamma: {gamma_true:.4f}\")\nprint(f\"  Error: {abs(gamma_hat - gamma_true):.4f}\")\nprint(f\"\\n  Estimated beta: {beta_hat:.4f}\")\nprint(f\"  True beta: {beta_true:.4f}\")\nprint(f\"  Error: {abs(beta_hat - beta_true):.4f}\")\n\n# Plot distance vs beta candidates\nfig, ax = plt.subplots(figsize=(10, 6))\nax.plot(beta_grid_used, distances, 'b-o', linewidth=2, markersize=8, label='Distance')\nax.axvline(beta_true, color='g', linestyle='--', linewidth=2, label=f'True beta = {beta_true:.2f}')\nax.axvline(beta_hat, color='r', linestyle='--', linewidth=2, label=f'Estimated beta = {beta_hat:.2f}')\nax.set_xlabel('Beta Candidate', fontsize=12)\nax.set_ylabel('Distance (CCP mismatch)', fontsize=12)\nax.set_title('Beta Grid Search: Distance vs Beta Candidates', fontsize=14, fontweight='bold')\nax.legend(fontsize=11)\nax.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n# Check if minimum is at or near true beta\nmin_idx = np.argmin(distances)\nprint(f\"\\nBeta Grid Search Diagnostics:\")\nprint(f\"  Minimum distance at beta = {beta_grid_used[min_idx]:.4f}\")\nprint(f\"  True beta = {beta_true:.4f}\")\nprint(f\"  Distance at true beta index: {distances[min_idx]:.6f}\")"
  },
  {
    "objectID": "estimate_mdp.html#save-estimation-results",
    "href": "estimate_mdp.html#save-estimation-results",
    "title": "Estimating Markov Decision Process Parameters",
    "section": "Save Estimation Results",
    "text": "Save Estimation Results\n\n# Save estimation results\noutput_dir = Path('../../output/estimate_mdp')\noutput_dir.mkdir(parents=True, exist_ok=True)\n\n# Save estimated parameters\nest_results = {\n    'gamma_hat': float(gamma_hat),\n    'beta_hat': float(beta_hat),\n    'gamma_true': float(gamma_true),\n    'beta_true': float(beta_true),\n    'gamma_error': float(abs(gamma_hat - gamma_true)),\n    'beta_error': float(abs(beta_hat - beta_true)),\n    'beta_grid': beta_grid.tolist(),\n    'N': int(N),\n    'epsilon_tol': float(epsilon_tol),\n    'max_iter': int(max_iter)\n}\n\nwith open(output_dir / 'estimation_results.json', 'w', encoding='utf-8') as f:\n    json.dump(est_results, f, indent=2)\n\nprint(f\"Saved estimation results to {output_dir.resolve()}\")"
  },
  {
    "objectID": "estimate_mdp.html#compare-estimated-and-true-parameters",
    "href": "estimate_mdp.html#compare-estimated-and-true-parameters",
    "title": "Estimating Markov Decision Process Parameters",
    "section": "Compare Estimated and True Parameters",
    "text": "Compare Estimated and True Parameters\n\n# Create comparison table\ncomparison_data = {\n    'Parameter': ['gamma', 'beta'],\n    'True Value': [gamma_true, beta_true],\n    'Estimated Value': [gamma_hat, beta_hat],\n    'Absolute Error': [abs(gamma_hat - gamma_true), abs(beta_hat - beta_true)],\n    'Relative Error (%)': [\n        100 * abs(gamma_hat - gamma_true) / gamma_true,\n        100 * abs(beta_hat - beta_true) / beta_true\n    ]\n}\n\ndf_comparison = pd.DataFrame(comparison_data)\nprint(\"\\nParameter Estimation Comparison:\")\nprint(df_comparison.to_string(index=False))"
  },
  {
    "objectID": "estimate_mdp.html#compare-equilibrium-objects",
    "href": "estimate_mdp.html#compare-equilibrium-objects",
    "title": "Estimating Markov Decision Process Parameters",
    "section": "Compare Equilibrium Objects",
    "text": "Compare Equilibrium Objects\nNow we compare the estimated and true equilibrium objects: conditional choice probabilities (CCP) and value functions. We load the true value functions from the solver results and compute the implied equilibrium objects under both true and estimated parameters.\n\n# Load true value functions from solver\nsolver_dir = Path('../../output/solve_mdp')\nif not solver_dir.exists():\n    raise FileNotFoundError(\n        f\"Expected solver results in {solver_dir}. Run solve_mdp.qmd first.\"\n    )\n\n# Load true networks\nv_theta_0_true = MonotonicNetwork(hidden_sizes=hyperparameters['hidden_sizes'])\nv_theta_1_true = MonotonicNetwork(hidden_sizes=hyperparameters['hidden_sizes'])\n\nv_theta_0_true.load_state_dict(torch.load(solver_dir / 'v_theta_0.pt', map_location='cpu'))\nv_theta_1_true.load_state_dict(torch.load(solver_dir / 'v_theta_1.pt', map_location='cpu'))\n\nv_theta_0_true.eval()\nv_theta_1_true.eval()\n\nprint(\"Loaded true value functions from solver\")\n\n\nfrom mdp_solver import GenerateStateGrid, SolveValueIteration\n\n# Solve for estimated value functions using estimated parameters\nprint(f\"\\nSolving for value functions under estimated parameters...\")\nprint(f\"  Using gamma_hat={gamma_hat:.4f}, beta_hat={beta_hat:.4f}\")\n\nv_theta_0_est, v_theta_1_est, _ = SolveValueIteration(\n    beta=beta_hat,\n    gamma=gamma_hat,\n    delta=delta,\n    gamma_E=gamma_E,\n    hyperparameters=hyperparameters,\n    N=N,\n    state_range=state_range,\n    max_iter=max_iter,\n    epsilon_tol=epsilon_tol,\n    num_epochs=num_epochs,\n    learning_rate=learning_rate\n)\n\nprint(\"Solved value functions under estimated parameters\")\n\n\n# Generate evaluation grid\nS_eval = GenerateStateGrid(N=N, state_range=state_range)\n\n# Compute true CCPs and values\nwith torch.no_grad():\n    # Compute value functions on grid\n    v0_true = v_theta_0_true(S_eval)\n    v1_true = v_theta_1_true(S_eval)\n    v0_est = v_theta_0_est(S_eval)\n    v1_est = v_theta_1_est(S_eval)\n\n    # Compute choice probabilities for each state in the grid\n    prob_a0_true_list = []\n    prob_a1_true_list = []\n    prob_a0_est_list = []\n    prob_a1_est_list = []\n\n    for i in range(N):\n        s_i = S_eval[i].item()\n\n        # True equilibrium probabilities\n        prob_a0_t, prob_a1_t = ComputeChoiceProbability(\n            s=s_i,\n            v_theta_0=v_theta_0_true,\n            v_theta_1=v_theta_1_true\n        )\n        prob_a0_true_list.append(prob_a0_t)\n        prob_a1_true_list.append(prob_a1_t)\n\n        # Estimated equilibrium probabilities\n        prob_a0_e, prob_a1_e = ComputeChoiceProbability(\n            s=s_i,\n            v_theta_0=v_theta_0_est,\n            v_theta_1=v_theta_1_est\n        )\n        prob_a0_est_list.append(prob_a0_e)\n        prob_a1_est_list.append(prob_a1_e)\n\n    # Convert to tensors\n    prob_a0_true = torch.tensor(prob_a0_true_list).reshape(-1, 1)\n    prob_a1_true = torch.tensor(prob_a1_true_list).reshape(-1, 1)\n    prob_a0_est = torch.tensor(prob_a0_est_list).reshape(-1, 1)\n    prob_a1_est = torch.tensor(prob_a1_est_list).reshape(-1, 1)\n\n# Convert to numpy\ns_grid = S_eval.numpy().flatten()\nprob_a1_true_np = prob_a1_true.numpy().flatten()\nprob_a1_est_np = prob_a1_est.numpy().flatten()\nv0_true_np = v0_true.numpy().flatten()\nv0_est_np = v0_est.numpy().flatten()\nv1_true_np = v1_true.numpy().flatten()\nv1_est_np = v1_est.numpy().flatten()\n\nprint(f\"\\nComputed equilibrium objects on grid of {N} points\")\n\n\n# Create comparison plots\nfig, axes = plt.subplots(2, 2, figsize=(14, 12))\n\n# Plot 1: CCP comparison (scatter with 45-degree line)\nax = axes[0, 0]\nax.scatter(prob_a1_true_np, prob_a1_est_np, alpha=0.6, s=20)\nax.plot([0, 1], [0, 1], 'k--', linewidth=2, label='45° line')\nax.set_xlabel('True P(a=1|s)', fontsize=12)\nax.set_ylabel('Estimated P(a=1|s)', fontsize=12)\nax.set_title('Conditional Choice Probability Comparison', fontsize=14)\nax.legend()\nax.grid(True, alpha=0.3)\nax.set_xlim(0, 1)\nax.set_ylim(0, 1)\nax.set_aspect('equal')\n\n# Compute CCP correlation\nccp_corr = np.corrcoef(prob_a1_true_np, prob_a1_est_np)[0, 1]\nccp_rmse = np.sqrt(np.mean((prob_a1_true_np - prob_a1_est_np)**2))\nax.text(0.05, 0.95, f'Corr: {ccp_corr:.4f}\\nRMSE: {ccp_rmse:.4f}',\n        transform=ax.transAxes, verticalalignment='top',\n        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n\n# Plot 2: Value function v0 comparison\nax = axes[0, 1]\nax.scatter(v0_true_np, v0_est_np, alpha=0.6, s=20)\nv0_min, v0_max = min(v0_true_np.min(), v0_est_np.min()), max(v0_true_np.max(), v0_est_np.max())\nax.plot([v0_min, v0_max], [v0_min, v0_max], 'k--', linewidth=2, label='45° line')\nax.set_xlabel('True v(s, a=0)', fontsize=12)\nax.set_ylabel('Estimated v(s, a=0)', fontsize=12)\nax.set_title('Value Function (a=0) Comparison', fontsize=14)\nax.legend()\nax.grid(True, alpha=0.3)\nax.set_aspect('equal', adjustable='box')\n\nv0_corr = np.corrcoef(v0_true_np, v0_est_np)[0, 1]\nv0_rmse = np.sqrt(np.mean((v0_true_np - v0_est_np)**2))\nax.text(0.05, 0.95, f'Corr: {v0_corr:.4f}\\nRMSE: {v0_rmse:.4f}',\n        transform=ax.transAxes, verticalalignment='top',\n        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n\n# Plot 3: Value function v1 comparison\nax = axes[1, 0]\nax.scatter(v1_true_np, v1_est_np, alpha=0.6, s=20)\nv1_min, v1_max = min(v1_true_np.min(), v1_est_np.min()), max(v1_true_np.max(), v1_est_np.max())\nax.plot([v1_min, v1_max], [v1_min, v1_max], 'k--', linewidth=2, label='45° line')\nax.set_xlabel('True v(s, a=1)', fontsize=12)\nax.set_ylabel('Estimated v(s, a=1)', fontsize=12)\nax.set_title('Value Function (a=1) Comparison', fontsize=14)\nax.legend()\nax.grid(True, alpha=0.3)\nax.set_aspect('equal', adjustable='box')\n\nv1_corr = np.corrcoef(v1_true_np, v1_est_np)[0, 1]\nv1_rmse = np.sqrt(np.mean((v1_true_np - v1_est_np)**2))\nax.text(0.05, 0.95, f'Corr: {v1_corr:.4f}\\nRMSE: {v1_rmse:.4f}',\n        transform=ax.transAxes, verticalalignment='top',\n        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n\n# Plot 4: CCP as function of state\nax = axes[1, 1]\nax.plot(s_grid, prob_a1_true_np, 'b-', linewidth=2, label='True', alpha=0.7)\nax.plot(s_grid, prob_a1_est_np, 'r--', linewidth=2, label='Estimated', alpha=0.7)\nax.set_xlabel('State (s)', fontsize=12)\nax.set_ylabel('P(a=1|s)', fontsize=12)\nax.set_title('CCP as Function of State', fontsize=14)\nax.legend()\nax.grid(True, alpha=0.3)\nax.set_ylim(0, 1)\n\nfig.tight_layout()\nplt.show()\n\nprint(f\"\\nEquilibrium Object Comparison:\")\nprint(f\"  CCP - Correlation: {ccp_corr:.4f}, RMSE: {ccp_rmse:.4f}\")\nprint(f\"  V0  - Correlation: {v0_corr:.4f}, RMSE: {v0_rmse:.4f}\")\nprint(f\"  V1  - Correlation: {v1_corr:.4f}, RMSE: {v1_rmse:.4f}\")"
  },
  {
    "objectID": "estimate_mdp.html#diagnostic-compare-estimated-ccp-with-true-ccp",
    "href": "estimate_mdp.html#diagnostic-compare-estimated-ccp-with-true-ccp",
    "title": "Estimating Markov Decision Process Parameters",
    "section": "Diagnostic: Compare Estimated CCP with True CCP",
    "text": "Diagnostic: Compare Estimated CCP with True CCP\nBefore running the full estimation, we diagnose the quality of CCP estimation by comparing the estimated CCP (from observed choices) with the true CCP (from the solver’s value functions).\n\n# Load solver configuration\nsolver_dir = Path('../../output/solve_mdp')\nwith open(solver_dir / 'config.json', 'r', encoding='utf-8') as f:\n    solver_output = json.load(f)\n\nsolver_config = solver_output['solver']\nhyperparameters = solver_config['hyperparameters']\nN = solver_config['N']\nstate_range = solver_config['state_range']\nnum_epochs = solver_config['num_epochs']\nlearning_rate = solver_config['learning_rate']\n\nprint(f\"\\n=== DIAGNOSTIC: CCP Estimation Quality ===\")\n\n# Estimate CCP from observed (s,a) pairs\n# EstimateCCP returns a network that predicts P(a=1|s)\nprint(f\"\\nEstimating CCP from observed choices...\")\nP_hat_network = EstimateCCP(\n    states=states,\n    actions=actions,\n    hyperparameters=hyperparameters,\n    num_epochs=num_epochs,\n    learning_rate=learning_rate\n)\n\n# Load true value functions from solver\nprint(f\"Loading true value functions from solver...\")\nv_theta_0_true = MonotonicNetwork(hidden_sizes=hyperparameters['hidden_sizes'])\nv_theta_1_true = MonotonicNetwork(hidden_sizes=hyperparameters['hidden_sizes'])\nv_theta_0_true.load_state_dict(torch.load(solver_dir / 'v_theta_0.pt', map_location='cpu'))\nv_theta_1_true.load_state_dict(torch.load(solver_dir / 'v_theta_1.pt', map_location='cpu'))\nv_theta_0_true.eval()\nv_theta_1_true.eval()\n\n# Generate evaluation grid\nS_eval = GenerateStateGrid(N=N, state_range=state_range)\n\n# Evaluate estimated CCP on grid\nwith torch.no_grad():\n    P_hat_eval = P_hat_network(S_eval).numpy().flatten()  # P(a=1|s)\n\n# Compute true CCP on grid from true value functions\nprob_a1_true_list = []\nfor i in range(N):\n    s_i = S_eval[i].item()\n    prob_a0_t, prob_a1_t = ComputeChoiceProbability(\n        s=s_i,\n        v_theta_0=v_theta_0_true,\n        v_theta_1=v_theta_1_true\n    )\n    prob_a1_true_list.append(prob_a1_t)\n\nP_true_eval = np.array(prob_a1_true_list)\n\n# Compute CCP errors\nccp_diff = np.abs(P_hat_eval - P_true_eval)\n\nprint(f\"\\n=== CCP Comparison ===\")\nprint(f\"Comparing: Estimated CCP (from data) vs. True CCP (from solver)\")\nprint(f\"\\nP(a=1|s) errors:\")\nprint(f\"  Mean absolute error: {ccp_diff.mean():.6f}\")\nprint(f\"  Max absolute error:  {ccp_diff.max():.6f}\")\nprint(f\"  Root mean squared error: {np.sqrt((ccp_diff**2).mean()):.6f}\")\n\n# Plot CCP comparison\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# Plot 1: CCPs over state space\nax = axes[0]\ns_grid = S_eval.numpy().flatten()\nax.plot(s_grid, P_true_eval, 'g-', linewidth=2, label='True CCP (solver)', alpha=0.8)\nax.plot(s_grid, P_hat_eval, 'r--', linewidth=2, label='Estimated CCP (from data)', alpha=0.8)\nax.set_xlabel('State (s)', fontsize=12)\nax.set_ylabel('P(a=1|s)', fontsize=12)\nax.set_title('Conditional Choice Probability', fontsize=14, fontweight='bold')\nax.legend(fontsize=11)\nax.grid(True, alpha=0.3)\nax.set_ylim([0, 1])\n\n# Plot 2: Scatter plot (estimated vs true)\nax = axes[1]\nax.scatter(P_true_eval, P_hat_eval, alpha=0.6, s=30)\nax.plot([0, 1], [0, 1], 'k--', linewidth=2, label='45° line')\nax.set_xlabel('True P(a=1|s)', fontsize=12)\nax.set_ylabel('Estimated P(a=1|s)', fontsize=12)\nax.set_title('CCP: Estimated vs True', fontsize=14, fontweight='bold')\nax.legend(fontsize=11)\nax.grid(True, alpha=0.3)\nax.set_xlim([0, 1])\nax.set_ylim([0, 1])\nax.set_aspect('equal')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n=== Interpretation ===\")\nprint(\"If CCP errors are LARGE: The CCP estimation is poor.\")\nprint(\"If CCP errors are SMALL: CCP estimation is good; beta estimation issues are isolated.\")\n\n\n=== DIAGNOSTIC: CCP Estimation Quality ===\n\nEstimating CCP from observed choices...\nLoading true value functions from solver...\n\n=== CCP Comparison ===\nComparing: Estimated CCP (from data) vs. True CCP (from solver)\n\nP(a=1|s) errors:\n  Mean absolute error: 0.044347\n  Max absolute error:  0.181708\n  Root mean squared error: 0.058151\n\n\n\n\n\n\n\n\n\n\n=== Interpretation ===\nIf CCP errors are LARGE: The CCP estimation is poor.\nIf CCP errors are SMALL: CCP estimation is good; beta estimation issues are isolated."
  }
]