---
title: "Estimating Markov Decision Process Parameters"
format:
  html:
    toc: true
    code-fold: false
jupyter: python3
execute:
  daemon: false
---

## Load Configuration and Simulation Results

```{python}
import sys
sys.path.insert(0, '..')
sys.path.insert(0, '../../src')

import json
from pathlib import Path
import numpy as np

from config_mdp import get_solver_config

# Load simulation results
sim_dir = Path('../../output/simulate_mdp')
if not sim_dir.exists():
    raise FileNotFoundError(
        f"Expected simulation results in {sim_dir}. Run simulate_mdp.qmd first."
    )

# Load configuration
with open(sim_dir / 'simulation_config.json', 'r', encoding='utf-8') as f:
    sim_config = json.load(f)

# Load simulated data
sim_data = np.load(sim_dir / 'simulation_results.npz')
states = sim_data['states']
actions = sim_data['actions']
rewards = sim_data['rewards']

M = sim_config['M']
T = sim_config['T']
delta = sim_config['delta']
state_range = tuple(sim_config['state_range'])
beta_true = sim_config['beta']
gamma_true = sim_config['gamma']

print(f"Loaded simulation data: M={M}, T={T}, delta={delta}")
print(f"True parameters: beta={beta_true}, gamma={gamma_true}")
```

## Estimation Strategy

We estimate the structural parameters $\beta$ and $\gamma$ using a two-step approach. The key insight is that in real data, we typically observe only states and actions - rewards are not observed.

**Observable data**: $\{(s_t^{(m)}, a_t^{(m)})\}_{m=1,t=0}^{M,T-1}$

**Known parameter**: $\delta$ (discount factor)

**Parameters to estimate**: $\beta$ (reward weight), $\gamma$ (depreciation rate)

### Step 1: Estimate State Depreciation Parameter $\gamma$

The state transition is deterministic:
$$s_{t+1} = (1 - \gamma) s_t + a_t$$

Rearranging:
$$\gamma = 1 - \frac{s_{t+1} - a_t}{s_t}$$

We can directly estimate $\gamma$ from observed state transitions and actions using ordinary least squares or method of moments.

### Step 2: Estimate Reward Parameter $\beta$ via Revealed Preference

The reward function is:
$$\bar{r}(s, a) = \beta \log(1 + s) - a$$

Since rewards are not observed, we use a revealed preference approach based on the optimality of observed actions. This follows an actor-critic style method:

#### Step 2a: Estimate Conditional Choice Probability (CCP)

Estimate the choice probability as a function of state:
$$\hat{P}(a=1|s) = f_\theta(s)$$

where $f_\theta$ is a neural network that is:
- Continuous in $s$
- Monotonically increasing in $s$ (enforced by architecture)

This is estimated by maximum likelihood using the observed state-action pairs.

#### Step 2b: Value Function Iteration Given CCP and Candidate $\beta$

For a candidate value of $\beta$, train value functions $v^{(0)}(s)$ and $v^{(1)}(s)$ that are consistent with the estimated CCP $\hat{P}(a|s)$ from Step 2a.

The value functions satisfy the Bellman equation under the estimated policy:
$$v^{(a)}(s) = \bar{r}(s, a) + \delta \mathbb{E}_{a' \sim \hat{P}(\cdot|s')} [v^{(a')}(s')]$$

where $s' = (1-\hat{\gamma})s + a$ using the estimated $\hat{\gamma}$ from Step 1.

This is the "critic" step - we evaluate the value functions under the estimated policy.

#### Step 2c: Compute Updated CCP from Value Functions

From the trained value functions, compute the implied choice probabilities using the logit formula:
$$P^{\text{updated}}(a=1|s) = \frac{\exp(v^{(1)}(s))}{\exp(v^{(0)}(s)) + \exp(v^{(1)}(s))}$$

#### Step 2d: Find $\beta$ that Minimizes Distance

Find the value of $\beta$ that minimizes the distance between the estimated CCP network and the updated CCP:
$$\hat{\beta} = \arg\min_\beta \sum_{s \in \mathcal{S}} \left[\hat{P}(a=1|s) - P^{\text{updated}}(a=1|s; \beta)\right]^2$$

where $\mathcal{S}$ is a grid of state values spanning the observed state range.

This is a nested fixed-point problem:
- **Outer loop**: Search over candidate values of $\beta$
- **Inner loop**: For each $\beta$, solve for value functions consistent with estimated CCP, then compute updated CCP

The optimal $\beta$ is the one where the estimated policy (from data) is consistent with the optimal policy (from the model).

## Pseudo Code

::: {.callout-note appearance="simple"}

## Algorithm: Two-Step Estimation of MDP Parameters

**Design Principles:**

This pseudo code is designed for direct implementation:

- **No placeholders**: Every operation corresponds to a concrete function or method call
- **Explicit parameters**: All inputs are passed as parameters, no implicit/global state
- **Reuses shared functions**: Uses `ComputeNextState`, `ComputeMeanReward`, `ComputeExpectedValue`, `ComputeChoiceProbability` from solve_mdp
- **No hard-coding**: All values are parameterized

**Input:**

- Observed data: states: Array[$M \times T$], actions: Array[$M \times T$]
- Known parameter: $\delta$: float (discount factor)
- State grid for evaluation: $N$: int, state_range: tuple[float, float]
- Network hyperparameters: hyperparameters: dict (containing `hidden_sizes`: list[int])
- Training parameters: num_epochs: int, learning_rate: float
- Search grid for $\beta$: beta_grid: Array[$K$] (candidate values of $\beta$)
- Tolerance: $\epsilon_{\text{tol}}$: float, max_iter: int
- Other constants: $\gamma_E$: float (Euler's constant)

**Output:**

- Estimated parameters: $\hat{\gamma}$: float, $\hat{\beta}$: float

---

### Main Algorithm

**Procedure** `EstimateMDP`(states: Array[$M \times T$], actions: Array[$M \times T$], $\delta$: float, $N$: int, state_range: tuple[float, float], hyperparameters: dict, num_epochs: int, learning_rate: float, beta_grid: Array[$K$], $\epsilon_{\text{tol}}$: float, max_iter: int, $\gamma_E$: float) $\to$ (float, float)

1. $\hat{\gamma}$: float $\leftarrow$ `EstimateGamma`(states, actions)

2. $\hat{\beta}$: float $\leftarrow$ `EstimateBeta`(states, actions, $\hat{\gamma}$, $\delta$, $N$, state_range, hyperparameters, num_epochs, learning_rate, beta_grid, $\epsilon_{\text{tol}}$, max_iter, $\gamma_E$)

3. **Return** $\hat{\gamma}$, $\hat{\beta}$

---

### Step 1: Estimate Gamma

**Procedure** `EstimateGamma`(states: Array[$M \times T$], actions: Array[$M \times T$]) $\to$ float

1. gamma_estimates: Array $\leftarrow$ Empty array

2. For $m$: int = 1 to $M$:

   a. For $t$: int = 1 to $T-1$:

   &nbsp;&nbsp;&nbsp;&nbsp;i. $s_t$: float $\leftarrow$ states[$m$, $t$]

   &nbsp;&nbsp;&nbsp;&nbsp;ii. $s_{t+1}$: float $\leftarrow$ states[$m$, $t+1$]

   &nbsp;&nbsp;&nbsp;&nbsp;iii. $a_t$: int $\leftarrow$ actions[$m$, $t$]

   &nbsp;&nbsp;&nbsp;&nbsp;iv. **If** $s_t \neq 0$:

   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\gamma_{mt}$: float $\leftarrow 1 - \frac{s_{t+1} - a_t}{s_t}$

   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Append $\gamma_{mt}$ to gamma_estimates

3. $\hat{\gamma}$: float $\leftarrow$ `Mean`(gamma_estimates)

4. **Return** $\hat{\gamma}$

---

### Step 2: Estimate Beta via Revealed Preference

**Procedure** `EstimateBeta`(states: Array[$M \times T$], actions: Array[$M \times T$], $\hat{\gamma}$: float, $\delta$: float, $N$: int, state_range: tuple[float, float], hyperparameters: dict, num_epochs: int, learning_rate: float, beta_grid: Array[$K$], $\epsilon_{\text{tol}}$: float, max_iter: int, $\gamma_E$: float) $\to$ float

1. $\hat{P}$: Network $\leftarrow$ `EstimateCCP`(states, actions, hyperparameters, num_epochs, learning_rate)

2. $S$: Tensor[$N \times 1$] $\leftarrow$ `GenerateStateGrid`($N$, state_range)

3. distances: Array[$K$] $\leftarrow$ Empty array

4. For $k$: int = 1 to $K$:

   a. $\beta_k$: float $\leftarrow$ beta_grid[$k$]

   b. $v_\theta^{(0)}$, $v_\theta^{(1)}$ $\leftarrow$ `SolveValueFunctionGivenCCP`($\hat{P}$, $\beta_k$, $\hat{\gamma}$, $\delta$, $\gamma_E$, hyperparameters, $S$, max_iter, $\epsilon_{\text{tol}}$, num_epochs, learning_rate)

   c. $P^{\text{updated}}$: Tensor[$N \times 1$] $\leftarrow$ `ComputeCCPFromValue`($S$, $v_\theta^{(0)}$, $v_\theta^{(1)}$)

   d. $\hat{P}_{\text{eval}}$: Tensor[$N \times 1$] $\leftarrow \hat{P}(S)$

   e. distance: float $\leftarrow$ `ComputeDistance`($\hat{P}_{\text{eval}}$, $P^{\text{updated}}$)

   f. distances[$k$] $\leftarrow$ distance

5. $k^*$: int $\leftarrow$ `ArgMin`(distances)

6. $\hat{\beta}$: float $\leftarrow$ beta_grid[$k^*$]

7. **Return** $\hat{\beta}$

---

### Subroutines for Step 2a: Estimate CCP

**Procedure** `EstimateCCP`(states: Array[$M \times T$], actions: Array[$M \times T$], hyperparameters: dict, num_epochs: int, learning_rate: float) $\to$ Network

1. $\hat{P}$: Network $\leftarrow$ `InitializeMonotonicNetwork`(hyperparameters)

2. optimizer: Optimizer $\leftarrow$ `CreateOptimizer`($\hat{P}$, learning_rate)

3. For epoch: int = 1 to num_epochs:

   a. optimizer.`zero_grad`()

   b. For $m$: int = 1 to $M$:

   &nbsp;&nbsp;&nbsp;&nbsp;i. For $t$: int = 1 to $T$:

   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$s_{mt}$: Tensor $\leftarrow$ states[$m$, $t$]

   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$a_{mt}$: int $\leftarrow$ actions[$m$, $t$]

   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\hat{p}_{mt}$: Tensor $\leftarrow \hat{P}(s_{mt})$ &nbsp;&nbsp;&nbsp;&nbsp;// Probability of $a=1$

   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;loss: Tensor $\leftarrow$ `ComputeBinaryCrossEntropy`($a_{mt}$, $\hat{p}_{mt}$)

   c. loss.`backward`()

   d. optimizer.`step`()

4. **Return** $\hat{P}$

---

### Subroutines for Step 2b: Solve Value Function Given CCP

**Procedure** `SolveValueFunctionGivenCCP`($\hat{P}$: Network, $\beta$: float, $\gamma$: float, $\delta$: float, $\gamma_E$: float, hyperparameters: dict, $S$: Tensor[$N \times 1$], max_iter: int, $\epsilon_{\text{tol}}$: float, num_epochs: int, learning_rate: float) $\to$ (Network, Network)

1. $v_\theta^{(0)}$: Network, $v_\theta^{(1)}$: Network $\leftarrow$ `InitializeNetworks`(hyperparameters)

2. optimizer: Optimizer $\leftarrow$ `CreateOptimizer`($[v_\theta^{(0)}, v_\theta^{(1)}]$, learning_rate)

3. For iteration: int = 1 to max_iter:

   a. $\{y_i^{(a)}\}$: Tensor[$N \times 2$] $\leftarrow$ `ComputeBellmanTargetsGivenCCP`($S$, $\hat{P}$, $v_\theta^{(0)}$, $v_\theta^{(1)}$, $\beta$, $\gamma$, $\delta$, $\gamma_E$)

   b. $v_\theta^{(0)}$, $v_\theta^{(1)}$ $\leftarrow$ `UpdateNetworks`($S$, $\{y_i^{(a)}\}$, $v_\theta^{(0)}$, $v_\theta^{(1)}$, num_epochs, optimizer)

   c. max_error: float $\leftarrow$ `CheckConvergence`($S$, $\{y_i^{(a)}\}$, $v_\theta^{(0)}$, $v_\theta^{(1)}$)

   d. **If** max_error $< \epsilon_{\text{tol}}$: **Break**

4. **Return** $v_\theta^{(0)}$, $v_\theta^{(1)}$

---

**Procedure** `ComputeBellmanTargetsGivenCCP`($S$: Tensor[$N \times 1$], $\hat{P}$: Network, $v_\theta^{(0)}$: Network, $v_\theta^{(1)}$: Network, $\beta$: float, $\gamma$: float, $\delta$: float, $\gamma_E$: float) $\to$ Tensor[$N \times 2$]

For each $s_i$: float $\in S$ and $a$: int $\in \{0, 1\}$:

&nbsp;&nbsp;&nbsp;&nbsp;$s'_i$: Tensor[$N \times 1$] $\leftarrow$ `ComputeNextState`($s_i$, $a$, $\gamma$)

&nbsp;&nbsp;&nbsp;&nbsp;$\hat{p}_i$: Tensor[$N \times 1$] $\leftarrow \hat{P}(s'_i)$ &nbsp;&nbsp;&nbsp;&nbsp;// Estimated $P(a=1|s'_i)$

&nbsp;&nbsp;&nbsp;&nbsp;$v_0$: Tensor[$N \times 1$] $\leftarrow v_\theta^{(0)}(s'_i)$

&nbsp;&nbsp;&nbsp;&nbsp;$v_1$: Tensor[$N \times 1$] $\leftarrow v_\theta^{(1)}(s'_i)$

&nbsp;&nbsp;&nbsp;&nbsp;$\text{EV}_i$: Tensor[$N \times 1$] $\leftarrow (1 - \hat{p}_i) \cdot v_0 + \hat{p}_i \cdot v_1 + \gamma_E$ &nbsp;&nbsp;&nbsp;&nbsp;// Expected value under estimated policy

&nbsp;&nbsp;&nbsp;&nbsp;$y_i^{(a)}$: Tensor[$N \times 1$] $\leftarrow$ `ComputeMeanReward`($s_i$, $a$, $\beta$) $+ \delta \cdot \text{EV}_i$

**Return** $\{y_i^{(a)}\}$: Tensor[$N \times 2$] for all $i$, $a$

---

### Subroutines for Step 2c: Compute CCP from Value Functions

**Procedure** `ComputeCCPFromValue`($S$: Tensor[$N \times 1$], $v_\theta^{(0)}$: Network, $v_\theta^{(1)}$: Network) $\to$ Tensor[$N \times 1$]

1. $v_0$: Tensor[$N \times 1$] $\leftarrow v_\theta^{(0)}(S)$

2. $v_1$: Tensor[$N \times 1$] $\leftarrow v_\theta^{(1)}(S)$

3. $P^{\text{updated}}$: Tensor[$N \times 1$] $\leftarrow \frac{\exp(v_1)}{\exp(v_0) + \exp(v_1)}$ &nbsp;&nbsp;&nbsp;&nbsp;// Probability of $a=1$

4. **Return** $P^{\text{updated}}$

---

### Subroutines for Step 2d: Compute Distance

**Procedure** `ComputeDistance`($\hat{P}_{\text{eval}}$: Tensor[$N \times 1$], $P^{\text{updated}}$: Tensor[$N \times 1$]) $\to$ float

1. diff: Tensor[$N \times 1$] $\leftarrow \hat{P}_{\text{eval}} - P^{\text{updated}}$

2. squared_diff: Tensor[$N \times 1$] $\leftarrow$ diff $\odot$ diff &nbsp;&nbsp;&nbsp;&nbsp;// Element-wise square

3. distance: float $\leftarrow$ `Sum`(squared_diff)

4. **Return** distance

---

### Shared Functions from solve_mdp

The following functions are imported from solve_mdp and reused directly:

**Procedure** `ComputeNextState`($s$: Tensor, $a$: int, $\gamma$: float) $\to$ Tensor

- Returns $(1 - \gamma) \cdot s + a$
- See solve_mdp documentation for implementation details

**Procedure** `ComputeMeanReward`($s$: Tensor, $a$: int, $\beta$: float) $\to$ Tensor

- Returns $\beta \cdot \log(1 + s) - a$
- See solve_mdp documentation for implementation details

**Procedure** `InitializeNetworks`(hyperparameters: dict) $\to$ (Network, Network)

- See solve_mdp documentation for implementation details

**Procedure** `UpdateNetworks`($S$: Tensor, targets: Tensor, $v_\theta^{(0)}$: Network, $v_\theta^{(1)}$: Network, num_epochs: int, optimizer: Optimizer) $\to$ (Network, Network)

- See solve_mdp documentation for implementation details

**Procedure** `CheckConvergence`($S$: Tensor, targets: Tensor, $v_\theta^{(0)}$: Network, $v_\theta^{(1)}$: Network) $\to$ float

- See solve_mdp documentation for implementation details

:::

**Key Implementation Details:**

- **Shared configuration**: Load $\delta$, $\gamma_E$ from the same config used in solve_mdp
- **Monotonic network for CCP**: The CCP network $\hat{P}$ uses monotonic architecture to ensure $\hat{P}(a=1|s)$ is increasing in $s$
- **Nested fixed-point**: The outer loop searches over $\beta$ candidates, inner loop solves value functions for each $\beta$
- **Binary cross-entropy loss**: For CCP estimation, use $-[a \log(\hat{p}) + (1-a) \log(1-\hat{p})]$
- **Grid search for $\beta$**: Can be replaced with gradient-free optimization methods like Nelder-Mead
