---
title: "Estimating Markov Decision Process Parameters"
format:
  html:
    toc: true
    code-fold: false
jupyter: python3
execute:
  daemon: false
---

## Load Configuration and Simulation Results

```{python}
import sys
sys.path.insert(0, '..')
sys.path.insert(0, '../../src')

import json
from pathlib import Path
import numpy as np
import torch
import matplotlib.pyplot as plt
import pandas as pd

# Import MDP modules
from mdp_estimator import EstimateMDP, EstimateCCP
from mdp_solver import MonotonicNetwork, ComputeChoiceProbability, GenerateStateGrid, SolveValueIteration
from config_mdp import get_estimator_config

# Load simulation results and configuration
sim_dir = Path('../../output/simulate_mdp')
if not sim_dir.exists():
    raise FileNotFoundError(
        f"Expected simulation results in {sim_dir}. Run simulate_mdp.qmd first."
    )

# Load configuration from simulator output (as data from previous step)
with open(sim_dir / 'config.json', 'r', encoding='utf-8') as f:
    config = json.load(f)

model_config = config['model']
simulator_config = config['simulator']

# Extract model parameters (true values for comparison)
beta_true = model_config['beta']
gamma_true = model_config['gamma']
delta = model_config['delta']
gamma_E = model_config['gamma_E']

# Extract simulator parameters
M = simulator_config['M']
T = simulator_config['T']

# Load simulated data
sim_data = np.load(sim_dir / 'simulation_results.npz')
states = sim_data['states']
actions = sim_data['actions']
rewards = sim_data['rewards']
state_range = tuple(sim_data['state_range'])

print(f"Loaded simulation data: M={M}, T={T}, delta={delta}")
print(f"True parameters: beta={beta_true}, gamma={gamma_true}")
```

## Estimation Strategy

We estimate the structural parameters $\beta$ and $\gamma$ using a two-step approach. The key insight is that in real data, we typically observe only states and actions - rewards are not observed.

**Observable data**: $\{(s_t^{(m)}, a_t^{(m)})\}_{m=1,t=0}^{M,T-1}$

**Known parameter**: $\delta$ (discount factor)

**Parameters to estimate**: $\beta$ (reward weight), $\gamma$ (depreciation rate)

### Step 1: Estimate State Depreciation Parameter $\gamma$

The state transition is deterministic:
$$s_{t+1} = (1 - \gamma) s_t + a_t$$

Rearranging:
$$\gamma = 1 - \frac{s_{t+1} - a_t}{s_t}$$

We can directly estimate $\gamma$ from observed state transitions and actions using ordinary least squares or method of moments.

### Step 2: Estimate Reward Parameter $\beta$ via Revealed Preference

The reward function is:
$$\bar{r}(s, a) = \beta \log(1 + s) - a$$

Since rewards are not observed, we use a revealed preference approach based on the optimality of observed actions. This follows an actor-critic style method:

#### Step 2a: Estimate Conditional Choice Probability (CCP)

Estimate the choice probability as a function of state:

$$\hat{P}(a=1|s) = \sigma(g_\theta(s))$$

where $g_\theta$ is a standard neural network:
- Continuous in $s$
- **No monotonicity constraints** (unconstrained weights)
- Flexibly fits the observed choice pattern from data
- Output layer applies sigmoid: $\hat{P}(a=1|s) = \sigma(g_\theta(s))$, output $\in [0,1]$

This is estimated by maximum likelihood using the observed state-action pairs.

#### Step 2b: Solve for Value Functions Given CCP and Candidate $\beta$

For a candidate value of $\beta$, solve for value functions $v^{(0)}(s)$ and $v^{(1)}(s)$ that are consistent with the estimated CCP $\hat{P}(a|s)$ from Step 2a.

The value functions satisfy the Bellman equation under the estimated policy:
$$v^{(a)}(s_i) = \bar{r}(s_i, a) + \delta \left[(1 - \hat{P}(a=1|s'_i)) v^{(0)}(s'_i) + \hat{P}(a=1|s'_i) v^{(1)}(s'_i)\right] + \delta \gamma_E$$

where $s'_i = (1-\hat{\gamma})s_i + a$ using the estimated $\hat{\gamma}$ from Step 1.

**Efficient Solution via Linear System**:

Since we evaluate on a discrete state grid $\{s_1, ..., s_N\}$, the Bellman equation forms a **linear system** in the value functions. We can solve it directly:

1. **Solve linear system**: For each action $a \in \{0,1\}$, solve $(I - \delta T^{(a)}) v^{(a)} = r^{(a)}$ where $T^{(a)}$ is the transition matrix under the estimated policy
2. **Fit neural networks**: Train networks $v_\theta^{(0)}, v_\theta^{(1)}$ to approximate the solved grid values via supervised learning

This is much faster than iterative value function training, as it requires only one linear solve and one supervised learning step per $\beta$ candidate.

#### Step 2c: Compute Updated CCP from Value Functions

From the trained value functions, compute the implied choice probabilities using the logit formula:
$$P^{\text{updated}}(a=1|s) = \frac{\exp(v^{(1)}(s))}{\exp(v^{(0)}(s)) + \exp(v^{(1)}(s))}$$

#### Step 2d: Find $\beta$ that Minimizes Distance

Find the value of $\beta$ that minimizes the distance between the estimated CCP network and the updated CCP:
$$\hat{\beta} = \arg\min_\beta \sum_{s \in \mathcal{S}} \left[\hat{P}(a=1|s) - P^{\text{updated}}(a=1|s; \beta)\right]^2$$

where $\mathcal{S}$ is a grid of state values spanning the observed state range.

This is a nested fixed-point problem:
- **Outer loop**: Search over candidate values of $\beta$ **in parallel**
- **Inner loop**: For each $\beta$, solve for value functions consistent with estimated CCP, then compute updated CCP

The optimal $\beta$ is the one where the estimated policy (from data) is consistent with the optimal policy (from the model).

**Parallelization**: Since the evaluation of each $\beta$ candidate is independent, we can parallelize the search across multiple workers, significantly reducing computation time.

## Pseudo Code

::: {.callout-note appearance="simple"}

## Algorithm: Two-Step Estimation of MDP Parameters

**Design Principles:**

This pseudo code is designed for direct implementation:

- **No placeholders**: Every operation corresponds to a concrete function or method call
- **Explicit parameters**: All inputs are passed as parameters, no implicit/global state
- **Reuses shared functions**: Uses `ComputeNextState`, `ComputeMeanReward`, `ComputeExpectedValue`, `ComputeChoiceProbability` from solve_mdp
- **No hard-coding**: All values are parameterized

**Input:**

- Observed data: states: Array[$M \times T$], actions: Array[$M \times T$]
- Known parameter: $\delta$: float (discount factor)
- State grid for evaluation: $N$: int, state_range: tuple[float, float]
- Network hyperparameters: hyperparameters: dict (containing `hidden_sizes`: list[int])
- Training parameters: num_epochs: int, learning_rate: float
- Search grid for $\beta$: beta_grid: Array[$K$] (candidate values of $\beta$)
- Tolerance: $\epsilon_{\text{tol}}$: float, max_iter: int
- Other constants: $\gamma_E$: float (Euler's constant)

**Output:**

- Estimated parameters: $\hat{\gamma}$: float, $\hat{\beta}$: float
- Beta search grid: beta_grid: Array[$K$]
- Distances for each beta candidate: distances: Array[$K$]

---

### Main Algorithm

**Procedure** `EstimateMDP`(states: Array[$M \times T$], actions: Array[$M \times T$], $\delta$: float, $N$: int, state_range: tuple[float, float], hyperparameters: dict, num_epochs: int, learning_rate: float, beta_grid: Array[$K$], $\epsilon_{\text{tol}}$: float, max_iter: int, $\gamma_E$: float) $\to$ (float, float, Array[$K$], Array[$K$])

1. $\hat{\gamma}$: float $\leftarrow$ `EstimateGamma`(states, actions)

2. $\hat{\beta}$: float, distances: Array[$K$] $\leftarrow$ `EstimateBeta`(states, actions, $\hat{\gamma}$, $\delta$, $N$, state_range, hyperparameters, num_epochs, learning_rate, beta_grid, $\epsilon_{\text{tol}}$, max_iter, $\gamma_E$)

3. **Return** $\hat{\gamma}$, $\hat{\beta}$, beta_grid, distances

---

### Step 1: Estimate Gamma

**Procedure** `EstimateGamma`(states: Array[$M \times T$], actions: Array[$M \times T$]) $\to$ float

1. gamma_estimates: Array $\leftarrow$ Empty array

2. For $m$: int = 1 to $M$:

   a. For $t$: int = 1 to $T-1$:

   &nbsp;&nbsp;&nbsp;&nbsp;i. $s_t$: float $\leftarrow$ states[$m$, $t$]

   &nbsp;&nbsp;&nbsp;&nbsp;ii. $s_{t+1}$: float $\leftarrow$ states[$m$, $t+1$]

   &nbsp;&nbsp;&nbsp;&nbsp;iii. $a_t$: int $\leftarrow$ actions[$m$, $t$]

   &nbsp;&nbsp;&nbsp;&nbsp;iv. **If** $s_t \neq 0$:

   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\gamma_{mt}$: float $\leftarrow 1 - \frac{s_{t+1} - a_t}{s_t}$

   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Append $\gamma_{mt}$ to gamma_estimates

3. $\hat{\gamma}$: float $\leftarrow$ `Mean`(gamma_estimates)

4. **Return** $\hat{\gamma}$

---

### Step 2: Estimate Beta via Revealed Preference

**Procedure** `EstimateBeta`(states: Array[$M \times T$], actions: Array[$M \times T$], $\hat{\gamma}$: float, $\delta$: float, $N$: int, state_range: tuple[float, float], hyperparameters: dict, num_epochs: int, learning_rate: float, beta_grid: Array[$K$], $\epsilon_{\text{tol}}$: float, max_iter: int, $\gamma_E$: float) $\to$ (float, Array[$K$])

1. $\hat{P}$: Network $\leftarrow$ `EstimateCCP`(states, actions, hyperparameters, num_epochs, learning_rate) &nbsp;&nbsp;&nbsp;&nbsp;// Returns network that predicts $P(a=1|s)$

2. $S$: Tensor[$N \times 1$] $\leftarrow$ `GenerateStateGrid`($N$, state_range)

3. $\hat{P}_{\text{eval}}$: Tensor[$N \times 1$] $\leftarrow \hat{P}(S)$ &nbsp;&nbsp;&nbsp;&nbsp;// Pre-compute $P(a=1|s)$ on grid once

4. distances: Array[$K$] $\leftarrow$ Empty array

5. **ParallelFor** $k$: int = 1 to $K$ **do in parallel**: &nbsp;&nbsp;&nbsp;&nbsp;// Evaluate beta candidates in parallel

   a. $\beta_k$: float $\leftarrow$ beta_grid[$k$]

   b. distance: float $\leftarrow$ `EvaluateBetaCandidate`($\beta_k$, $\hat{P}$, $\hat{P}_{\text{eval}}$, $\hat{\gamma}$, $\delta$, $\gamma_E$, hyperparameters, $S$, max_iter, $\epsilon_{\text{tol}}$, num_epochs, learning_rate)

   c. distances[$k$] $\leftarrow$ distance

6. $k^*$: int $\leftarrow$ `ArgMin`(distances)

7. $\hat{\beta}$: float $\leftarrow$ beta_grid[$k^*$]

8. **Return** $\hat{\beta}$, distances

---

### Subroutine for Parallel Beta Evaluation

**Procedure** `EvaluateBetaCandidate`($\beta_k$: float, $\hat{P}$: Network, $\hat{P}_{\text{eval}}$: Tensor[$N \times 1$], $\gamma$: float, $\delta$: float, $\gamma_E$: float, hyperparameters: dict, $S$: Tensor[$N \times 1$], max_iter: int, $\epsilon_{\text{tol}}$: float, num_epochs: int, learning_rate: float) $\to$ float

1. $v_\theta^{(0)}$, $v_\theta^{(1)}$ $\leftarrow$ `SolveValueFunctionGivenCCP`($\hat{P}$, $\beta_k$, $\gamma$, $\delta$, $\gamma_E$, hyperparameters, $S$, max_iter, $\epsilon_{\text{tol}}$, num_epochs, learning_rate)

2. $P^{\text{updated}}$: Tensor[$N \times 1$] $\leftarrow$ `ComputeCCPFromValue`($S$, $v_\theta^{(0)}$, $v_\theta^{(1)}$) &nbsp;&nbsp;&nbsp;&nbsp;// Returns $P(a=1|s)$

3. distance: float $\leftarrow$ `ComputeDistance`($\hat{P}_{\text{eval}}$, $P^{\text{updated}}$) &nbsp;&nbsp;&nbsp;&nbsp;// Both are $P(a=1|s)$

4. **Return** distance

---

### Subroutines for Step 2a: Estimate CCP

**Procedure** `EstimateCCP`(states: Array[$M \times T$], actions: Array[$M \times T$], hyperparameters: dict, num_epochs: int, learning_rate: float) $\to$ Network

1. $\hat{P}$: Network $\leftarrow$ `InitializeCCPNetwork`(hyperparameters) &nbsp;&nbsp;&nbsp;&nbsp;// Standard network: unconstrained weights, sigmoid output $\in [0,1]$

2. optimizer: Optimizer $\leftarrow$ `CreateOptimizer`($\hat{P}$, learning_rate)

3. For epoch: int = 1 to num_epochs:

   a. optimizer.`zero_grad`()

   b. For $m$: int = 1 to $M$:

   &nbsp;&nbsp;&nbsp;&nbsp;i. For $t$: int = 1 to $T$:

   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$s_{mt}$: Tensor $\leftarrow$ states[$m$, $t$]

   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$a_{mt}$: int $\leftarrow$ actions[$m$, $t$]

   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\hat{p}_{mt}$: Tensor $\leftarrow \hat{P}(s_{mt})$ &nbsp;&nbsp;&nbsp;&nbsp;// Probability of $a=1$

   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;loss: Tensor $\leftarrow$ `ComputeBinaryCrossEntropy`($a_{mt}$, $\hat{p}_{mt}$)

   c. loss.`backward`()

   d. optimizer.`step`()

4. **Return** $\hat{P}$ &nbsp;&nbsp;&nbsp;&nbsp;// Returns network that predicts $P(a=1|s)$

---

### Subroutines for Step 2b: Solve Value Function Given CCP

**Procedure** `SolveValueFunctionGivenCCP`($\hat{P}$: Network, $\beta$: float, $\gamma$: float, $\delta$: float, $\gamma_E$: float, hyperparameters: dict, $S$: Tensor[$N \times 1$], num_epochs: int, learning_rate: float) $\to$ (Network, Network)

1. $v^{(0)}$: Array[$N$], $v^{(1)}$: Array[$N$] $\leftarrow$ `SolveLinearBellman`($\hat{P}$, $\beta$, $\gamma$, $\delta$, $\gamma_E$, $S$)

2. $v_\theta^{(0)}$, $v_\theta^{(1)}$ $\leftarrow$ `FitNetworksToValues`($S$, $v^{(0)}$, $v^{(1)}$, hyperparameters, num_epochs, learning_rate)

3. **Return** $v_\theta^{(0)}$, $v_\theta^{(1)}$

---

**Procedure** `SolveLinearBellman`($\hat{P}$: Network, $\beta$: float, $\gamma$: float, $\delta$: float, $\gamma_E$: float, $S$: Tensor[$N \times 1$]) $\to$ (Array[$N$], Array[$N$])

1. $N$: int $\leftarrow$ Length($S$)

2. For each action $a$: int $\in \{0, 1\}$:

   a. $r^{(a)}$: Array[$N$] $\leftarrow$ Empty array

   b. $T^{(a)}$: Matrix[$N \times N$] $\leftarrow$ Zero matrix

   c. For $i$: int = 1 to $N$:

   &nbsp;&nbsp;&nbsp;&nbsp;i. $s_i$: float $\leftarrow S[i]$

   &nbsp;&nbsp;&nbsp;&nbsp;ii. $r^{(a)}[i]$ $\leftarrow$ `ComputeMeanReward`($s_i$, $a$, $\beta$)

   &nbsp;&nbsp;&nbsp;&nbsp;iii. $s'_i$: float $\leftarrow$ `ComputeNextState`($s_i$, $a$, $\gamma$)

   &nbsp;&nbsp;&nbsp;&nbsp;iv. $\hat{p}_i$: float $\leftarrow \hat{P}(s'_i)$ &nbsp;&nbsp;&nbsp;&nbsp;// $P(a=1|s'_i)$ from network

   &nbsp;&nbsp;&nbsp;&nbsp;v. For $j$: int = 1 to $N$: &nbsp;&nbsp;&nbsp;&nbsp;// Find which grid point $s'_i$ maps to

   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**If** $s'_i \approx S[j]$: &nbsp;&nbsp;&nbsp;&nbsp;// Interpolate or find nearest neighbor

   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$T^{(a)}[i, j]$ $\leftarrow (1 - \hat{p}_i)$ &nbsp;&nbsp;&nbsp;&nbsp;// Transition to $v^{(0)}$

   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;// Also add contribution to $v^{(1)}$ via second matrix (or handle jointly)

   d. Add $\delta \gamma_E$ to $r^{(a)}$ &nbsp;&nbsp;&nbsp;&nbsp;// Constant term from Euler constant

   e. $v^{(a)}$: Array[$N$] $\leftarrow$ `SolveLinearSystem`($(I - \delta T^{(a)})$, $r^{(a)}$)

3. **Return** $v^{(0)}$, $v^{(1)}$

---

**Procedure** `FitNetworksToValues`($S$: Tensor[$N \times 1$], $v^{(0)}$: Array[$N$], $v^{(1)}$: Array[$N$], hyperparameters: dict, num_epochs: int, learning_rate: float) $\to$ (Network, Network)

1. $v_\theta^{(0)}$: Network, $v_\theta^{(1)}$: Network $\leftarrow$ `InitializeNetworks`(hyperparameters)

2. optimizer: Optimizer $\leftarrow$ `CreateOptimizer`($[v_\theta^{(0)}, v_\theta^{(1)}]$, learning_rate)

3. targets: Tensor[$N \times 2$] $\leftarrow$ `Stack`($v^{(0)}$, $v^{(1)}$)

4. $v_\theta^{(0)}$, $v_\theta^{(1)}$ $\leftarrow$ `UpdateNetworks`($S$, targets, $v_\theta^{(0)}$, $v_\theta^{(1)}$, num_epochs, optimizer)

5. **Return** $v_\theta^{(0)}$, $v_\theta^{(1)}$

---

### Subroutines for Step 2c: Compute CCP from Value Functions

**Procedure** `ComputeCCPFromValue`($S$: Tensor[$N \times 1$], $v_\theta^{(0)}$: Network, $v_\theta^{(1)}$: Network) $\to$ Tensor[$N \times 1$]

1. $v_0$: Tensor[$N \times 1$] $\leftarrow v_\theta^{(0)}(S)$

2. $v_1$: Tensor[$N \times 1$] $\leftarrow v_\theta^{(1)}(S)$

3. $P^{\text{updated}}$: Tensor[$N \times 1$] $\leftarrow \frac{\exp(v_1)}{\exp(v_0) + \exp(v_1)}$ &nbsp;&nbsp;&nbsp;&nbsp;// Probability of $a=1$

4. **Return** $P^{\text{updated}}$

---

### Subroutines for Step 2d: Compute Distance

**Procedure** `ComputeDistance`($\hat{P}_{\text{eval}}$: Tensor[$N \times 1$], $P^{\text{updated}}$: Tensor[$N \times 1$]) $\to$ float

1. diff: Tensor[$N \times 1$] $\leftarrow \hat{P}_{\text{eval}} - P^{\text{updated}}$

2. squared_diff: Tensor[$N \times 1$] $\leftarrow$ diff $\odot$ diff &nbsp;&nbsp;&nbsp;&nbsp;// Element-wise square

3. distance: float $\leftarrow$ `Sum`(squared_diff)

4. **Return** distance

---

### Shared Functions from solve_mdp

The following functions are imported from solve_mdp and reused directly:

**Procedure** `ComputeNextState`($s$: Tensor, $a$: int, $\gamma$: float) $\to$ Tensor

- Returns $(1 - \gamma) \cdot s + a$
- See solve_mdp documentation for implementation details

**Procedure** `ComputeMeanReward`($s$: Tensor, $a$: int, $\beta$: float) $\to$ Tensor

- Returns $\beta \cdot \log(1 + s) - a$
- See solve_mdp documentation for implementation details

**Procedure** `InitializeNetworks`(hyperparameters: dict) $\to$ (Network, Network)

- See solve_mdp documentation for implementation details

**Procedure** `UpdateNetworks`($S$: Tensor, targets: Tensor, $v_\theta^{(0)}$: Network, $v_\theta^{(1)}$: Network, num_epochs: int, optimizer: Optimizer) $\to$ (Network, Network)

- See solve_mdp documentation for implementation details

**Procedure** `CheckConvergence`($S$: Tensor, targets: Tensor, $v_\theta^{(0)}$: Network, $v_\theta^{(1)}$: Network) $\to$ float

- See solve_mdp documentation for implementation details

:::

**Key Implementation Details:**

- **Shared configuration**: Load $\delta$, $\gamma_E$ from the same config used in solve_mdp
- **Standard network for CCP**: Use a standard neural network with **unconstrained weights** to flexibly fit $P(a=1|s)$ from observed choices. Output layer applies sigmoid $\sigma$ to constrain to $[0,1]$. Train on action indicator $1\{a=1\}$ using binary cross-entropy loss
- **Nested fixed-point**: The outer loop searches over $\beta$ candidates, inner loop solves value functions for each $\beta$
- **Binary cross-entropy loss**: For CCP estimation, use $-[a \log(\hat{p}) + (1-a) \log(1-\hat{p})]$ where $a = 1\{a=1\}$ and $\hat{p} = \hat{P}(s)$
- **Grid search for $\beta$**: Can be replaced with gradient-free optimization methods like Nelder-Mead

## Diagnostic: Compare Estimated CCP with True CCP

Before running the full estimation, we diagnose the quality of CCP estimation by comparing the estimated CCP (from observed choices) with the true CCP (from the solver's value functions).

```{python}
# Load solver configuration
solver_dir = Path('../../output/solve_mdp')
with open(solver_dir / 'config.json', 'r', encoding='utf-8') as f:
    solver_output = json.load(f)

solver_config = solver_output['solver']
hyperparameters = solver_config['hyperparameters']
N = solver_config['N']
state_range = solver_config['state_range']
num_epochs = solver_config['num_epochs']
learning_rate = solver_config['learning_rate']

print(f"\n=== DIAGNOSTIC: CCP Estimation Quality ===")

# Estimate CCP from observed (s,a) pairs
# EstimateCCP returns a network that predicts P(a=1|s)
print(f"\nEstimating CCP from observed choices...")
P_hat_network = EstimateCCP(
    states=states,
    actions=actions,
    hyperparameters=hyperparameters,
    num_epochs=num_epochs,
    learning_rate=learning_rate
)

# Load true value functions from solver
print(f"Loading true value functions from solver...")
v_theta_0_true = MonotonicNetwork(hidden_sizes=hyperparameters['hidden_sizes'])
v_theta_1_true = MonotonicNetwork(hidden_sizes=hyperparameters['hidden_sizes'])
v_theta_0_true.load_state_dict(torch.load(solver_dir / 'v_theta_0.pt', map_location='cpu'))
v_theta_1_true.load_state_dict(torch.load(solver_dir / 'v_theta_1.pt', map_location='cpu'))
v_theta_0_true.eval()
v_theta_1_true.eval()

# Generate evaluation grid
S_eval = GenerateStateGrid(N=N, state_range=state_range)

# Evaluate estimated CCP on grid
with torch.no_grad():
    P_hat_eval = P_hat_network(S_eval).numpy().flatten()  # P(a=1|s)

# Compute true CCP on grid from true value functions
prob_a1_true_list = []
for i in range(N):
    s_i = S_eval[i].item()
    prob_a0_t, prob_a1_t = ComputeChoiceProbability(
        s=s_i,
        v_theta_0=v_theta_0_true,
        v_theta_1=v_theta_1_true
    )
    prob_a1_true_list.append(prob_a1_t)

P_true_eval = np.array(prob_a1_true_list)

# Compute CCP errors
ccp_diff = np.abs(P_hat_eval - P_true_eval)

print(f"\n=== CCP Comparison ===")
print(f"Comparing: Estimated CCP (from data) vs. True CCP (from solver)")
print(f"\nP(a=1|s) errors:")
print(f"  Mean absolute error: {ccp_diff.mean():.6f}")
print(f"  Max absolute error:  {ccp_diff.max():.6f}")
print(f"  Root mean squared error: {np.sqrt((ccp_diff**2).mean()):.6f}")

# Plot CCP comparison
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# Plot 1: CCPs over state space
ax = axes[0]
s_grid = S_eval.numpy().flatten()
ax.plot(s_grid, P_true_eval, 'g-', linewidth=2, label='True CCP (solver)', alpha=0.8)
ax.plot(s_grid, P_hat_eval, 'r--', linewidth=2, label='Estimated CCP (from data)', alpha=0.8)
ax.set_xlabel('State (s)', fontsize=12)
ax.set_ylabel('P(a=1|s)', fontsize=12)
ax.set_title('Conditional Choice Probability', fontsize=14, fontweight='bold')
ax.legend(fontsize=11)
ax.grid(True, alpha=0.3)
ax.set_ylim([0, 1])

# Plot 2: Scatter plot (estimated vs true)
ax = axes[1]
ax.scatter(P_true_eval, P_hat_eval, alpha=0.6, s=30)
ax.plot([0, 1], [0, 1], 'k--', linewidth=2, label='45째 line')
ax.set_xlabel('True P(a=1|s)', fontsize=12)
ax.set_ylabel('Estimated P(a=1|s)', fontsize=12)
ax.set_title('CCP: Estimated vs True', fontsize=14, fontweight='bold')
ax.legend(fontsize=11)
ax.grid(True, alpha=0.3)
ax.set_xlim([0, 1])
ax.set_ylim([0, 1])
ax.set_aspect('equal')

plt.tight_layout()
plt.show()

print("\n=== Interpretation ===")
print("If CCP errors are LARGE: The CCP estimation is poor.")
print("If CCP errors are SMALL: CCP estimation is good; beta estimation issues are isolated.")
```

## Run Estimation

```{python}
#| eval: false

# Load solver configuration from solver output (for estimation hyperparameters)
solver_dir = Path('../../output/solve_mdp')
with open(solver_dir / 'config.json', 'r', encoding='utf-8') as f:
    solver_output = json.load(f)

solver_config = solver_output['solver']
hyperparameters = solver_config['hyperparameters']
N = solver_config['N']
epsilon_tol = solver_config['epsilon_tol']
max_iter = solver_config['max_iter']
num_epochs = solver_config['num_epochs']
learning_rate = solver_config['learning_rate']

# Load estimator configuration (estimation method parameters)
estimator_config = get_estimator_config()
beta_grid = np.linspace(
    estimator_config['beta_grid_min'],
    estimator_config['beta_grid_max'],
    estimator_config['beta_grid_points']
)

print(f"\nEstimation Configuration:")
print(f"  State grid size (N): {N}")
print(f"  Beta grid: {beta_grid}")
print(f"  Max iterations: {max_iter}")
print(f"  Tolerance: {epsilon_tol}")
print(f"  Training epochs per iteration: {num_epochs}")
print(f"  Learning rate: {learning_rate}")

# Run estimation
print(f"\nRunning estimation...")
gamma_hat, beta_hat, beta_grid_used, distances = EstimateMDP(
    states=states,
    actions=actions,
    delta=delta,
    N=N,
    state_range=state_range,
    hyperparameters=hyperparameters,
    num_epochs=num_epochs,
    learning_rate=learning_rate,
    beta_grid=beta_grid,
    epsilon_tol=epsilon_tol,
    max_iter=max_iter,
    gamma_E=gamma_E
)

print(f"\nEstimation Results:")
print(f"  Estimated gamma: {gamma_hat:.4f}")
print(f"  True gamma: {gamma_true:.4f}")
print(f"  Error: {abs(gamma_hat - gamma_true):.4f}")
print(f"\n  Estimated beta: {beta_hat:.4f}")
print(f"  True beta: {beta_true:.4f}")
print(f"  Error: {abs(beta_hat - beta_true):.4f}")

# Plot distance vs beta candidates
fig, ax = plt.subplots(figsize=(10, 6))
ax.plot(beta_grid_used, distances, 'b-o', linewidth=2, markersize=8, label='Distance')
ax.axvline(beta_true, color='g', linestyle='--', linewidth=2, label=f'True beta = {beta_true:.2f}')
ax.axvline(beta_hat, color='r', linestyle='--', linewidth=2, label=f'Estimated beta = {beta_hat:.2f}')
ax.set_xlabel('Beta Candidate', fontsize=12)
ax.set_ylabel('Distance (CCP mismatch)', fontsize=12)
ax.set_title('Beta Grid Search: Distance vs Beta Candidates', fontsize=14, fontweight='bold')
ax.legend(fontsize=11)
ax.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# Check if minimum is at or near true beta
min_idx = np.argmin(distances)
print(f"\nBeta Grid Search Diagnostics:")
print(f"  Minimum distance at beta = {beta_grid_used[min_idx]:.4f}")
print(f"  True beta = {beta_true:.4f}")
print(f"  Distance at true beta index: {distances[min_idx]:.6f}")
```

## Save Estimation Results

```{python}
#| eval: false

# Save estimation results
output_dir = Path('../../output/estimate_mdp')
output_dir.mkdir(parents=True, exist_ok=True)

# Save estimated parameters
est_results = {
    'gamma_hat': float(gamma_hat),
    'beta_hat': float(beta_hat),
    'gamma_true': float(gamma_true),
    'beta_true': float(beta_true),
    'gamma_error': float(abs(gamma_hat - gamma_true)),
    'beta_error': float(abs(beta_hat - beta_true)),
    'beta_grid': beta_grid.tolist(),
    'N': int(N),
    'epsilon_tol': float(epsilon_tol),
    'max_iter': int(max_iter)
}

with open(output_dir / 'estimation_results.json', 'w', encoding='utf-8') as f:
    json.dump(est_results, f, indent=2)

print(f"Saved estimation results to {output_dir.resolve()}")
```

## Compare Estimated and True Parameters

```{python}
#| eval: false

# Create comparison table
comparison_data = {
    'Parameter': ['gamma', 'beta'],
    'True Value': [gamma_true, beta_true],
    'Estimated Value': [gamma_hat, beta_hat],
    'Absolute Error': [abs(gamma_hat - gamma_true), abs(beta_hat - beta_true)],
    'Relative Error (%)': [
        100 * abs(gamma_hat - gamma_true) / gamma_true,
        100 * abs(beta_hat - beta_true) / beta_true
    ]
}

df_comparison = pd.DataFrame(comparison_data)
print("\nParameter Estimation Comparison:")
print(df_comparison.to_string(index=False))
```

## Compare Equilibrium Objects

Now we compare the estimated and true equilibrium objects: conditional choice probabilities (CCP) and value functions. We load the true value functions from the solver results and compute the implied equilibrium objects under both true and estimated parameters.

```{python}
#| eval: false

# Load true value functions from solver
solver_dir = Path('../../output/solve_mdp')
if not solver_dir.exists():
    raise FileNotFoundError(
        f"Expected solver results in {solver_dir}. Run solve_mdp.qmd first."
    )

# Load true networks
v_theta_0_true = MonotonicNetwork(hidden_sizes=hyperparameters['hidden_sizes'])
v_theta_1_true = MonotonicNetwork(hidden_sizes=hyperparameters['hidden_sizes'])

v_theta_0_true.load_state_dict(torch.load(solver_dir / 'v_theta_0.pt', map_location='cpu'))
v_theta_1_true.load_state_dict(torch.load(solver_dir / 'v_theta_1.pt', map_location='cpu'))

v_theta_0_true.eval()
v_theta_1_true.eval()

print("Loaded true value functions from solver")
```

```{python}
#| eval: false

from mdp_solver import GenerateStateGrid, SolveValueIteration

# Solve for estimated value functions using estimated parameters
print(f"\nSolving for value functions under estimated parameters...")
print(f"  Using gamma_hat={gamma_hat:.4f}, beta_hat={beta_hat:.4f}")

v_theta_0_est, v_theta_1_est, _ = SolveValueIteration(
    beta=beta_hat,
    gamma=gamma_hat,
    delta=delta,
    gamma_E=gamma_E,
    hyperparameters=hyperparameters,
    N=N,
    state_range=state_range,
    max_iter=max_iter,
    epsilon_tol=epsilon_tol,
    num_epochs=num_epochs,
    learning_rate=learning_rate
)

print("Solved value functions under estimated parameters")
```

```{python}
#| eval: false

# Generate evaluation grid
S_eval = GenerateStateGrid(N=N, state_range=state_range)

# Compute true CCPs and values
with torch.no_grad():
    # Compute value functions on grid
    v0_true = v_theta_0_true(S_eval)
    v1_true = v_theta_1_true(S_eval)
    v0_est = v_theta_0_est(S_eval)
    v1_est = v_theta_1_est(S_eval)

    # Compute choice probabilities for each state in the grid
    prob_a0_true_list = []
    prob_a1_true_list = []
    prob_a0_est_list = []
    prob_a1_est_list = []

    for i in range(N):
        s_i = S_eval[i].item()

        # True equilibrium probabilities
        prob_a0_t, prob_a1_t = ComputeChoiceProbability(
            s=s_i,
            v_theta_0=v_theta_0_true,
            v_theta_1=v_theta_1_true
        )
        prob_a0_true_list.append(prob_a0_t)
        prob_a1_true_list.append(prob_a1_t)

        # Estimated equilibrium probabilities
        prob_a0_e, prob_a1_e = ComputeChoiceProbability(
            s=s_i,
            v_theta_0=v_theta_0_est,
            v_theta_1=v_theta_1_est
        )
        prob_a0_est_list.append(prob_a0_e)
        prob_a1_est_list.append(prob_a1_e)

    # Convert to tensors
    prob_a0_true = torch.tensor(prob_a0_true_list).reshape(-1, 1)
    prob_a1_true = torch.tensor(prob_a1_true_list).reshape(-1, 1)
    prob_a0_est = torch.tensor(prob_a0_est_list).reshape(-1, 1)
    prob_a1_est = torch.tensor(prob_a1_est_list).reshape(-1, 1)

# Convert to numpy
s_grid = S_eval.numpy().flatten()
prob_a1_true_np = prob_a1_true.numpy().flatten()
prob_a1_est_np = prob_a1_est.numpy().flatten()
v0_true_np = v0_true.numpy().flatten()
v0_est_np = v0_est.numpy().flatten()
v1_true_np = v1_true.numpy().flatten()
v1_est_np = v1_est.numpy().flatten()

print(f"\nComputed equilibrium objects on grid of {N} points")
```

```{python}
#| eval: false

# Create comparison plots
fig, axes = plt.subplots(2, 2, figsize=(14, 12))

# Plot 1: CCP comparison (scatter with 45-degree line)
ax = axes[0, 0]
ax.scatter(prob_a1_true_np, prob_a1_est_np, alpha=0.6, s=20)
ax.plot([0, 1], [0, 1], 'k--', linewidth=2, label='45째 line')
ax.set_xlabel('True P(a=1|s)', fontsize=12)
ax.set_ylabel('Estimated P(a=1|s)', fontsize=12)
ax.set_title('Conditional Choice Probability Comparison', fontsize=14)
ax.legend()
ax.grid(True, alpha=0.3)
ax.set_xlim(0, 1)
ax.set_ylim(0, 1)
ax.set_aspect('equal')

# Compute CCP correlation
ccp_corr = np.corrcoef(prob_a1_true_np, prob_a1_est_np)[0, 1]
ccp_rmse = np.sqrt(np.mean((prob_a1_true_np - prob_a1_est_np)**2))
ax.text(0.05, 0.95, f'Corr: {ccp_corr:.4f}\nRMSE: {ccp_rmse:.4f}',
        transform=ax.transAxes, verticalalignment='top',
        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

# Plot 2: Value function v0 comparison
ax = axes[0, 1]
ax.scatter(v0_true_np, v0_est_np, alpha=0.6, s=20)
v0_min, v0_max = min(v0_true_np.min(), v0_est_np.min()), max(v0_true_np.max(), v0_est_np.max())
ax.plot([v0_min, v0_max], [v0_min, v0_max], 'k--', linewidth=2, label='45째 line')
ax.set_xlabel('True v(s, a=0)', fontsize=12)
ax.set_ylabel('Estimated v(s, a=0)', fontsize=12)
ax.set_title('Value Function (a=0) Comparison', fontsize=14)
ax.legend()
ax.grid(True, alpha=0.3)
ax.set_aspect('equal', adjustable='box')

v0_corr = np.corrcoef(v0_true_np, v0_est_np)[0, 1]
v0_rmse = np.sqrt(np.mean((v0_true_np - v0_est_np)**2))
ax.text(0.05, 0.95, f'Corr: {v0_corr:.4f}\nRMSE: {v0_rmse:.4f}',
        transform=ax.transAxes, verticalalignment='top',
        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

# Plot 3: Value function v1 comparison
ax = axes[1, 0]
ax.scatter(v1_true_np, v1_est_np, alpha=0.6, s=20)
v1_min, v1_max = min(v1_true_np.min(), v1_est_np.min()), max(v1_true_np.max(), v1_est_np.max())
ax.plot([v1_min, v1_max], [v1_min, v1_max], 'k--', linewidth=2, label='45째 line')
ax.set_xlabel('True v(s, a=1)', fontsize=12)
ax.set_ylabel('Estimated v(s, a=1)', fontsize=12)
ax.set_title('Value Function (a=1) Comparison', fontsize=14)
ax.legend()
ax.grid(True, alpha=0.3)
ax.set_aspect('equal', adjustable='box')

v1_corr = np.corrcoef(v1_true_np, v1_est_np)[0, 1]
v1_rmse = np.sqrt(np.mean((v1_true_np - v1_est_np)**2))
ax.text(0.05, 0.95, f'Corr: {v1_corr:.4f}\nRMSE: {v1_rmse:.4f}',
        transform=ax.transAxes, verticalalignment='top',
        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

# Plot 4: CCP as function of state
ax = axes[1, 1]
ax.plot(s_grid, prob_a1_true_np, 'b-', linewidth=2, label='True', alpha=0.7)
ax.plot(s_grid, prob_a1_est_np, 'r--', linewidth=2, label='Estimated', alpha=0.7)
ax.set_xlabel('State (s)', fontsize=12)
ax.set_ylabel('P(a=1|s)', fontsize=12)
ax.set_title('CCP as Function of State', fontsize=14)
ax.legend()
ax.grid(True, alpha=0.3)
ax.set_ylim(0, 1)

fig.tight_layout()
plt.show()

print(f"\nEquilibrium Object Comparison:")
print(f"  CCP - Correlation: {ccp_corr:.4f}, RMSE: {ccp_rmse:.4f}")
print(f"  V0  - Correlation: {v0_corr:.4f}, RMSE: {v0_rmse:.4f}")
print(f"  V1  - Correlation: {v1_corr:.4f}, RMSE: {v1_rmse:.4f}")
```
