---
title: "Estimating Markov Decision Processes"
format:
  html:
    toc: true
    code-fold: false
jupyter: python3
execute:
  daemon: false
---

## Overview

This document loads the configuration and simulated data from the previous steps to estimate the MDP parameters.

## Load Configuration and Simulated Data

```{python}
import sys
sys.path.insert(0, '..')
sys.path.insert(0, '../../src')

import json
from pathlib import Path

import numpy as np
import torch
import matplotlib.pyplot as plt

# Load configuration and simulated data from simulator output
artifact_dir = Path('../../output/simulate_mdp')
if not artifact_dir.exists():
    raise FileNotFoundError(
        f"Expected simulation artifacts in {artifact_dir}. Run simulate_mdp.qmd first."
    )

# Load configuration from simulator output (as data from previous step)
with open(artifact_dir / 'config.json', 'r', encoding='utf-8') as config_file:
    config = json.load(config_file)

model_config = config['model']
simulator_config = config['simulator']

# Extract model parameters (true values used for simulation)
beta = model_config['beta']
gamma = model_config['gamma']
delta = model_config['delta']
gamma_E = model_config['gamma_E']

# Extract simulator parameters
M = simulator_config['M']
T = simulator_config['T']
seed = simulator_config['seed']

print(f"Loaded configuration:")
print(f"  Model parameters: β={beta}, γ={gamma}, δ={delta}")
print(f"  Simulation: M={M} paths, T={T} periods, seed={seed}")

# Load simulated data
simulation_data = np.load(artifact_dir / 'simulation_results.npz')
states = simulation_data['states']
actions = simulation_data['actions']
rewards = simulation_data['rewards']
state_range = tuple(simulation_data['state_range'])

print(f"\nLoaded simulated data:")
print(f"  States shape: {states.shape}")
print(f"  Actions shape: {actions.shape}")
print(f"  Rewards shape: {rewards.shape}")
print(f"  State range: {state_range}")
```

## Data Summary

```{python}
# Basic summary statistics
print("\nData Summary Statistics:")
print(f"\nStates:")
print(f"  Mean: {states.mean():.4f}")
print(f"  Std: {states.std():.4f}")
print(f"  Min: {states.min():.4f}")
print(f"  Max: {states.max():.4f}")

print(f"\nActions:")
print(f"  Fraction a=1: {actions.mean():.4f}")
print(f"  Total observations: {actions.size}")

print(f"\nRewards:")
print(f"  Mean: {rewards.mean():.4f}")
print(f"  Std: {rewards.std():.4f}")
print(f"  Min: {rewards.min():.4f}")
print(f"  Max: {rewards.max():.4f}")

# Plot data distributions
fig, axes = plt.subplots(1, 3, figsize=(15, 4))

# States distribution
ax = axes[0]
ax.hist(states.flatten(), bins=50, alpha=0.7, edgecolor='black')
ax.set_xlabel('State value')
ax.set_ylabel('Frequency')
ax.set_title('Distribution of States')
ax.grid(True, alpha=0.3)

# Actions distribution
ax = axes[1]
action_counts = np.bincount(actions.flatten().astype(int))
ax.bar([0, 1], action_counts, alpha=0.7, edgecolor='black')
ax.set_xlabel('Action')
ax.set_ylabel('Count')
ax.set_title('Distribution of Actions')
ax.set_xticks([0, 1])
ax.grid(True, alpha=0.3)

# Rewards distribution
ax = axes[2]
ax.hist(rewards.flatten(), bins=50, alpha=0.7, edgecolor='black')
ax.set_xlabel('Reward value')
ax.set_ylabel('Frequency')
ax.set_title('Distribution of Rewards')
ax.grid(True, alpha=0.3)

fig.tight_layout()
plt.show()
```

## Nested Fixed Point Estimation Algorithm

We estimate the MDP parameters $\beta$ (reward weight on state) and $\gamma$ (state persistence) using a nested fixed point (NFXP) algorithm, treating the discount factor $\delta$ as known.

### Model Specification

**Observed Data**: We observe panel data $\{(s_{mt}, a_{mt})\}_{t=1}^{T}$ for $m = 1, \ldots, M$ paths, where:
- $s_{mt}$: state for path $m$ at time $t$
- $a_{mt} \in \{0, 1\}$: action chosen for path $m$ at time $t$

**Structural Model**: The data are generated by the following MDP:

- **State transition**: $s_{t+1} = (1 - \gamma) s_t + a_t$
- **Mean reward**: $\bar{r}(s, a) = \beta \log(1 + s) - a$
- **Choice probabilities**:
$$P(a | s; \beta, \gamma, \delta) = \frac{\exp(v(s, a; \beta, \gamma, \delta))}{\sum_{a' \in \{0,1\}} \exp(v(s, a'; \beta, \gamma, \delta))}$$
where $v(s, a; \beta, \gamma, \delta)$ is the choice-specific value function that solves the Bellman equation.

**Parameters to Estimate**: $\theta = (\beta, \gamma)$

**Known Parameters**: $\delta$ (discount factor), $\gamma_E$ (Euler-Mascheroni constant)

### Two-Step Estimation Procedure

#### Step 1: Estimate State Transition Parameter $\gamma$

The state transition equation $s_{t+1} = (1 - \gamma) s_t + a_t$ is **linear** in the parameters and does **not depend on $\beta$**. Therefore, we can estimate $\gamma$ directly from the observed state transitions using ordinary least squares (OLS).

**Estimation Equation**:
$$s_{t+1} = (1 - \gamma) s_t + a_t + \epsilon_t$$

where $\epsilon_t$ represents measurement or approximation error in the state transition.

**Estimator**: Stack all observations $(s_{mt}, a_{mt}, s_{m,t+1})$ across paths and time periods, and run OLS regression:
$$\hat{\gamma} = \arg\min_{\gamma} \sum_{m=1}^{M} \sum_{t=1}^{T-1} \left(s_{m,t+1} - (1-\gamma) s_{mt} - a_{mt}\right)^2$$

This can be solved in closed form:
$$\hat{\gamma} = 1 - \frac{\sum_{m,t} s_{mt}(s_{m,t+1} - a_{mt})}{\sum_{m,t} s_{mt}^2}$$

**Advantages**:
- Simple, fast, and has a closed-form solution
- Does not require solving the MDP
- Consistent estimator under standard regularity conditions

#### Step 2: Estimate Reward Parameter $\beta$ via Maximum Likelihood

Given the estimated $\hat{\gamma}$ from Step 1, we estimate $\beta$ by maximizing the likelihood of the observed action choices.

**Conditional Choice Probability (CCP)**: For a given parameter value $\beta$, the probability of observing action $a$ in state $s$ is:
$$P(a | s; \beta, \hat{\gamma}, \delta) = \frac{\exp(v(s, a; \beta, \hat{\gamma}, \delta))}{\sum_{a' \in \{0,1\}} \exp(v(s, a'; \beta, \hat{\gamma}, \delta))}$$

**Computing the CCP**: To evaluate $P(a | s; \beta, \hat{\gamma}, \delta)$, we must:
1. Solve the Bellman equation for the choice-specific value functions $v(s, 0; \beta, \hat{\gamma}, \delta)$ and $v(s, 1; \beta, \hat{\gamma}, \delta)$ using the value iteration algorithm from [solve_mdp.qmd](../solve_mdp/solve_mdp.qmd)
2. Compute the logit probabilities using the solved value functions

**Log-Likelihood Function**: The log-likelihood of the observed data is:
$$\ell(\beta; \hat{\gamma}) = \sum_{m=1}^{M} \sum_{t=1}^{T} \log P(a_{mt} | s_{mt}; \beta, \hat{\gamma}, \delta)$$

**Maximum Likelihood Estimator**:
$$\hat{\beta} = \arg\max_{\beta} \ell(\beta; \hat{\gamma}) = \arg\max_{\beta} \sum_{m=1}^{M} \sum_{t=1}^{T} \log P(a_{mt} | s_{mt}; \beta, \hat{\gamma}, \delta)$$

**Nested Fixed Point Structure**: For each candidate value of $\beta$:
1. **Inner loop (Fixed Point)**: Solve the MDP using value iteration to obtain $v(s, a; \beta, \hat{\gamma}, \delta)$
2. **Compute likelihood**: Evaluate $P(a_{mt} | s_{mt}; \beta, \hat{\gamma}, \delta)$ for all observations and compute $\ell(\beta; \hat{\gamma})$
3. **Outer loop (Optimization)**: Use numerical optimization (e.g., Nelder-Mead, BFGS) to search for $\beta$ that maximizes $\ell(\beta; \hat{\gamma})$

### Pseudo Code

::: {.callout-note appearance="simple"}

## Algorithm: Nested Fixed Point Estimation

**Design Principles:**

This pseudo code is designed for direct implementation:

- **No placeholders**: Every operation corresponds to a concrete function or method call
- **Explicit parameters**: All inputs are passed as parameters, no implicit/global state
- **Reuses shared functions**: Uses `SolveValueIteration`, `ComputeChoiceProbability`, `ComputeNextState`, `ComputeMeanReward` from solve_mdp
- **No hard-coding**: All values are parameterized

**Input:**

- Observed data: states: Array[$M \times T$], actions: Array[$M \times T$]
- Known parameters: $\delta$: float, $\gamma_E$: float
- MDP solver configuration: solver_config: dict (containing hyperparameters, $N$, state_range, max_iter, $\epsilon_{\text{tol}}$, num_epochs, learning_rate)
- Optimization configuration: optimization_config: dict (containing beta_bounds: tuple[float, float], method: str, tolerance: float)

**Output:**

- Estimated parameters: $\hat{\beta}$: float, $\hat{\gamma}$: float

---

### Main Estimation Procedure

**Procedure** `EstimateMDP`(states: Array[$M \times T$], actions: Array[$M \times T$], $\delta$: float, $\gamma_E$: float, solver_config: dict, optimization_config: dict) $\to$ (float, float)

1. $\hat{\gamma}$: float $\leftarrow$ `EstimateGamma`(states, actions)

2. $\hat{\beta}$: float $\leftarrow$ `EstimateBeta`(states, actions, $\hat{\gamma}$, $\delta$, $\gamma_E$, solver_config, optimization_config)

3. **Return** $\hat{\beta}$, $\hat{\gamma}$

---

### Subroutines

**Procedure** `EstimateGamma`(states: Array[$M \times T$], actions: Array[$M \times T$]) $\to$ float

1. Initialize sums: $\text{numer}$: float $\leftarrow 0$, $\text{denom}$: float $\leftarrow 0$

2. For $m$: int = 1 to $M$:

   For $t$: int = 1 to $T-1$:

   &nbsp;&nbsp;&nbsp;&nbsp;$s_t$: float $\leftarrow$ states[$m$, $t$]

   &nbsp;&nbsp;&nbsp;&nbsp;$a_t$: int $\leftarrow$ actions[$m$, $t$]

   &nbsp;&nbsp;&nbsp;&nbsp;$s_{t+1}$: float $\leftarrow$ states[$m$, $t+1$]

   &nbsp;&nbsp;&nbsp;&nbsp;$\text{numer} \leftarrow \text{numer} + s_t \cdot (s_{t+1} - a_t)$

   &nbsp;&nbsp;&nbsp;&nbsp;$\text{denom} \leftarrow \text{denom} + s_t^2$

3. $\hat{\gamma}$: float $\leftarrow 1 - \text{numer} / \text{denom}$

4. **Return** $\hat{\gamma}$

---

**Procedure** `EstimateBeta`(states: Array[$M \times T$], actions: Array[$M \times T$], $\hat{\gamma}$: float, $\delta$: float, $\gamma_E$: float, solver_config: dict, optimization_config: dict) $\to$ float

1. Define log-likelihood function:

   **Procedure** `LogLikelihood`($\beta$: float) $\to$ float

   a. Solve MDP using value iteration (reuses solver from solve_mdp):

   &nbsp;&nbsp;&nbsp;&nbsp;$v_\theta^{(0)}$: Network, $v_\theta^{(1)}$: Network $\leftarrow$ `SolveValueIteration`($\beta$, $\hat{\gamma}$, $\delta$, $\gamma_E$, solver_config)

   b. Initialize log-likelihood: $\ell$: float $\leftarrow 0$

   c. For $m$: int = 1 to $M$:

   &nbsp;&nbsp;&nbsp;&nbsp;For $t$: int = 1 to $T$:

   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$s_{mt}$: float $\leftarrow$ states[$m$, $t$]

   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$a_{mt}$: int $\leftarrow$ actions[$m$, $t$]

   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$P_0$: float, $P_1$: float $\leftarrow$ `ComputeChoiceProbability`($s_{mt}$, $v_\theta^{(0)}$, $v_\theta^{(1)}$)

   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**If** $a_{mt} = 0$: $\ell \leftarrow \ell + \log(P_0)$

   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Else**: $\ell \leftarrow \ell + \log(P_1)$

   d. **Return** $\ell$

2. Extract optimization parameters:

   beta_bounds: tuple[float, float] $\leftarrow$ optimization_config['beta_bounds']

   method: str $\leftarrow$ optimization_config['method']

   tolerance: float $\leftarrow$ optimization_config['tolerance']

3. Maximize log-likelihood over $\beta$ using numerical optimization:

   result: OptimizationResult $\leftarrow$ `scipy.optimize.minimize`(

   &nbsp;&nbsp;&nbsp;&nbsp;fun=lambda $\beta$: $-$`LogLikelihood`($\beta$),

   &nbsp;&nbsp;&nbsp;&nbsp;x0=initial_beta,

   &nbsp;&nbsp;&nbsp;&nbsp;method=method,

   &nbsp;&nbsp;&nbsp;&nbsp;bounds=[beta_bounds],

   &nbsp;&nbsp;&nbsp;&nbsp;options={'ftol': tolerance}

   )

4. $\hat{\beta}$: float $\leftarrow$ result.x[0]

5. **Return** $\hat{\beta}$

---

### Shared Functions from solve_mdp

The following functions are imported from solve_mdp and reused directly to ensure DRY:

**Procedure** `SolveValueIteration`($\beta$: float, $\gamma$: float, $\delta$: float, $\gamma_E$: float, solver_config: dict) $\to$ (Network, Network)

- Solves the MDP using neural network value iteration
- Returns trained networks $v_\theta^{(0)}$ and $v_\theta^{(1)}$
- See solve_mdp documentation for implementation details

**Procedure** `ComputeChoiceProbability`($s$: float, $v_\theta^{(0)}$: Network, $v_\theta^{(1)}$: Network) $\to$ (float, float)

- Computes $P(a=0|s)$ and $P(a=1|s)$ using logit formula:
  $$P(a|s) = \frac{\exp(v(s,a))}{\sum_{a'} \exp(v(s,a'))}$$
- See solve_mdp documentation for implementation details

**Procedure** `ComputeNextState`($s$: float, $a$: int, $\gamma$: float) $\to$ float

- Returns $(1 - \gamma) \cdot s + a$
- Used for validation and diagnostics
- See solve_mdp documentation for implementation details

**Procedure** `ComputeMeanReward`($s$: float, $a$: int, $\beta$: float) $\to$ float

- Returns $\beta \cdot \log(1 + s) - a$
- Used for validation and diagnostics
- See solve_mdp documentation for implementation details

:::

### Key Implementation Details

**Step 1 (Gamma Estimation)**:
- Use vectorized operations for efficiency
- Check that $\hat{\gamma} \in (0, 1)$ for economic validity (positive persistence, stationarity)
- Can compute standard errors using OLS formula if needed

**Step 2 (Beta Estimation)**:
- **Computational challenge**: Each evaluation of the log-likelihood requires solving the entire MDP via value iteration (inner fixed point)
- **Optimization method**: Use derivative-free methods (e.g., Nelder-Mead) since computing gradients would require differentiating through the value iteration algorithm
- **Bounds**: Constrain $\beta > 0$ to ensure economic interpretability (positive value of state)
- **Initial value**: Use a reasonable starting point (e.g., $\beta = 1$) or grid search to avoid local optima
- **Convergence**: The MDP solver must converge to high precision at each iteration for accurate likelihood evaluation

**Numerical Stability**:
- Use log-sum-exp trick when computing choice probabilities to avoid numerical overflow
- Monitor MDP solver convergence at each likelihood evaluation
- Consider caching MDP solutions if the same $\beta$ is evaluated multiple times

**Statistical Properties**:
- Under standard regularity conditions, $(\hat{\beta}, \hat{\gamma})$ is consistent and asymptotically normal
- Step 1 is $\sqrt{MT}$-consistent since it's a linear regression
- Step 2 is also $\sqrt{MT}$-consistent but asymptotic variance accounts for estimation error in $\hat{\gamma}$
- Standard errors can be computed using the outer product of the gradient (OPG) or bootstrapping
