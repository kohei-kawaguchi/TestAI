---
title: "Estimating Markov Decision Processes"
format:
  html:
    toc: true
    code-fold: false
jupyter: python3
execute:
  daemon: false
---

## Overview

This document loads the configuration and simulated data from the previous steps to estimate the MDP parameters.

## Load Configuration and Simulated Data

```{python}
import sys
sys.path.insert(0, '..')
sys.path.insert(0, '../../src')

import json
from pathlib import Path

import numpy as np
import torch
import matplotlib.pyplot as plt

# Load configuration and simulated data from simulator output
artifact_dir = Path('../../output/simulate_mdp')
if not artifact_dir.exists():
    raise FileNotFoundError(
        f"Expected simulation artifacts in {artifact_dir}. Run simulate_mdp.qmd first."
    )

# Load configuration from simulator output (as data from previous step)
with open(artifact_dir / 'config.json', 'r', encoding='utf-8') as config_file:
    config = json.load(config_file)

model_config = config['model']
simulator_config = config['simulator']

# Extract model parameters (true values used for simulation)
beta = model_config['beta']
gamma = model_config['gamma']
delta = model_config['delta']
gamma_E = model_config['gamma_E']

# Extract simulator parameters
M = simulator_config['M']
T = simulator_config['T']
seed = simulator_config['seed']

print(f"Loaded configuration:")
print(f"  Model parameters: β={beta}, γ={gamma}, δ={delta}")
print(f"  Simulation: M={M} paths, T={T} periods, seed={seed}")

# Load simulated data
simulation_data = np.load(artifact_dir / 'simulation_results.npz')
states = simulation_data['states']
actions = simulation_data['actions']
rewards = simulation_data['rewards']
state_range = tuple(simulation_data['state_range'])

print(f"\nLoaded simulated data:")
print(f"  States shape: {states.shape}")
print(f"  Actions shape: {actions.shape}")
print(f"  Rewards shape: {rewards.shape}")
print(f"  State range: {state_range}")
```

## Data Summary

```{python}
# Basic summary statistics
print("\nData Summary Statistics:")
print(f"\nStates:")
print(f"  Mean: {states.mean():.4f}")
print(f"  Std: {states.std():.4f}")
print(f"  Min: {states.min():.4f}")
print(f"  Max: {states.max():.4f}")

print(f"\nActions:")
print(f"  Fraction a=1: {actions.mean():.4f}")
print(f"  Total observations: {actions.size}")

print(f"\nRewards:")
print(f"  Mean: {rewards.mean():.4f}")
print(f"  Std: {rewards.std():.4f}")
print(f"  Min: {rewards.min():.4f}")
print(f"  Max: {rewards.max():.4f}")

# Plot data distributions
fig, axes = plt.subplots(1, 3, figsize=(15, 4))

# States distribution
ax = axes[0]
ax.hist(states.flatten(), bins=50, alpha=0.7, edgecolor='black')
ax.set_xlabel('State value')
ax.set_ylabel('Frequency')
ax.set_title('Distribution of States')
ax.grid(True, alpha=0.3)

# Actions distribution
ax = axes[1]
action_counts = np.bincount(actions.flatten().astype(int))
ax.bar([0, 1], action_counts, alpha=0.7, edgecolor='black')
ax.set_xlabel('Action')
ax.set_ylabel('Count')
ax.set_title('Distribution of Actions')
ax.set_xticks([0, 1])
ax.grid(True, alpha=0.3)

# Rewards distribution
ax = axes[2]
ax.hist(rewards.flatten(), bins=50, alpha=0.7, edgecolor='black')
ax.set_xlabel('Reward value')
ax.set_ylabel('Frequency')
ax.set_title('Distribution of Rewards')
ax.grid(True, alpha=0.3)

fig.tight_layout()
plt.show()
```

## Nested Fixed Point Estimation Algorithm

We estimate the MDP parameters $\beta$ (reward weight on state) and $\gamma$ (state persistence) using a nested fixed point (NFXP) algorithm, treating the discount factor $\delta$ as known.

### Model Specification

**Observed Data**: We observe panel data $\{(s_{mt}, a_{mt})\}_{t=1}^{T}$ for $m = 1, \ldots, M$ paths, where:
- $s_{mt}$: state for path $m$ at time $t$
- $a_{mt} \in \{0, 1\}$: action chosen for path $m$ at time $t$

**Structural Model**: The data are generated by the following MDP:

- **State transition**: $s_{t+1} = (1 - \gamma) s_t + a_t$
- **Mean reward**: $\bar{r}(s, a) = \beta \log(1 + s) - a$
- **Choice probabilities**:
$$P(a | s; \beta, \gamma, \delta) = \frac{\exp(v(s, a; \beta, \gamma, \delta))}{\sum_{a' \in \{0,1\}} \exp(v(s, a'; \beta, \gamma, \delta))}$$
where $v(s, a; \beta, \gamma, \delta)$ is the choice-specific value function that solves the Bellman equation.

**Parameters to Estimate**: $\theta = (\beta, \gamma)$

**Known Parameters**: $\delta$ (discount factor), $\gamma_E$ (Euler-Mascheroni constant)

### Two-Step Estimation Procedure

#### Step 1: Estimate State Transition Parameter $\gamma$

The state transition equation $s_{t+1} = (1 - \gamma) s_t + a_t$ is **linear** in the parameters and does **not depend on $\beta$**. Therefore, we can estimate $\gamma$ directly from the observed state transitions using ordinary least squares (OLS).

**Estimation Equation**:
$$s_{t+1} = (1 - \gamma) s_t + a_t + \epsilon_t$$

where $\epsilon_t$ represents measurement or approximation error in the state transition.

**Estimator**: Stack all observations $(s_{mt}, a_{mt}, s_{m,t+1})$ across paths and time periods, and run OLS regression:
$$\hat{\gamma} = \arg\min_{\gamma} \sum_{m=1}^{M} \sum_{t=1}^{T-1} \left(s_{m,t+1} - (1-\gamma) s_{mt} - a_{mt}\right)^2$$

This can be solved in closed form:
$$\hat{\gamma} = 1 - \frac{\sum_{m,t} s_{mt}(s_{m,t+1} - a_{mt})}{\sum_{m,t} s_{mt}^2}$$

**Advantages**:
- Simple, fast, and has a closed-form solution
- Does not require solving the MDP
- Consistent estimator under standard regularity conditions

#### Step 2: Estimate Reward Parameter $\beta$ via Maximum Likelihood

Given the estimated $\hat{\gamma}$ from Step 1, we estimate $\beta$ by maximizing the likelihood of the observed action choices.

**Conditional Choice Probability (CCP)**: For a given parameter value $\beta$, the probability of observing action $a$ in state $s$ is:
$$P(a | s; \beta, \hat{\gamma}, \delta) = \frac{\exp(v(s, a; \beta, \hat{\gamma}, \delta))}{\sum_{a' \in \{0,1\}} \exp(v(s, a'; \beta, \hat{\gamma}, \delta))}$$

**Computing the CCP**: To evaluate $P(a | s; \beta, \hat{\gamma}, \delta)$, we must:
1. Solve the Bellman equation for the choice-specific value functions $v(s, 0; \beta, \hat{\gamma}, \delta)$ and $v(s, 1; \beta, \hat{\gamma}, \delta)$ using the value iteration algorithm from [solve_mdp.qmd](../solve_mdp/solve_mdp.qmd)
2. Compute the logit probabilities using the solved value functions

**Log-Likelihood Function**: The log-likelihood of the observed data is:
$$\ell(\beta; \hat{\gamma}) = \sum_{m=1}^{M} \sum_{t=1}^{T} \log P(a_{mt} | s_{mt}; \beta, \hat{\gamma}, \delta)$$

**Maximum Likelihood Estimator**:
$$\hat{\beta} = \arg\max_{\beta} \ell(\beta; \hat{\gamma}) = \arg\max_{\beta} \sum_{m=1}^{M} \sum_{t=1}^{T} \log P(a_{mt} | s_{mt}; \beta, \hat{\gamma}, \delta)$$

**Nested Fixed Point Structure**: For each candidate value of $\beta$:
1. **Inner loop (Fixed Point)**: Solve the MDP using value iteration to obtain $v(s, a; \beta, \hat{\gamma}, \delta)$
2. **Compute likelihood**: Evaluate $P(a_{mt} | s_{mt}; \beta, \hat{\gamma}, \delta)$ for all observations and compute $\ell(\beta; \hat{\gamma})$
3. **Outer loop (Optimization)**: Use numerical optimization (e.g., Nelder-Mead, BFGS) to search for $\beta$ that maximizes $\ell(\beta; \hat{\gamma})$

### Pseudo Code

::: {.callout-note appearance="simple"}

## Algorithm: Nested Fixed Point Estimation

**Design Principles:**

This pseudo code is designed for direct implementation:

- **No placeholders**: Every operation corresponds to a concrete function or method call
- **Explicit parameters**: All inputs are passed as parameters, no implicit/global state
- **Reuses shared functions**: Uses `SolveValueIteration`, `ComputeChoiceProbability`, `ComputeNextState`, `ComputeMeanReward` from solve_mdp
- **No hard-coding**: All values are parameterized

**Input:**

- Observed data: states: Array[$M \times T$], actions: Array[$M \times T$]
- Known parameters: $\delta$: float, $\gamma_E$: float
- MDP solver configuration: solver_config: dict (containing hyperparameters, $N$, state_range, max_iter, $\epsilon_{\text{tol}}$, num_epochs, learning_rate)
- Optimization configuration: optimization_config: dict (containing beta_bounds: tuple[float, float], method: str, tolerance: float)

**Output:**

- Estimated parameters: $\hat{\beta}$: float, $\hat{\gamma}$: float

---

### Main Estimation Procedure

**Procedure** `EstimateMDP`(states: Array[$M \times T$], actions: Array[$M \times T$], $\delta$: float, $\gamma_E$: float, solver_config: dict, optimization_config: dict) $\to$ (float, float)

1. $\hat{\gamma}$: float $\leftarrow$ `EstimateGamma`(states, actions)

2. $\hat{\beta}$: float $\leftarrow$ `EstimateBeta`(states, actions, $\hat{\gamma}$, $\delta$, $\gamma_E$, solver_config, optimization_config)

3. **Return** $\hat{\beta}$, $\hat{\gamma}$

---

### Subroutines

**Procedure** `EstimateGamma`(states: Array[$M \times T$], actions: Array[$M \times T$]) $\to$ float

1. Initialize sums: $\text{numer}$: float $\leftarrow 0$, $\text{denom}$: float $\leftarrow 0$

2. For $m$: int = 1 to $M$:

   For $t$: int = 1 to $T-1$:

   &nbsp;&nbsp;&nbsp;&nbsp;$s_t$: float $\leftarrow$ states[$m$, $t$]

   &nbsp;&nbsp;&nbsp;&nbsp;$a_t$: int $\leftarrow$ actions[$m$, $t$]

   &nbsp;&nbsp;&nbsp;&nbsp;$s_{t+1}$: float $\leftarrow$ states[$m$, $t+1$]

   &nbsp;&nbsp;&nbsp;&nbsp;$\text{numer} \leftarrow \text{numer} + s_t \cdot (s_{t+1} - a_t)$

   &nbsp;&nbsp;&nbsp;&nbsp;$\text{denom} \leftarrow \text{denom} + s_t^2$

3. $\hat{\gamma}$: float $\leftarrow 1 - \text{numer} / \text{denom}$

4. **Return** $\hat{\gamma}$

---

**Procedure** `EstimateBeta`(states: Array[$M \times T$], actions: Array[$M \times T$], $\hat{\gamma}$: float, $\delta$: float, $\gamma_E$: float, solver_config: dict, optimization_config: dict) $\to$ float

1. Define log-likelihood function:

   **Procedure** `LogLikelihood`($\beta$: float) $\to$ float

   a. Solve MDP using value iteration (reuses solver from solve_mdp):

   &nbsp;&nbsp;&nbsp;&nbsp;$v_\theta^{(0)}$: Network, $v_\theta^{(1)}$: Network $\leftarrow$ `SolveValueIteration`($\beta$, $\hat{\gamma}$, $\delta$, $\gamma_E$, solver_config)

   b. Initialize log-likelihood: $\ell$: float $\leftarrow 0$

   c. For $m$: int = 1 to $M$:

   &nbsp;&nbsp;&nbsp;&nbsp;For $t$: int = 1 to $T$:

   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$s_{mt}$: float $\leftarrow$ states[$m$, $t$]

   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$a_{mt}$: int $\leftarrow$ actions[$m$, $t$]

   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$P_0$: float, $P_1$: float $\leftarrow$ `ComputeChoiceProbability`($s_{mt}$, $v_\theta^{(0)}$, $v_\theta^{(1)}$)

   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**If** $a_{mt} = 0$: $\ell \leftarrow \ell + \log(P_0)$

   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Else**: $\ell \leftarrow \ell + \log(P_1)$

   d. **Return** $\ell$

2. Extract optimization parameters:

   beta_bounds: tuple[float, float] $\leftarrow$ optimization_config['beta_bounds']

   method: str $\leftarrow$ optimization_config['method']

   tolerance: float $\leftarrow$ optimization_config['tolerance']

   initial_beta: float $\leftarrow$ optimization_config['initial_beta']

3. Maximize log-likelihood over $\beta$ using numerical optimization:

   result: OptimizationResult $\leftarrow$ `scipy.optimize.minimize`(

   &nbsp;&nbsp;&nbsp;&nbsp;fun=lambda $\beta$: $-$`LogLikelihood`($\beta$),

   &nbsp;&nbsp;&nbsp;&nbsp;x0=initial_beta,

   &nbsp;&nbsp;&nbsp;&nbsp;method=method,

   &nbsp;&nbsp;&nbsp;&nbsp;bounds=[beta_bounds],

   &nbsp;&nbsp;&nbsp;&nbsp;options={'ftol': tolerance}

   )

4. $\hat{\beta}$: float $\leftarrow$ result.x[0]

5. **Return** $\hat{\beta}$

---

### Shared Functions from solve_mdp

The following functions are imported from solve_mdp and reused directly to ensure DRY:

**Procedure** `SolveValueIteration`($\beta$: float, $\gamma$: float, $\delta$: float, $\gamma_E$: float, solver_config: dict) $\to$ (Network, Network)

- Solves the MDP using neural network value iteration
- Returns trained networks $v_\theta^{(0)}$ and $v_\theta^{(1)}$
- See solve_mdp documentation for implementation details

**Procedure** `ComputeChoiceProbability`($s$: float, $v_\theta^{(0)}$: Network, $v_\theta^{(1)}$: Network) $\to$ (float, float)

- Computes $P(a=0|s)$ and $P(a=1|s)$ using logit formula:
  $$P(a|s) = \frac{\exp(v(s,a))}{\sum_{a'} \exp(v(s,a'))}$$
- See solve_mdp documentation for implementation details

**Procedure** `ComputeNextState`($s$: float, $a$: int, $\gamma$: float) $\to$ float

- Returns $(1 - \gamma) \cdot s + a$
- Used for validation and diagnostics
- See solve_mdp documentation for implementation details

**Procedure** `ComputeMeanReward`($s$: float, $a$: int, $\beta$: float) $\to$ float

- Returns $\beta \cdot \log(1 + s) - a$
- Used for validation and diagnostics
- See solve_mdp documentation for implementation details

:::

### Key Implementation Details

**Step 1 (Gamma Estimation)**:
- Use vectorized operations for efficiency
- Check that $\hat{\gamma} \in (0, 1)$ for economic validity (positive persistence, stationarity)
- Can compute standard errors using OLS formula if needed

**Step 2 (Beta Estimation)**:
- **Computational challenge**: Each evaluation of the log-likelihood requires solving the entire MDP via value iteration (inner fixed point)
- **Optimization method**: Use derivative-free methods (e.g., Nelder-Mead) since computing gradients would require differentiating through the value iteration algorithm
- **Bounds**: Constrain $\beta > 0$ to ensure economic interpretability (positive value of state)
- **Initial value**: Use a reasonable starting point (e.g., $\beta = 1$) or grid search to avoid local optima
- **Convergence**: The MDP solver must converge to high precision at each iteration for accurate likelihood evaluation

**Numerical Stability**:
- Use log-sum-exp trick when computing choice probabilities to avoid numerical overflow
- Monitor MDP solver convergence at each likelihood evaluation
- Consider caching MDP solutions if the same $\beta$ is evaluated multiple times

**Statistical Properties**:
- Under standard regularity conditions, $(\hat{\beta}, \hat{\gamma})$ is consistent and asymptotically normal
- Step 1 is $\sqrt{MT}$-consistent since it's a linear regression
- Step 2 is also $\sqrt{MT}$-consistent but asymptotic variance accounts for estimation error in $\hat{\gamma}$
- Standard errors can be computed using the outer product of the gradient (OPG) or bootstrapping

## Diagnostic: Likelihood Around True Parameter

Before running the full estimation (which is computationally expensive), let's verify that the likelihood function is correctly specified by evaluating it around the true parameter values.

### Load Estimator Functions

```{python}
from mdp_estimator import EstimateGamma
from mdp_solver import SolveValueIteration, ComputeChoiceProbability

# Load solver config from solve_mdp output (reused for nested estimation)
solve_artifact_dir = Path('../../output/solve_mdp')
with open(solve_artifact_dir / 'config.json', 'r', encoding='utf-8') as f:
    solve_config = json.load(f)

solver_config = solve_config['solver']

print("Solver configuration (from solve_mdp output):")
print(f"  N: {solver_config['N']}")
print(f"  State range: {solver_config['state_range']}")
```

### Estimate Gamma (OLS - Fast)

```{python}
# Step 1: Estimate gamma using OLS (closed-form, fast)
gamma_hat = EstimateGamma(states=states, actions=actions)

print("\nStep 1 Results (OLS Estimation of γ):")
print(f"  True γ: {gamma:.6f}")
print(f"  Estimated γ̂: {gamma_hat:.6f}")
print(f"  Error: {gamma_hat - gamma:.6f}")
print(f"  Relative error: {100*(gamma_hat - gamma)/gamma:.2f}%")
```

### Evaluate Likelihood Around True Beta

```{python}
import time

print("\n" + "="*70)
print("LIKELIHOOD DIAGNOSTIC: EVALUATING AROUND TRUE β")
print("="*70)

# Define grid around true beta
beta_true = beta
beta_grid = np.linspace(beta_true - 0.5, beta_true + 0.5, 11)

# Compute log-likelihood for each beta value
log_likelihoods = []

print(f"\nEvaluating log-likelihood on grid of {len(beta_grid)} β values...")
print(f"Using estimated γ̂ = {gamma_hat:.4f}")
print(f"This requires solving the MDP {len(beta_grid)} times...\n")

start_time = time.time()

for i, beta_val in enumerate(beta_grid):
    print(f"  [{i+1:2d}/{len(beta_grid)}] β = {beta_val:.4f}...", end=" ")

    # Solve MDP for this beta (using estimated gamma)
    v0, v1, _ = SolveValueIteration(
        beta=beta_val,
        gamma=gamma_hat,
        delta=delta,
        gamma_E=gamma_E,
        hyperparameters=solver_config['hyperparameters'],
        N=solver_config['N'],
        state_range=tuple(solver_config['state_range']),
        max_iter=solver_config['max_iter'],
        epsilon_tol=solver_config['epsilon_tol'],
        num_epochs=solver_config['num_epochs'],
        learning_rate=solver_config['learning_rate'],
        verbose=False
    )

    # Compute log-likelihood
    log_likelihood = 0.0
    for m in range(states.shape[0]):
        for t in range(states.shape[1]):
            s_mt = states[m, t]
            a_mt = actions[m, t]

            P_0, P_1 = ComputeChoiceProbability(
                s=float(s_mt),
                v_theta_0=v0,
                v_theta_1=v1
            )

            if a_mt == 0:
                log_likelihood += np.log(P_0)
            else:
                log_likelihood += np.log(P_1)

    log_likelihoods.append(log_likelihood)
    print(f"ℓ = {log_likelihood:.2f}")

log_likelihoods = np.array(log_likelihoods)
elapsed_time = time.time() - start_time

# Find maximum
max_idx = np.argmax(log_likelihoods)
beta_max_likelihood = beta_grid[max_idx]

print(f"\nCompleted in {elapsed_time:.2f} seconds")
print(f"\nLikelihood Profile Results:")
print(f"  True β: {beta_true:.4f}")
print(f"  Maximum at β: {beta_max_likelihood:.4f}")
print(f"  Difference: {abs(beta_true - beta_max_likelihood):.4f}")
```

### Plot Likelihood Profile

```{python}
fig, ax = plt.subplots(1, 1, figsize=(10, 6))

# Plot log-likelihood profile
ax.plot(beta_grid, log_likelihoods, 'b-o', linewidth=2.5, markersize=6, label='Log-likelihood')

# Mark true parameter
ax.axvline(x=beta_true, color='green', linestyle='--', linewidth=2,
           label=f'True β = {beta_true:.3f}')

# Mark maximum of likelihood profile
ax.plot(beta_max_likelihood, log_likelihoods[max_idx], 'r*',
        markersize=20, label=f'Maximum at β = {beta_max_likelihood:.3f}')

ax.set_xlabel('β (Reward Parameter)', fontsize=13)
ax.set_ylabel('Log-Likelihood', fontsize=13)
ax.set_title('Log-Likelihood Profile Around True Parameter', fontsize=15)
ax.legend(fontsize=11)
ax.grid(True, alpha=0.3)

fig.tight_layout()
plt.show()

# Verification
print("\nVerification:")
if abs(beta_true - beta_max_likelihood) < 0.1:
    print("  ✓ Likelihood is maximized at or near the true parameter")
    print("  ✓ Likelihood function is correctly specified")
else:
    print("  ⚠ Likelihood maximum differs from true parameter")
    print(f"    This may indicate an issue with the likelihood specification")
```

## Full Estimation (Not Evaluated)

The code below shows how to run the full nested fixed point estimation. This is computationally expensive and is set to `eval: false` for this report.

### Run Nested Fixed Point Estimation

```{python}
#| eval: false

import time
from mdp_estimator import EstimateMDP
from mdp_config import get_estimator_config

estimator_config = get_estimator_config()
optimization_config = estimator_config['optimization_config']

print("\n" + "="*70)
print("RUNNING NESTED FIXED POINT ESTIMATION")
print("="*70)

start_time = time.time()

# Run the two-step estimator
beta_hat, gamma_hat = EstimateMDP(
    states=states,
    actions=actions,
    delta=delta,
    gamma_E=gamma_E,
    solver_config=solver_config,
    optimization_config=optimization_config
)

elapsed_time = time.time() - start_time

print(f"\nEstimation completed in {elapsed_time:.2f} seconds")
print("\n" + "="*70)
print("ESTIMATION RESULTS")
print("="*70)
```

### Compare Estimated vs True Parameters

```{python}
#| eval: false

# True parameters (used for simulation)
beta_true = beta
gamma_true = gamma

print("\nStructural Parameters:")
print(f"{'Parameter':<15} {'True':>12} {'Estimated':>12} {'Error':>12} {'% Error':>12}")
print("-" * 70)
print(f"{'β (reward)':<15} {beta_true:>12.6f} {beta_hat:>12.6f} {beta_hat - beta_true:>12.6f} {100*(beta_hat - beta_true)/beta_true:>11.2f}%")
print(f"{'γ (depreciation)':<15} {gamma_true:>12.6f} {gamma_hat:>12.6f} {gamma_hat - gamma_true:>12.6f} {100*(gamma_hat - gamma_true)/gamma_true:>11.2f}%")

# Compute relative errors
beta_rel_error = abs(beta_hat - beta_true) / abs(beta_true)
gamma_rel_error = abs(gamma_hat - gamma_true) / abs(gamma_true)

print(f"\nRelative errors:")
print(f"  β: {beta_rel_error:.4%}")
print(f"  γ: {gamma_rel_error:.4%}")
```

### Solve MDP with Estimated Parameters

```{python}
#| eval: false

print("\n" + "="*70)
print("SOLVING MDP WITH ESTIMATED PARAMETERS")
print("="*70)

# Solve MDP with estimated parameters
v_theta_0_est, v_theta_1_est, history_est = SolveValueIteration(
    beta=beta_hat,
    gamma=gamma_hat,
    delta=delta,
    gamma_E=gamma_E,
    hyperparameters=solver_config['hyperparameters'],
    N=solver_config['N'],
    state_range=tuple(solver_config['state_range']),
    max_iter=solver_config['max_iter'],
    epsilon_tol=solver_config['epsilon_tol'],
    num_epochs=solver_config['num_epochs'],
    learning_rate=solver_config['learning_rate'],
    verbose=True
)

print(f"\nEstimated model converged in {len(history_est['iterations'])} iterations")
```

### Solve MDP with True Parameters (for comparison)

```{python}
#| eval: false

print("\n" + "="*70)
print("SOLVING MDP WITH TRUE PARAMETERS")
print("="*70)

# Solve MDP with true parameters
v_theta_0_true, v_theta_1_true, history_true = SolveValueIteration(
    beta=beta_true,
    gamma=gamma_true,
    delta=delta,
    gamma_E=gamma_E,
    hyperparameters=solver_config['hyperparameters'],
    N=solver_config['N'],
    state_range=tuple(solver_config['state_range']),
    max_iter=solver_config['max_iter'],
    epsilon_tol=solver_config['epsilon_tol'],
    num_epochs=solver_config['num_epochs'],
    learning_rate=solver_config['learning_rate'],
    verbose=True
)

print(f"\nTrue model converged in {len(history_true['iterations'])} iterations")
```

### Compare Equilibrium Objects

#### Choice Probabilities (CCP)

```{python}
#| eval: false

# Evaluate CCPs on a grid of states
state_grid = np.linspace(state_range[0], state_range[1], 50)

# Compute CCPs for estimated parameters
ccp_est_a0 = []
ccp_est_a1 = []
for s in state_grid:
    p0, p1 = ComputeChoiceProbability(s=float(s), v_theta_0=v_theta_0_est, v_theta_1=v_theta_1_est)
    ccp_est_a0.append(p0)
    ccp_est_a1.append(p1)

ccp_est_a0 = np.array(ccp_est_a0)
ccp_est_a1 = np.array(ccp_est_a1)

# Compute CCPs for true parameters
ccp_true_a0 = []
ccp_true_a1 = []
for s in state_grid:
    p0, p1 = ComputeChoiceProbability(s=float(s), v_theta_0=v_theta_0_true, v_theta_1=v_theta_1_true)
    ccp_true_a0.append(p0)
    ccp_true_a1.append(p1)

ccp_true_a0 = np.array(ccp_true_a0)
ccp_true_a1 = np.array(ccp_true_a1)

# Compute max absolute difference
max_ccp_diff = np.max(np.abs(ccp_est_a1 - ccp_true_a1))
mean_ccp_diff = np.mean(np.abs(ccp_est_a1 - ccp_true_a1))

print("\nChoice Probability Comparison (P(a=1|s)):")
print(f"  Max absolute difference: {max_ccp_diff:.6f}")
print(f"  Mean absolute difference: {mean_ccp_diff:.6f}")

# Plot CCPs
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Panel 1: CCP comparison
ax = axes[0]
ax.plot(state_grid, ccp_true_a1, 'b-', linewidth=2, label='True parameters')
ax.plot(state_grid, ccp_est_a1, 'r--', linewidth=2, label='Estimated parameters')
ax.set_xlabel('State (s)', fontsize=12)
ax.set_ylabel('P(a=1|s)', fontsize=12)
ax.set_title('Choice Probability: True vs Estimated', fontsize=14)
ax.legend(fontsize=11)
ax.grid(True, alpha=0.3)

# Panel 2: CCP difference
ax = axes[1]
ax.plot(state_grid, ccp_est_a1 - ccp_true_a1, 'k-', linewidth=2)
ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)
ax.set_xlabel('State (s)', fontsize=12)
ax.set_ylabel('Difference in P(a=1|s)', fontsize=12)
ax.set_title('Estimation Error in Choice Probability', fontsize=14)
ax.grid(True, alpha=0.3)

fig.tight_layout()
plt.show()
```

#### Value Functions

```{python}
#| eval: false

# Compute value functions on state grid
with torch.no_grad():
    state_grid_tensor = torch.tensor(state_grid).reshape(-1, 1).float()

    # Estimated parameters
    v0_est = v_theta_0_est(state_grid_tensor).numpy().flatten()
    v1_est = v_theta_1_est(state_grid_tensor).numpy().flatten()

    # True parameters
    v0_true = v_theta_0_true(state_grid_tensor).numpy().flatten()
    v1_true = v_theta_1_true(state_grid_tensor).numpy().flatten()

# Compute max absolute differences
max_v0_diff = np.max(np.abs(v0_est - v0_true))
max_v1_diff = np.max(np.abs(v1_est - v1_true))
mean_v0_diff = np.mean(np.abs(v0_est - v0_true))
mean_v1_diff = np.mean(np.abs(v1_est - v1_true))

print("\nValue Function Comparison:")
print(f"  V(s, a=0) - Max absolute difference: {max_v0_diff:.6f}")
print(f"  V(s, a=0) - Mean absolute difference: {mean_v0_diff:.6f}")
print(f"  V(s, a=1) - Max absolute difference: {max_v1_diff:.6f}")
print(f"  V(s, a=1) - Mean absolute difference: {mean_v1_diff:.6f}")

# Plot value functions
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# Panel 1: V(s, a=0) comparison
ax = axes[0, 0]
ax.plot(state_grid, v0_true, 'b-', linewidth=2, label='True parameters')
ax.plot(state_grid, v0_est, 'r--', linewidth=2, label='Estimated parameters')
ax.set_xlabel('State (s)', fontsize=12)
ax.set_ylabel('V(s, a=0)', fontsize=12)
ax.set_title('Value Function for a=0: True vs Estimated', fontsize=14)
ax.legend(fontsize=11)
ax.grid(True, alpha=0.3)

# Panel 2: V(s, a=0) difference
ax = axes[0, 1]
ax.plot(state_grid, v0_est - v0_true, 'k-', linewidth=2)
ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)
ax.set_xlabel('State (s)', fontsize=12)
ax.set_ylabel('Difference in V(s, a=0)', fontsize=12)
ax.set_title('Estimation Error in V(s, a=0)', fontsize=14)
ax.grid(True, alpha=0.3)

# Panel 3: V(s, a=1) comparison
ax = axes[1, 0]
ax.plot(state_grid, v1_true, 'b-', linewidth=2, label='True parameters')
ax.plot(state_grid, v1_est, 'r--', linewidth=2, label='Estimated parameters')
ax.set_xlabel('State (s)', fontsize=12)
ax.set_ylabel('V(s, a=1)', fontsize=12)
ax.set_title('Value Function for a=1: True vs Estimated', fontsize=14)
ax.legend(fontsize=11)
ax.grid(True, alpha=0.3)

# Panel 4: V(s, a=1) difference
ax = axes[1, 1]
ax.plot(state_grid, v1_est - v1_true, 'k-', linewidth=2)
ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)
ax.set_xlabel('State (s)', fontsize=12)
ax.set_ylabel('Difference in V(s, a=1)', fontsize=12)
ax.set_title('Estimation Error in V(s, a=1)', fontsize=14)
ax.grid(True, alpha=0.3)

fig.tight_layout()
plt.show()
```

## Likelihood Profile Around True Parameter (Old Section - Not Evaluated)

This section is kept for reference but set to `eval: false` since we already computed the likelihood profile above.

```{python}
#| eval: false

print("\n" + "="*70)
print("COMPUTING LIKELIHOOD PROFILE")
print("="*70)

# Define grid around true beta
beta_grid = np.linspace(beta_true - 0.5, beta_true + 0.5, 15)

# Compute log-likelihood for each beta value
log_likelihoods = []

print(f"\nEvaluating log-likelihood on grid of {len(beta_grid)} beta values...")
print(f"This requires solving the MDP {len(beta_grid)} times...")

for i, beta_val in enumerate(beta_grid):
    print(f"  [{i+1}/{len(beta_grid)}] β = {beta_val:.4f}...", end=" ")

    # Solve MDP for this beta (using estimated gamma)
    v0, v1, _ = SolveValueIteration(
        beta=beta_val,
        gamma=gamma_hat,
        delta=delta,
        gamma_E=gamma_E,
        hyperparameters=solver_config['hyperparameters'],
        N=solver_config['N'],
        state_range=tuple(solver_config['state_range']),
        max_iter=solver_config['max_iter'],
        epsilon_tol=solver_config['epsilon_tol'],
        num_epochs=solver_config['num_epochs'],
        learning_rate=solver_config['learning_rate'],
        verbose=False
    )

    # Compute log-likelihood
    log_likelihood = 0.0
    for m in range(states.shape[0]):
        for t in range(states.shape[1]):
            s_mt = states[m, t]
            a_mt = actions[m, t]

            P_0, P_1 = ComputeChoiceProbability(
                s=float(s_mt),
                v_theta_0=v0,
                v_theta_1=v1
            )

            if a_mt == 0:
                log_likelihood += np.log(P_0)
            else:
                log_likelihood += np.log(P_1)

    log_likelihoods.append(log_likelihood)
    print(f"ℓ = {log_likelihood:.2f}")

log_likelihoods = np.array(log_likelihoods)

# Find maximum
max_idx = np.argmax(log_likelihoods)
beta_max_likelihood = beta_grid[max_idx]

print(f"\nLikelihood maximized at β = {beta_max_likelihood:.4f}")
print(f"True β = {beta_true:.4f}")
print(f"Estimated β = {beta_hat:.4f}")
```

### Plot Likelihood Profile (Old - Not Evaluated)

```{python}
#| eval: false

fig, ax = plt.subplots(1, 1, figsize=(10, 6))

# Plot log-likelihood profile
ax.plot(beta_grid, log_likelihoods, 'b-', linewidth=2.5, label='Log-likelihood')

# Mark true parameter
ax.axvline(x=beta_true, color='green', linestyle='--', linewidth=2,
           label=f'True β = {beta_true:.3f}')

# Mark estimated parameter
ax.axvline(x=beta_hat, color='red', linestyle='--', linewidth=2,
           label=f'Estimated β = {beta_hat:.3f}')

# Mark maximum of likelihood profile
ax.plot(beta_max_likelihood, log_likelihoods[max_idx], 'ko',
        markersize=10, label=f'Max at β = {beta_max_likelihood:.3f}')

ax.set_xlabel('β (Reward Parameter)', fontsize=13)
ax.set_ylabel('Log-Likelihood', fontsize=13)
ax.set_title('Log-Likelihood Profile Around True Parameter', fontsize=15)
ax.legend(fontsize=11)
ax.grid(True, alpha=0.3)

fig.tight_layout()
plt.show()

print("\nVerification:")
if abs(beta_true - beta_max_likelihood) < 0.05:
    print("  ✓ Likelihood is maximized near the true parameter")
else:
    print("  ⚠ Likelihood maximum differs from true parameter")

if abs(beta_hat - beta_max_likelihood) < 0.1:
    print("  ✓ Estimated parameter is near the likelihood maximum")
else:
    print("  ⚠ Estimated parameter differs from likelihood maximum")
```

## Summary (Not Evaluated)

```{python}
#| eval: false

print("\n" + "="*70)
print("ESTIMATION SUMMARY")
print("="*70)

print(f"\nData:")
print(f"  Paths (M): {M}")
print(f"  Periods (T): {T}")
print(f"  Total observations: {M * T}")

print(f"\nTrue Parameters:")
print(f"  β = {beta_true:.6f}")
print(f"  γ = {gamma_true:.6f}")

print(f"\nEstimated Parameters:")
print(f"  β̂ = {beta_hat:.6f}  (error: {100*(beta_hat - beta_true)/beta_true:+.2f}%)")
print(f"  γ̂ = {gamma_hat:.6f}  (error: {100*(gamma_hat - gamma_true)/gamma_true:+.2f}%)")

print(f"\nEquilibrium Object Errors:")
print(f"  CCP max error: {max_ccp_diff:.6f}")
print(f"  V(s,a=0) max error: {max_v0_diff:.6f}")
print(f"  V(s,a=1) max error: {max_v1_diff:.6f}")

print(f"\nComputational Cost:")
print(f"  Total estimation time: {elapsed_time:.2f} seconds")
print(f"  Likelihood evaluations: Multiple (nested optimization)")

print("\n" + "="*70)
```
