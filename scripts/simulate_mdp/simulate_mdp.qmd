---
title: "Simulating Markov Decision Processes"
format:
  html:
    toc: true
    code-fold: false
jupyter: python3
execute:
  daemon: false
---

## Overview

This document loads the policy learned in `solve_mdp.qmd`, reconstructs the trained neural
networks, and inspects the stored artifacts for verification.

## Load Trained Policy

```{python}
import sys
sys.path.insert(0, '..')
sys.path.insert(0, '../../src')

import json
from pathlib import Path

import numpy as np
import torch
import matplotlib.pyplot as plt

from mdp_solver import MonotonicNetwork
from config_mdp import get_solver_config, get_comparative_statics

artifact_dir = Path('../../output/sovle_mdp')
if not artifact_dir.exists():
    raise FileNotFoundError(
        f"Expected trained artifacts in {artifact_dir}. Run solve_mdp.qmd first."
    )

module_config = get_solver_config()
module_statics = get_comparative_statics()

# Load configuration written alongside the trained policy
with open(artifact_dir / 'config.json', 'r', encoding='utf-8') as config_file:
    config = json.load(config_file)

# Load precomputed grids for diagnostics
policy_store = np.load(artifact_dir / 'policy_data.npz')
state_grid = policy_store['state_grid']
v0_grid = policy_store['v0']
v1_grid = policy_store['v1']
prob_a0_grid = policy_store['prob_a0']
prob_a1_grid = policy_store['prob_a1']

history_store = np.load(artifact_dir / 'convergence_history.npz')
history_iterations = history_store['iterations']
history_max_errors = history_store['max_errors']

print(f"Loaded trained networks with hidden sizes {hidden_sizes}")

module_config_for_compare = module_config.copy()
module_config_for_compare['state_range'] = list(module_config_for_compare['state_range'])

stored_subset = {k: config[k] for k in module_config}
if module_config_for_compare != stored_subset:
    raise ValueError("Module configuration and stored configuration are inconsistent.")

if module_statics != config.get('comparative_statics', {}):
    raise ValueError("Comparative statics configuration mismatch.")

beta = module_config['beta']
gamma = module_config['gamma']
delta = module_config['delta']
gamma_E = module_config['gamma_E']
hyperparameters = module_config['hyperparameters']
state_range = tuple(module_config['state_range'])
hidden_sizes = hyperparameters.get('hidden_sizes', [32, 32])
epsilon_tol = module_config['epsilon_tol']

# Rebuild networks and load weights
v_theta_0 = MonotonicNetwork(hidden_sizes=hidden_sizes)
v_theta_1 = MonotonicNetwork(hidden_sizes=hidden_sizes)

v_theta_0.load_state_dict(torch.load(artifact_dir / 'v_theta_0.pt', map_location='cpu'))
v_theta_1.load_state_dict(torch.load(artifact_dir / 'v_theta_1.pt', map_location='cpu'))

v_theta_0.eval()
v_theta_1.eval()
```

## Stored Policy Diagnostics

```{python}
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

ax1.plot(state_grid, v0_grid, label='v(s, a=0)', color='tab:blue')
ax1.plot(state_grid, v1_grid, label='v(s, a=1)', color='tab:red')
ax1.set_xlabel('State')
ax1.set_ylabel('Value')
ax1.set_title('Choice-Specific Value Functions')
ax1.grid(alpha=0.3)
ax1.legend()

ax2.plot(history_iterations, history_max_errors, color='tab:green')
ax2.axhline(epsilon_tol, color='tab:gray', linestyle='--', linewidth=1)
ax2.set_xlabel('Iteration')
ax2.set_ylabel('Max Error')
ax2.set_title('Convergence History')
ax2.set_yscale('log')
ax2.grid(alpha=0.3)

plt.tight_layout()
plt.show()
```

```{python}
fig, ax = plt.subplots(figsize=(8, 5))
ax.plot(state_grid, prob_a1_grid, label='P(a=1|s)', color='tab:orange')
ax.plot(state_grid, prob_a0_grid, label='P(a=0|s)', color='tab:purple')
ax.axhline(0.5, color='tab:gray', linestyle=':')
ax.set_xlabel('State')
ax.set_ylabel('Choice Probability')
ax.set_title('Stored Policy Probabilities')
ax.set_ylim(0, 1)
ax.grid(alpha=0.3)
ax.legend()
plt.tight_layout()
plt.show()
```
