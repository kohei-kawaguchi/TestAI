---
title: "Simulating Markov Decision Processes"
format:
  html:
    toc: true
    code-fold: false
jupyter: python3
execute:
  daemon: false
---

## Overview

This document loads the policy learned in `solve_mdp.qmd`, reconstructs the trained neural
networks, and inspects the stored artifacts for verification.

## Load Trained Policy

```{python}
import sys
sys.path.insert(0, '../../src')

import json
from pathlib import Path

import numpy as np
import torch
import matplotlib.pyplot as plt

from mdp_solver import MonotonicNetwork

artifact_dir = Path('../../output/sovle_mdp')
if not artifact_dir.exists():
    raise FileNotFoundError(
        f"Expected trained artifacts in {artifact_dir}. Run solve_mdp.qmd first."
    )

# Load configuration
with open(artifact_dir / 'config.json', 'r', encoding='utf-8') as config_file:
    config = json.load(config_file)

beta = config['beta']
gamma = config['gamma']
delta = config['delta']
gamma_E = config['gamma_E']
hyperparameters = config['hyperparameters']
state_range = config['state_range']
hidden_sizes = hyperparameters.get('hidden_sizes', [32, 32])

# Rebuild networks and load weights
v_theta_0 = MonotonicNetwork(hidden_sizes=hidden_sizes)
v_theta_1 = MonotonicNetwork(hidden_sizes=hidden_sizes)

v_theta_0.load_state_dict(torch.load(artifact_dir / 'v_theta_0.pt', map_location='cpu'))
v_theta_1.load_state_dict(torch.load(artifact_dir / 'v_theta_1.pt', map_location='cpu'))

v_theta_0.eval()
v_theta_1.eval()

# Load precomputed grids for diagnostics
policy_store = np.load(artifact_dir / 'policy_data.npz')
state_grid = policy_store['state_grid']
v0_grid = policy_store['v0']
v1_grid = policy_store['v1']
prob_a0_grid = policy_store['prob_a0']
prob_a1_grid = policy_store['prob_a1']

history_store = np.load(artifact_dir / 'convergence_history.npz')
history_iterations = history_store['iterations']
history_max_errors = history_store['max_errors']

print(f"Loaded trained networks with hidden sizes {hidden_sizes}")
```

## Stored Policy Diagnostics

```{python}
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

ax1.plot(state_grid, v0_grid, label='v(s, a=0)', color='tab:blue')
ax1.plot(state_grid, v1_grid, label='v(s, a=1)', color='tab:red')
ax1.set_xlabel('State')
ax1.set_ylabel('Value')
ax1.set_title('Choice-Specific Value Functions')
ax1.grid(alpha=0.3)
ax1.legend()

ax2.plot(history_iterations, history_max_errors, color='tab:green')
ax2.axhline(config['epsilon_tol'], color='tab:gray', linestyle='--', linewidth=1)
ax2.set_xlabel('Iteration')
ax2.set_ylabel('Max Error')
ax2.set_title('Convergence History')
ax2.set_yscale('log')
ax2.grid(alpha=0.3)

plt.tight_layout()
plt.show()
```

```{python}
fig, ax = plt.subplots(figsize=(8, 5))
ax.plot(state_grid, prob_a1_grid, label='P(a=1|s)', color='tab:orange')
ax.plot(state_grid, prob_a0_grid, label='P(a=0|s)', color='tab:purple')
ax.axhline(0.5, color='tab:gray', linestyle=':')
ax.set_xlabel('State')
ax.set_ylabel('Choice Probability')
ax.set_title('Stored Policy Probabilities')
ax.set_ylim(0, 1)
ax.grid(alpha=0.3)
ax.legend()
plt.tight_layout()
plt.show()
```
