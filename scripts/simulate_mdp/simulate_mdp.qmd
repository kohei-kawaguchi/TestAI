---
title: "Simulating Markov Decision Processes"
format:
  html:
    toc: true
    code-fold: false
jupyter: python3
execute:
  daemon: false
---

## Overview

This document loads the policy learned in `solve_mdp.qmd`, reconstructs the trained neural
networks, and runs Monte Carlo simulations to analyze the dynamic behavior of the optimal policy.

## Monte Carlo Simulation Algorithm

We simulate the Markov Decision Process under the optimal policy learned by the neural networks. The simulation generates multiple paths of state evolution and action choices to analyze the long-run behavior of the system.

**Model Specification** (from [solve_mdp.qmd](../solve_mdp/solve_mdp.qmd)):

- **State transition**: $s_{t+1} = (1 - \gamma) s_t + a_t$
- **Mean reward**: $\bar{r}(s, a) = \beta \log(1 + s) - a$
- **Choice probabilities**: $P(a | s) = \frac{\exp(v(s, a))}{\sum_{a' \in \{0,1\}} \exp(v(s, a'))}$
  where $v(s, a)$ is the choice-specific value function learned by the neural networks

**Simulation Parameters**:

- Number of paths: $M = 100$
- Time periods per path: $T = 100$
- Initial state: $s_0$ (to be specified)

**Algorithm**:

For each path $m = 1, \ldots, M$:

1. **Initialize**: Set $s_0^{(m)}$ to the initial state value

2. **Simulate forward** for $t = 0, \ldots, T-1$:

   a. **Compute choice probabilities**: Evaluate the trained neural networks at current state $s_t^{(m)}$:
      $$P(a=0 | s_t^{(m)}) = \frac{\exp(v_\theta^{(0)}(s_t^{(m)}))}{\exp(v_\theta^{(0)}(s_t^{(m)})) + \exp(v_\theta^{(1)}(s_t^{(m)}))}$$
      $$P(a=1 | s_t^{(m)}) = \frac{\exp(v_\theta^{(1)}(s_t^{(m)}))}{\exp(v_\theta^{(0)}(s_t^{(m)})) + \exp(v_\theta^{(1)}(s_t^{(m)}))}$$

   b. **Draw action**: Sample action from the choice probabilities:
      $$a_t^{(m)} \sim \text{Categorical}(P(a=0 | s_t^{(m)}), P(a=1 | s_t^{(m)}))$$

   c. **Compute mean reward**:
      $$r_t^{(m)} = \beta \log(1 + s_t^{(m)}) - a_t^{(m)}$$

   d. **Update state**:
      $$s_{t+1}^{(m)} = (1 - \gamma) s_t^{(m)} + a_t^{(m)}$$

3. **Store path**: Save $(s_t^{(m)}, a_t^{(m)}, r_t^{(m)})_{t=0}^{T-1}$ for analysis

**Output Statistics**:

- State evolution: Mean, median, and quantiles of $s_t$ across paths
- Action frequency: Fraction of times each action is chosen at each period
- Cumulative rewards: Mean discounted rewards $\sum_{t=0}^{T-1} \delta^t r_t^{(m)}$ across paths
- Policy behavior: State-dependent action choices and convergence to steady state

## Load Trained Policy

```{python}
import sys
sys.path.insert(0, '..')
sys.path.insert(0, '../../src')

import json
from pathlib import Path

import numpy as np
import torch
import matplotlib.pyplot as plt

from mdp_solver import MonotonicNetwork
from config_mdp import get_solver_config, get_comparative_statics
from mdp_utils import (
    plot_choice_value_functions,
    plot_convergence_history,
    plot_policy_probabilities,
)

artifact_dir = Path('../../output/sovle_mdp')
if not artifact_dir.exists():
    raise FileNotFoundError(
        f"Expected trained artifacts in {artifact_dir}. Run solve_mdp.qmd first."
    )

module_config = get_solver_config()
module_statics = get_comparative_statics()

# Load configuration written alongside the trained policy
with open(artifact_dir / 'config.json', 'r', encoding='utf-8') as config_file:
    config = json.load(config_file)

# Load precomputed grids for diagnostics
policy_store = np.load(artifact_dir / 'policy_data.npz')
state_grid = policy_store['state_grid']
v0_grid = policy_store['v0']
v1_grid = policy_store['v1']
prob_a0_grid = policy_store['prob_a0']
prob_a1_grid = policy_store['prob_a1']

history_store = np.load(artifact_dir / 'convergence_history.npz')
history_iterations = history_store['iterations']
history_max_errors = history_store['max_errors']

module_config_for_compare = module_config.copy()
module_config_for_compare['state_range'] = list(module_config_for_compare['state_range'])

stored_subset = {k: config[k] for k in module_config}
if module_config_for_compare != stored_subset:
    raise ValueError("Module configuration and stored configuration are inconsistent.")

if module_statics != config.get('comparative_statics', {}):
    raise ValueError("Comparative statics configuration mismatch.")

beta = module_config['beta']
gamma = module_config['gamma']
delta = module_config['delta']
gamma_E = module_config['gamma_E']
hyperparameters = module_config['hyperparameters']
state_range = tuple(module_config['state_range'])
hidden_sizes = hyperparameters.get('hidden_sizes', [32, 32])
epsilon_tol = module_config['epsilon_tol']

print(f"Loaded trained networks with hidden sizes {hidden_sizes}")

# Rebuild networks and load weights
v_theta_0 = MonotonicNetwork(hidden_sizes=hidden_sizes)
v_theta_1 = MonotonicNetwork(hidden_sizes=hidden_sizes)

v_theta_0.load_state_dict(torch.load(artifact_dir / 'v_theta_0.pt', map_location='cpu'))
v_theta_1.load_state_dict(torch.load(artifact_dir / 'v_theta_1.pt', map_location='cpu'))

v_theta_0.eval()
v_theta_1.eval()
```

## Stored Policy Diagnostics

```{python}
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))
plot_choice_value_functions(
    state_grid=state_grid,
    v0_values=v0_grid,
    v1_values=v1_grid,
    ax=ax1,
    title='Choice-Specific Value Functions',
)
plot_convergence_history(
    iterations=history_iterations,
    max_errors=history_max_errors,
    epsilon_tol=epsilon_tol,
    ax=ax2,
    title='Convergence History',
)
fig.tight_layout()
plt.show()
```

```{python}
ax = plot_policy_probabilities(
    state_grid=state_grid,
    prob_a0=prob_a0_grid,
    prob_a1=prob_a1_grid,
)
ax.figure.tight_layout()
plt.show()
```
