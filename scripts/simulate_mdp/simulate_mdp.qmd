---
title: "Simulating Markov Decision Processes"
format:
  html:
    toc: true
    code-fold: false
jupyter: python3
execute:
  daemon: false
---

## Overview

This document loads the policy learned in `solve_mdp.qmd`, reconstructs the trained neural
networks, and uses them to simulate Markov Decision Process trajectories.

## Load Trained Policy

```{python}
import sys
sys.path.insert(0, '../../src')

import json
from pathlib import Path

import numpy as np
import pandas as pd
import torch
import matplotlib.pyplot as plt

from mdp_solver import MonotonicNetwork

artifact_dir = Path('../../output/sovle_mdp')
if not artifact_dir.exists():
    raise FileNotFoundError(
        f"Expected trained artifacts in {artifact_dir}. Run solve_mdp.qmd first."
    )

# Load configuration
with open(artifact_dir / 'config.json', 'r', encoding='utf-8') as config_file:
    config = json.load(config_file)

beta = config['beta']
gamma = config['gamma']
delta = config['delta']
gamma_E = config['gamma_E']
hyperparameters = config['hyperparameters']
state_range = config['state_range']
hidden_sizes = hyperparameters.get('hidden_sizes', [32, 32])

# Rebuild networks and load weights
v_theta_0 = MonotonicNetwork(hidden_sizes=hidden_sizes)
v_theta_1 = MonotonicNetwork(hidden_sizes=hidden_sizes)

v_theta_0.load_state_dict(torch.load(artifact_dir / 'v_theta_0.pt', map_location='cpu'))
v_theta_1.load_state_dict(torch.load(artifact_dir / 'v_theta_1.pt', map_location='cpu'))

v_theta_0.eval()
v_theta_1.eval()

# Load precomputed grids for diagnostics
policy_store = np.load(artifact_dir / 'policy_data.npz')
state_grid = policy_store['state_grid']
v0_grid = policy_store['v0']
v1_grid = policy_store['v1']
prob_a0_grid = policy_store['prob_a0']
prob_a1_grid = policy_store['prob_a1']

history_store = np.load(artifact_dir / 'convergence_history.npz')
history_iterations = history_store['iterations']
history_max_errors = history_store['max_errors']

print(f"Loaded trained networks with hidden sizes {hidden_sizes}")
```

## Stored Policy Diagnostics

```{python}
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

ax1.plot(state_grid, v0_grid, label='v(s, a=0)', color='tab:blue')
ax1.plot(state_grid, v1_grid, label='v(s, a=1)', color='tab:red')
ax1.set_xlabel('State')
ax1.set_ylabel('Value')
ax1.set_title('Choice-Specific Value Functions')
ax1.grid(alpha=0.3)
ax1.legend()

ax2.plot(history_iterations, history_max_errors, color='tab:green')
ax2.axhline(config['epsilon_tol'], color='tab:gray', linestyle='--', linewidth=1)
ax2.set_xlabel('Iteration')
ax2.set_ylabel('Max Error')
ax2.set_title('Convergence History')
ax2.set_yscale('log')
ax2.grid(alpha=0.3)

plt.tight_layout()
plt.show()
```

```{python}
fig, ax = plt.subplots(figsize=(8, 5))
ax.plot(state_grid, prob_a1_grid, label='P(a=1|s)', color='tab:orange')
ax.plot(state_grid, prob_a0_grid, label='P(a=0|s)', color='tab:purple')
ax.axhline(0.5, color='tab:gray', linestyle=':')
ax.set_xlabel('State')
ax.set_ylabel('Choice Probability')
ax.set_title('Stored Policy Probabilities')
ax.set_ylim(0, 1)
ax.grid(alpha=0.3)
ax.legend()
plt.tight_layout()
plt.show()
```

## Simulation Utilities

```{python}
def evaluate_policy(states: np.ndarray) -> np.ndarray:
    """
    Compute action probabilities for a batch of states.

    Args:
        states: Array of shape (N,) with state values.

    Returns:
        Array of shape (N, 2) with probabilities for actions 0 and 1.
    """
    states_tensor = torch.as_tensor(states, dtype=torch.float32).reshape(-1, 1)

    with torch.no_grad():
        v0 = v_theta_0(states_tensor)
        v1 = v_theta_1(states_tensor)
        logits = torch.cat((v0, v1), dim=1)
        probs = torch.softmax(logits, dim=1)

    return probs.numpy()


def simulate_mdp_paths(
    *,
    num_agents: int = 200,
    num_periods: int = 30,
    initial_state: float | np.ndarray = 1.0,
    seed: int | None = 0,
) -> pd.DataFrame:
    """
    Simulate MDP trajectories using the learned policy.

    Args:
        num_agents: Number of independent agents to simulate.
        num_periods: Number of decision periods per agent.
        initial_state: Scalar initial state or array of size num_agents.
        seed: Random seed for reproducibility.

    Returns:
        DataFrame containing simulated trajectories.
    """
    rng = np.random.default_rng(seed)

    states = np.empty((num_agents, num_periods + 1), dtype=np.float64)
    if np.isscalar(initial_state):
        states[:, 0] = float(initial_state)
    else:
        initial_state = np.asarray(initial_state, dtype=np.float64)
        if initial_state.shape != (num_agents,):
            raise ValueError("initial_state array must have shape (num_agents,)")
        states[:, 0] = initial_state

    action_probs = np.empty((num_agents, num_periods), dtype=np.float64)
    actions = np.empty((num_agents, num_periods), dtype=np.int64)
    rewards = np.empty((num_agents, num_periods), dtype=np.float64)

    for t in range(num_periods):
        s_t = states[:, t]
        probs = evaluate_policy(s_t)
        action_probs[:, t] = probs[:, 1]

        draws = rng.random(num_agents)
        chosen_actions = (draws < probs[:, 1]).astype(np.int64)
        actions[:, t] = chosen_actions

        mean_reward = beta * np.log1p(s_t) - chosen_actions
        eps0 = rng.gumbel(loc=0.0, scale=1.0, size=num_agents)
        eps1 = rng.gumbel(loc=0.0, scale=1.0, size=num_agents)
        realized_shock = np.where(chosen_actions == 0, eps0, eps1)
        rewards[:, t] = mean_reward + realized_shock

        next_state = (1.0 - gamma) * s_t + chosen_actions
        states[:, t + 1] = next_state

    agent_ids = np.repeat(np.arange(num_agents), num_periods)
    period_ids = np.tile(np.arange(num_periods), num_agents)

    data = {
        "agent": agent_ids,
        "period": period_ids,
        "state": states[:, :-1].reshape(-1),
        "action": actions.reshape(-1),
        "prob_action_1": action_probs.reshape(-1),
        "reward": rewards.reshape(-1),
        "next_state": states[:, 1:].reshape(-1),
    }

    return pd.DataFrame(data)
```

## Generate Simulated Data

```{python}
simulated_df = simulate_mdp_paths(num_agents=200, num_periods=40, initial_state=1.0, seed=42)
simulated_df.head()
```

```{python}
summary = (
    simulated_df.groupby('action')
    .agg(
        count=('reward', 'size'),
        mean_reward=('reward', 'mean'),
        mean_prob_action_1=('prob_action_1', 'mean'),
        mean_state=('state', 'mean'),
    )
    .reset_index()
)
summary
```

```{python}
fig, ax = plt.subplots(figsize=(10, 5))
sample_agents = simulated_df['agent'].unique()[:5]
for agent in sample_agents:
    agent_states = simulated_df.loc[simulated_df['agent'] == agent, ['period', 'state']]
    ax.plot(agent_states['period'], agent_states['state'], marker='o', alpha=0.7, label=f'Agent {agent}')

ax.set_xlabel('Period')
ax.set_ylabel('State')
ax.set_title('Sample State Trajectories')
ax.grid(alpha=0.3)
ax.legend()
plt.tight_layout()
plt.show()
```

```{python}
output_path = artifact_dir / 'simulated_trajectories.csv'
simulated_df.to_csv(output_path, index=False)
print(f"Wrote {len(simulated_df)} simulated observations to {output_path.resolve()}")
```
