---
title: "Simulating Markov Decision Processes"
format:
  html:
    toc: true
    code-fold: false
jupyter: python3
execute:
  daemon: false
---

## Overview

This document loads the policy learned in `solve_mdp.qmd`, reconstructs the trained neural
networks, and inspects the stored artifacts for verification.

## Load Trained Policy

```{python}
import sys
sys.path.insert(0, '..')
sys.path.insert(0, '../../src')

import json
from pathlib import Path

import numpy as np
import torch
import matplotlib.pyplot as plt

from mdp_solver import MonotonicNetwork
from config_mdp import get_solver_config, get_comparative_statics
from mdp_utils import (
    plot_choice_value_functions,
    plot_convergence_history,
    plot_policy_probabilities,
)

artifact_dir = Path('../../output/sovle_mdp')
if not artifact_dir.exists():
    raise FileNotFoundError(
        f"Expected trained artifacts in {artifact_dir}. Run solve_mdp.qmd first."
    )

module_config = get_solver_config()
module_statics = get_comparative_statics()

# Load configuration written alongside the trained policy
with open(artifact_dir / 'config.json', 'r', encoding='utf-8') as config_file:
    config = json.load(config_file)

# Load precomputed grids for diagnostics
policy_store = np.load(artifact_dir / 'policy_data.npz')
state_grid = policy_store['state_grid']
v0_grid = policy_store['v0']
v1_grid = policy_store['v1']
prob_a0_grid = policy_store['prob_a0']
prob_a1_grid = policy_store['prob_a1']

history_store = np.load(artifact_dir / 'convergence_history.npz')
history_iterations = history_store['iterations']
history_max_errors = history_store['max_errors']

print(f"Loaded trained networks with hidden sizes {hidden_sizes}")

module_config_for_compare = module_config.copy()
module_config_for_compare['state_range'] = list(module_config_for_compare['state_range'])

stored_subset = {k: config[k] for k in module_config}
if module_config_for_compare != stored_subset:
    raise ValueError("Module configuration and stored configuration are inconsistent.")

if module_statics != config.get('comparative_statics', {}):
    raise ValueError("Comparative statics configuration mismatch.")

beta = module_config['beta']
gamma = module_config['gamma']
delta = module_config['delta']
gamma_E = module_config['gamma_E']
hyperparameters = module_config['hyperparameters']
state_range = tuple(module_config['state_range'])
hidden_sizes = hyperparameters.get('hidden_sizes', [32, 32])
epsilon_tol = module_config['epsilon_tol']

# Rebuild networks and load weights
v_theta_0 = MonotonicNetwork(hidden_sizes=hidden_sizes)
v_theta_1 = MonotonicNetwork(hidden_sizes=hidden_sizes)

v_theta_0.load_state_dict(torch.load(artifact_dir / 'v_theta_0.pt', map_location='cpu'))
v_theta_1.load_state_dict(torch.load(artifact_dir / 'v_theta_1.pt', map_location='cpu'))

v_theta_0.eval()
v_theta_1.eval()
```

## Stored Policy Diagnostics

```{python}
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))
plot_choice_value_functions(
    state_grid=state_grid,
    v0_values=v0_grid,
    v1_values=v1_grid,
    ax=ax1,
    title='Choice-Specific Value Functions',
)
plot_convergence_history(
    iterations=history_iterations,
    max_errors=history_max_errors,
    epsilon_tol=epsilon_tol,
    ax=ax2,
    title='Convergence History',
)
fig.tight_layout()
plt.show()
```

```{python}
ax = plot_policy_probabilities(
    state_grid=state_grid,
    prob_a0=prob_a0_grid,
    prob_a1=prob_a1_grid,
)
ax.figure.tight_layout()
plt.show()
```
