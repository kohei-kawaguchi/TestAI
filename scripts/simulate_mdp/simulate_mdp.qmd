---
title: "Simulating Markov Decision Processes"
format:
  html:
    toc: true
    code-fold: false
jupyter: python3
execute:
  daemon: false
---

## Overview

This document loads the policy learned in `solve_mdp.qmd`, reconstructs the trained neural
networks, and runs Monte Carlo simulations to analyze the dynamic behavior of the optimal policy.

## Monte Carlo Simulation Algorithm

We simulate the Markov Decision Process under the optimal policy learned by the neural networks. The simulation generates multiple paths of state evolution and action choices to analyze the long-run behavior of the system.

**Model Specification** (from [solve_mdp.qmd](../solve_mdp/solve_mdp.qmd)):

- **State transition**: $s_{t+1} = (1 - \gamma) s_t + a_t$
- **Mean reward**: $\bar{r}(s, a) = \beta \log(1 + s) - a$
- **Choice probabilities**: $P(a | s) = \frac{\exp(v(s, a))}{\sum_{a' \in \{0,1\}} \exp(v(s, a'))}$
  where $v(s, a)$ is the choice-specific value function learned by the neural networks

**Simulation Parameters**:

- Number of paths: $M = 100$
- Time periods per path: $T = 100$
- Initial state: $s_0$ (to be specified)

**Algorithm**:

For each path $m = 1, \ldots, M$:

1. **Initialize**: Set $s_0^{(m)}$ to the initial state value

2. **Simulate forward** for $t = 0, \ldots, T-1$:

   a. **Compute choice probabilities**: Evaluate the trained neural networks at current state $s_t^{(m)}$:
      $$P(a=0 | s_t^{(m)}) = \frac{\exp(v_\theta^{(0)}(s_t^{(m)}))}{\exp(v_\theta^{(0)}(s_t^{(m)})) + \exp(v_\theta^{(1)}(s_t^{(m)}))}$$
      $$P(a=1 | s_t^{(m)}) = \frac{\exp(v_\theta^{(1)}(s_t^{(m)}))}{\exp(v_\theta^{(0)}(s_t^{(m)})) + \exp(v_\theta^{(1)}(s_t^{(m)}))}$$

   b. **Draw action**: Sample action from the choice probabilities:
      $$a_t^{(m)} \sim \text{Categorical}(P(a=0 | s_t^{(m)}), P(a=1 | s_t^{(m)}))$$

   c. **Compute mean reward**:
      $$r_t^{(m)} = \beta \log(1 + s_t^{(m)}) - a_t^{(m)}$$

   d. **Update state**:
      $$s_{t+1}^{(m)} = (1 - \gamma) s_t^{(m)} + a_t^{(m)}$$

3. **Store path**: Save $(s_t^{(m)}, a_t^{(m)}, r_t^{(m)})_{t=0}^{T-1}$ for analysis

**Output Statistics**:

- State evolution: Mean, median, and quantiles of $s_t$ across paths
- Action frequency: Fraction of times each action is chosen at each period
- Cumulative rewards: Mean discounted rewards $\sum_{t=0}^{T-1} \delta^t r_t^{(m)}$ across paths
- Policy behavior: State-dependent action choices and convergence to steady state

### Pseudo Code

::: {.callout-note appearance="simple"}

## Algorithm: Monte Carlo Simulation of MDP Policy

**Design Principles:**

This pseudo code is designed for direct implementation:

- **No placeholders**: Every operation corresponds to a concrete function or method call
- **Explicit parameters**: All inputs are passed as parameters, no implicit/global state
- **Reuses shared functions**: Uses `ComputeChoiceProbability`, `ComputeNextState`, `ComputeMeanReward` from solve_mdp
- **No hard-coding**: All values are parameterized

**Input:**

- Trained networks: $v_\theta^{(0)}$: Network, $v_\theta^{(1)}$: Network (from solve_mdp)
- MDP parameters: $\beta$: float, $\gamma$: float, $\delta$: float (from shared config)
- Simulation parameters: $M$: int (number of paths), $T$: int (time periods), $s_0$: float (initial state)
- Random seed: seed: int (for reproducibility)

**Output:**

- Simulated paths: states: Array[$M \times T$], actions: Array[$M \times T$], rewards: Array[$M \times T$]

---

### Main Algorithm

**Procedure** `SimulateMDP`($v_\theta^{(0)}$: Network, $v_\theta^{(1)}$: Network, $\beta$: float, $\gamma$: float, $\delta$: float, $M$: int, $T$: int, $s_0$: float, seed: int) $\to$ (Array[$M \times T$], Array[$M \times T$], Array[$M \times T$])

1. Set random seed to seed

2. states: Array[$M \times T$], actions: Array[$M \times T$], rewards: Array[$M \times T$] $\leftarrow$ Empty arrays

3. For $m$: int = 1 to $M$:

   a. states[$m$, 1] $\leftarrow s_0$ &nbsp;&nbsp;&nbsp;&nbsp;// Initialize first state

   b. For $t$: int = 1 to $T$:

   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;i. $s_t$: float $\leftarrow$ states[$m$, $t$]

   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ii. $P(a=0|s_t)$: float, $P(a=1|s_t)$: float $\leftarrow$ `ComputeChoiceProbability`($s_t$, $v_\theta^{(0)}$, $v_\theta^{(1)}$)

   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;iii. $a_t$: int $\leftarrow$ `DrawAction`($P(a=0|s_t)$, $P(a=1|s_t)$)

   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;iv. actions[$m$, $t$] $\leftarrow a_t$

   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;v. $r_t$: float $\leftarrow$ `ComputeMeanReward`($s_t$, $a_t$, $\beta$)

   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;vi. rewards[$m$, $t$] $\leftarrow r_t$

   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;vii. **If** $t < T$:

   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$s_{t+1}$: float $\leftarrow$ `ComputeNextState`($s_t$, $a_t$, $\gamma$)

   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;states[$m$, $t+1$] $\leftarrow s_{t+1}$

4. **Return** states, actions, rewards

---

### Subroutines

**Procedure** `DrawAction`($P_0$: float, $P_1$: float) $\to$ int

- Draw $u$: float $\sim$ Uniform(0, 1)
- **If** $u < P_0$: **Return** 0
- **Else**: **Return** 1

---

### Shared Functions from solve_mdp

The following functions are imported from solve_mdp and reused directly:

**Procedure** `ComputeChoiceProbability`($s$: float, $v_\theta^{(0)}$: Network, $v_\theta^{(1)}$: Network) $\to$ (float, float)

- Computes $P(a=0|s)$ and $P(a=1|s)$ using logit formula
- See solve_mdp documentation for implementation details

**Procedure** `ComputeNextState`($s$: float, $a$: int, $\gamma$: float) $\to$ float

- Returns $(1 - \gamma) \cdot s + a$
- See solve_mdp documentation for implementation details

**Procedure** `ComputeMeanReward`($s$: float, $a$: int, $\beta$: float) $\to$ float

- Returns $\beta \cdot \log(1 + s) - a$
- See solve_mdp documentation for implementation details

:::

**Key Implementation Details:**

- **Shared configuration**: Load $\beta$, $\gamma$, $\delta$ from the same config used in solve_mdp
- **Reproducibility**: Set random seed before simulation for reproducible results
- **Categorical sampling**: Use inverse CDF method to draw actions from choice probabilities
- **Storage efficiency**: Pre-allocate arrays for states, actions, and rewards before loop

## Load Trained Policy

```{python}
import sys
sys.path.insert(0, '..')
sys.path.insert(0, '../../src')

import json
from pathlib import Path

import numpy as np
import torch
import matplotlib.pyplot as plt

from mdp_solver import MonotonicNetwork
from config_mdp import get_solver_config, get_comparative_statics
from mdp_utils import (
    plot_choice_value_functions,
    plot_convergence_history,
    plot_policy_probabilities,
)

artifact_dir = Path('../../output/sovle_mdp')
if not artifact_dir.exists():
    raise FileNotFoundError(
        f"Expected trained artifacts in {artifact_dir}. Run solve_mdp.qmd first."
    )

module_config = get_solver_config()
module_statics = get_comparative_statics()

# Load configuration written alongside the trained policy
with open(artifact_dir / 'config.json', 'r', encoding='utf-8') as config_file:
    config = json.load(config_file)

# Load precomputed grids for diagnostics
policy_store = np.load(artifact_dir / 'policy_data.npz')
state_grid = policy_store['state_grid']
v0_grid = policy_store['v0']
v1_grid = policy_store['v1']
prob_a0_grid = policy_store['prob_a0']
prob_a1_grid = policy_store['prob_a1']

history_store = np.load(artifact_dir / 'convergence_history.npz')
history_iterations = history_store['iterations']
history_max_errors = history_store['max_errors']

module_config_for_compare = module_config.copy()
module_config_for_compare['state_range'] = list(module_config_for_compare['state_range'])

stored_subset = {k: config[k] for k in module_config}
if module_config_for_compare != stored_subset:
    raise ValueError("Module configuration and stored configuration are inconsistent.")

if module_statics != config.get('comparative_statics', {}):
    raise ValueError("Comparative statics configuration mismatch.")

beta = module_config['beta']
gamma = module_config['gamma']
delta = module_config['delta']
gamma_E = module_config['gamma_E']
hyperparameters = module_config['hyperparameters']
state_range = tuple(module_config['state_range'])
hidden_sizes = hyperparameters.get('hidden_sizes', [32, 32])
epsilon_tol = module_config['epsilon_tol']

print(f"Loaded trained networks with hidden sizes {hidden_sizes}")

# Rebuild networks and load weights
v_theta_0 = MonotonicNetwork(hidden_sizes=hidden_sizes)
v_theta_1 = MonotonicNetwork(hidden_sizes=hidden_sizes)

v_theta_0.load_state_dict(torch.load(artifact_dir / 'v_theta_0.pt', map_location='cpu'))
v_theta_1.load_state_dict(torch.load(artifact_dir / 'v_theta_1.pt', map_location='cpu'))

v_theta_0.eval()
v_theta_1.eval()
```

## Stored Policy Diagnostics

```{python}
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))
plot_choice_value_functions(
    state_grid=state_grid,
    v0_values=v0_grid,
    v1_values=v1_grid,
    ax=ax1,
    title='Choice-Specific Value Functions',
)
plot_convergence_history(
    iterations=history_iterations,
    max_errors=history_max_errors,
    epsilon_tol=epsilon_tol,
    ax=ax2,
    title='Convergence History',
)
fig.tight_layout()
plt.show()
```

```{python}
ax = plot_policy_probabilities(
    state_grid=state_grid,
    prob_a0=prob_a0_grid,
    prob_a1=prob_a1_grid,
)
ax.figure.tight_layout()
plt.show()
```
