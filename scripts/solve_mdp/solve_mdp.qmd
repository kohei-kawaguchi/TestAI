---
title: "Solving Markov Decision Processes"
author: ""
format:
  html:
    code-fold: false
    toc: true
jupyter: python3
---

## Introduction

## Problem Setup

We consider a Markov Decision Process with the following components:

**State Space**: The state variable $s \in \mathbb{R}$ represents a continuous real-valued state.

**Action Space**: The action variable $a \in \{0, 1\}$ is binary.

**Reward Function**: The mean reward function is given by:
$$\bar{r}(s, a) = \beta s - a$$
where $\beta$ is a parameter that determines the weight on the state value.

The realized reward includes an action-specific shock:
$$r(s, a) = \bar{r}(s, a) + \epsilon(a) = \beta s - a + \epsilon(a)$$
where $\epsilon(a)$ represents the shock associated with action $a$.

**Reward Shocks**: The shocks $\epsilon(0)$ and $\epsilon(1)$ are:
- Independent of each other
- Independent across time periods
- Each follows a Type-I Extreme Value distribution (Gumbel distribution)

This specification leads to the discrete choice model with tractable choice probabilities.

**State Transition**: The state evolves according to the deterministic transition function:
$$s' = (1 - \gamma) s + a$$
where $\gamma \in (0, 1)$ is a depreciation parameter that determines how much the current state decays, and the action directly contributes to the next state.

**Interpretation**: In this model:
- The state $s$ can be interpreted as a resource or capital stock
- The action $a$ represents whether to invest/add to the stock ($a=1$) or not ($a=0$)
- The reward incentivizes having a high state value (through $\beta s$) but penalizes taking action ($-a$)
- The state transition shows that without action, the state decays at rate $\gamma$, while action adds to the state

## Value Iteration

```{python}
# Value iteration implementation
```

## Policy Iteration

```{python}
# Policy iteration implementation
```

## Example: Grid World

```{python}
# Grid world example
```

## Results

## Visualization

```{python}
# Visualization code
```

## Conclusion
