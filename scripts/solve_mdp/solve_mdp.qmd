---
title: "Solving Markov Decision Processes"
author: ""
format:
  html:
    code-fold: false
    toc: true
jupyter: python3
---

## Introduction

## Problem Setup

We consider a Markov Decision Process with the following components:

**State Space**: The state variable $s \in \mathbb{R}$ represents a continuous real-valued state.

**Action Space**: The action variable $a \in \{0, 1\}$ is binary.

**Reward Function**: The mean reward function is given by:
$$\bar{r}(s, a) = \beta s - a$$
where $\beta$ is a parameter that determines the weight on the state value.

The realized reward includes an action-specific shock:
$$r(s, a) = \bar{r}(s, a) + \epsilon(a) = \beta s - a + \epsilon(a)$$
where $\epsilon(a)$ represents the shock associated with action $a$.

**Reward Shocks**: The shocks $\epsilon(0)$ and $\epsilon(1)$ are:
- Independent of each other
- Independent across time periods
- Each follows a Type-I Extreme Value distribution (Gumbel distribution)

This specification leads to the discrete choice model with tractable choice probabilities.

**State Transition**: The state evolves according to the deterministic transition function:
$$s' = (1 - \gamma) s + a$$
where $\gamma \in (0, 1)$ is a depreciation parameter that determines how much the current state decays, and the action directly contributes to the next state.

**Interpretation**: In this model:
- The state $s$ can be interpreted as a resource or capital stock
- The action $a$ represents whether to invest/add to the stock ($a=1$) or not ($a=0$)
- The reward incentivizes having a high state value (through $\beta s$) but penalizes taking action ($-a$)
- The state transition shows that without action, the state decays at rate $\gamma$, while action adds to the state

## Optimal Markov Decision Policy

**Discount Factor**: Let $\delta \in (0, 1)$ denote the discount factor that determines how much the agent values future rewards relative to current rewards.

**Value Function**: The value function $V(s, \epsilon)$ represents the expected present discounted value of rewards from state $s$ with shock vector $\epsilon = (\epsilon(0), \epsilon(1))$, assuming optimal decision-making from this period onward.

**Bellman Equation**: The optimal value function satisfies the recursive Bellman equation:
$$V(s, \epsilon) = \max_{a \in \{0,1\}} \left\{ \bar{r}(s, a) + \epsilon(a) + \delta \mathbb{E}_{\epsilon'} \left[ V(s', \epsilon') \right] \right\}$$
where:
- $s' = (1 - \gamma) s + a$ is the next period state
- $\epsilon' = (\epsilon'(0), \epsilon'(1))$ represents the next period shocks
- The expectation is taken over the distribution of future shocks

**Choice-Specific Value Function**: It is useful to define the choice-specific value function:
$$v(s, a) = \bar{r}(s, a) + \delta \mathbb{E}_{\epsilon'} \left[ V(s', \epsilon') \right]$$
which represents the expected value of choosing action $a$ in state $s$, before observing the shock $\epsilon(a)$.

The Bellman equation can then be rewritten as:
$$V(s, \epsilon) = \max_{a \in \{0,1\}} \left\{ v(s, a) + \epsilon(a) \right\}$$

**Optimal Policy**: The optimal policy is the decision rule that maximizes the value function:
$$a^*(s, \epsilon) = \arg\max_{a \in \{0,1\}} \left\{ v(s, a) + \epsilon(a) \right\}$$

Given the Type-I Extreme Value distribution of the shocks, the probability of choosing action $a$ follows the logit formula:
$$P(a | s) = \frac{\exp(v(s, a))}{\sum_{a' \in \{0,1\}} \exp(v(s, a'))}$$

## Value Iteration

### Neural Network Architecture for Choice-Specific Value Functions

Since the action space is binary, we represent the choice-specific value function using **two separate neural networks**:
- $v_\theta^{(0)}(s)$ for action $a=0$
- $v_\theta^{(1)}(s)$ for action $a=1$

Each network maps the continuous state $s \in \mathbb{R}$ to a scalar value.

**Network Design Requirements:**

1. **Smoothness in $s$**: We use smooth activation functions (tanh or softplus) to ensure the value function is differentiable with respect to the state.

2. **Monotonicity in $s$**: Since higher state values should lead to higher rewards (due to the $\beta s$ term), we enforce monotonicity by constraining the network weights to be non-negative. This is achieved by applying the softplus transformation:
   $$w = \text{softplus}(\tilde{w}) = \log(1 + \exp(\tilde{w}))$$
   where $\tilde{w}$ are unconstrained parameters. The softplus function is used instead of exponential to avoid weight explosion.

3. **Shallow Architecture**: Given that the reward function is linear in $s$ and the state space is one-dimensional, we use a shallow network (2-3 hidden layers) with moderate width (e.g., 32-64 units).

**Network Structure:**
$$v_\theta^{(a)}(s) = W_L^{(a)} h_{L-1} + b_L^{(a)}$$
where:
- $h_0 = s$ (input)
- $h_\ell = \sigma(W_\ell^{(a)} h_{\ell-1} + b_\ell^{(a)})$ for $\ell = 1, \ldots, L-1$
- $W_\ell^{(a)} = \text{softplus}(\tilde{W}_\ell^{(a)})$ (non-negative weights)
- $\sigma$ is a smooth activation function (e.g., tanh)

### Value Function Iteration Algorithm

The value iteration algorithm solves for the optimal choice-specific value functions by iteratively applying the Bellman operator.

**Bellman Update for Choice-Specific Value Function:**

Given the current value function approximations $v_\theta^{(0)}$ and $v_\theta^{(1)}$, we can compute the expected value of the next state:
$$\mathbb{E}_{\epsilon'} [V(s', \epsilon')] = \log\left(\sum_{a' \in \{0,1\}} \exp(v_\theta^{(a')}(s'))\right) + \gamma_E$$
where $\gamma_E \approx 0.5772$ is the Euler-Mascheroni constant (this formula uses the property of the Type-I Extreme Value distribution).

The Bellman update for the choice-specific value function is:
$$v_{\text{new}}(s, a) = \bar{r}(s, a) + \delta \mathbb{E}_{\epsilon'} [V(s', \epsilon')]$$
$$= \beta s - a + \delta \log\left(\sum_{a' \in \{0,1\}} \exp(v_\theta^{(a')}((1-\gamma)s + a))\right) + \delta \gamma_E$$

**Algorithm:**

1. **Initialize**: Set $v_\theta^{(0)}$ and $v_\theta^{(1)}$ to initial values (e.g., zeros or small random values)

2. **Sample states**: Generate a set of state values $\{s_i\}_{i=1}^N$ covering the relevant state space

3. **Iterate** until convergence:

   a. For each sampled state $s_i$ and action $a \in \{0,1\}$:
      - Compute next state: $s'_i = (1-\gamma)s_i + a$
      - Compute expected continuation value:
        $$\text{EV}_i = \log\left(\exp(v_\theta^{(0)}(s'_i)) + \exp(v_\theta^{(1)}(s'_i))\right) + \gamma_E$$
      - Compute target value:
        $$y_i^{(a)} = \beta s_i - a + \delta \cdot \text{EV}_i$$

   b. Update network parameters $\theta$ by minimizing the loss:
      $$L(\theta) = \sum_{i=1}^N \sum_{a \in \{0,1\}} \left(v_\theta^{(a)}(s_i) - y_i^{(a)}\right)^2$$

   c. Check convergence: If $\max_{i,a} |v_\theta^{(a)}(s_i) - y_i^{(a)}| < \epsilon_{\text{tol}}$, stop

4. **Output**: The converged networks $v_\theta^{(0)}$ and $v_\theta^{(1)}$ approximate the optimal choice-specific value functions

**Convergence**: The value iteration algorithm is guaranteed to converge to the unique fixed point by the Contraction Mapping Theorem, since the Bellman operator is a contraction with modulus $\delta < 1$.

### Pseudo Code

::: {.callout-note appearance="simple"}

## Algorithm: Value Function Iteration with Neural Networks

**Input:**

- Parameters: $\beta$, $\gamma$, $\delta$, $\gamma_E$ (Euler-Mascheroni constant)
- Network hyperparameters: `num_layers`, `hidden_size`, `learning_rate`
- Training hyperparameters: $N$ (`num_states`), `max_iter`, $\epsilon_{\text{tol}}$, `num_epochs`

**Output:**

- Trained networks $v_\theta^{(0)}$ and $v_\theta^{(1)}$

---

### Main Algorithm

**Procedure** `SolveValueIteration`($\beta$: float, $\gamma$: float, $\delta$: float, $\gamma_E$: float, hyperparameters: dict, $N$: int, state_range: tuple[float, float], max_iter: int, $\epsilon_{\text{tol}}$: float, num_epochs: int, learning_rate: float, optimizer: Optimizer) $\to$ (Network, Network)

1. $v_\theta^{(0)}$: Network, $v_\theta^{(1)}$: Network $\leftarrow$ `InitializeNetworks`(hyperparameters)

2. $S$: Tensor[$N \times 1$] $\leftarrow$ `GenerateStateGrid`($N$, state_range)

3. For `iteration`: int = 1 to max_iter:

   a. $\{y_i^{(a)}\}$: Tensor[$N \times 2$] $\leftarrow$ `ComputeBellmanTargets`($S$, $v_\theta^{(0)}$, $v_\theta^{(1)}$, $\beta$, $\gamma$, $\delta$, $\gamma_E$)

   b. $v_\theta^{(0)}$, $v_\theta^{(1)}$ $\leftarrow$ `UpdateNetworks`($S$, $\{y_i^{(a)}\}$, $v_\theta^{(0)}$, $v_\theta^{(1)}$, num_epochs, optimizer)

   c. max_error: float $\leftarrow$ `CheckConvergence`($S$, $\{y_i^{(a)}\}$, $v_\theta^{(0)}$, $v_\theta^{(1)}$)

   d. **If** max_error $< \epsilon_{\text{tol}}$: **Break**

4. **Return** $v_\theta^{(0)}$, $v_\theta^{(1)}$

---

### Subroutines

**Procedure** `InitializeNetworks`(hyperparameters: dict) $\to$ (Network, Network)

- Create networks $v_\theta^{(0)}$: Network and $v_\theta^{(1)}$: Network with monotonic weight constraints
- Initialize parameters $\theta$ randomly
- **Return** $v_\theta^{(0)}$, $v_\theta^{(1)}$

---

**Procedure** `GenerateStateGrid`($N$: int, state_range: tuple[float, float]) $\to$ Tensor[$N \times 1$]

- Create uniform grid: $S$: Tensor[$N \times 1$] = $\{s_1, \ldots, s_N\}$ over state_range
- **Return** $S$

---

**Procedure** `ComputeBellmanTargets`($S$: Tensor[$N \times 1$], $v_\theta^{(0)}$: Network, $v_\theta^{(1)}$: Network, $\beta$: float, $\gamma$: float, $\delta$: float, $\gamma_E$: float) $\to$ Tensor[$N \times 2$]

For each $s_i$: float $\in S$ and $a$: int $\in \{0, 1\}$:

&nbsp;&nbsp;&nbsp;&nbsp;$s'_i$: Tensor[$N \times 1$] $\leftarrow$ `ComputeNextState`($s_i$, $a$, $\gamma$)

&nbsp;&nbsp;&nbsp;&nbsp;$\text{EV}_i$: Tensor[$N \times 1$] $\leftarrow$ `ComputeExpectedValue`($s'_i$, $v_\theta^{(0)}$, $v_\theta^{(1)}$, $\gamma_E$)

&nbsp;&nbsp;&nbsp;&nbsp;$y_i^{(a)}$: Tensor[$N \times 1$] $\leftarrow$ `ComputeMeanReward`($s_i$, $a$, $\beta$) $+ \delta \cdot \text{EV}_i$

**Return** $\{y_i^{(a)}\}$: Tensor[$N \times 2$] for all $i$, $a$

---

**Procedure** `ComputeNextState`($s$: Tensor[$N \times 1$], $a$: int, $\gamma$: float) $\to$ Tensor[$N \times 1$]

- **Return** $(1 - \gamma) \cdot s + a$

---

**Procedure** `ComputeMeanReward`($s$: Tensor[$N \times 1$], $a$: int, $\beta$: float) $\to$ Tensor[$N \times 1$]

- **Return** $\beta \cdot s - a$

---

**Procedure** `ComputeExpectedValue`($s'$: Tensor[$N \times 1$], $v_\theta^{(0)}$: Network, $v_\theta^{(1)}$: Network, $\gamma_E$: float) $\to$ Tensor[$N \times 1$]

- $v_0$: Tensor[$N \times 1$] $\leftarrow v_\theta^{(0)}(s')$
- $v_1$: Tensor[$N \times 1$] $\leftarrow v_\theta^{(1)}(s')$
- $\text{EV}$: Tensor[$N \times 1$] $\leftarrow$ `LogSumExp`($v_0$, $v_1$) $+ \gamma_E$
- **Return** EV

---

**Procedure** `LogSumExp`($v_0$: Tensor[$N \times 1$], $v_1$: Tensor[$N \times 1$]) $\to$ Tensor[$N \times 1$]

- $\text{max}_v$: Tensor[$N \times 1$] $\leftarrow \max(v_0, v_1)$
- **Return** $\text{max}_v + \log(\exp(v_0 - \text{max}_v) + \exp(v_1 - \text{max}_v))$

---

**Procedure** `UpdateNetworks`($S$: Tensor[$N \times 1$], $\{y_i^{(a)}\}$: Tensor[$N \times 2$], $v_\theta^{(0)}$: Network, $v_\theta^{(1)}$: Network, num_epochs: int, optimizer: Optimizer) $\to$ (Network, Network)

For `epoch`: int = 1 to num_epochs:

&nbsp;&nbsp;&nbsp;&nbsp;optimizer.zero_grad()

&nbsp;&nbsp;&nbsp;&nbsp;$L$: float $\leftarrow$ `ComputeLoss`($S$, $\{y_i^{(a)}\}$, $v_\theta^{(0)}$, $v_\theta^{(1)}$)

&nbsp;&nbsp;&nbsp;&nbsp;$L$.backward()

&nbsp;&nbsp;&nbsp;&nbsp;optimizer.step()

**Return** $v_\theta^{(0)}$, $v_\theta^{(1)}$

---

**Procedure** `ComputeLoss`($S$: Tensor[$N \times 1$], $\{y_i^{(a)}\}$: Tensor[$N \times 2$], $v_\theta^{(0)}$: Network, $v_\theta^{(1)}$: Network) $\to$ float

- $L$: float $\leftarrow \sum_{i=1}^N \sum_{a \in \{0,1\}} (v_\theta^{(a)}(s_i) - y_i^{(a)})^2$
- **Return** $L$

---

**Procedure** `CheckConvergence`($S$: Tensor[$N \times 1$], $\{y_i^{(a)}\}$: Tensor[$N \times 2$], $v_\theta^{(0)}$: Network, $v_\theta^{(1)}$: Network) $\to$ float

- max_error: float $\leftarrow \max_{i,a} |v_\theta^{(a)}(s_i) - y_i^{(a)}|$
- **Return** max_error

:::

**Key Implementation Details:**

- **Monotonic network constraint**: Apply softplus transformation to weight matrices during forward pass
- **Numerical stability**: Use log-sum-exp trick: $\log(\exp(a) + \exp(b)) = \max(a,b) + \log(1 + \exp(-|a-b|))$
- **State sampling**: Use uniform grid over state_range parameter
- **Optimizer**: Use adaptive optimizers such as Adam for faster convergence
- **Batch updates**: Process all states in parallel for efficiency

```{python}
# Import the MDP solver from the test_ai package
# Set parameters: beta, gamma, delta
# Create MDPSolver instance
# Solve the MDP using value iteration
```

## Results

```{python}
# Plot convergence history
```

## Visualization

```{python}
# Plot value functions for both actions
# Plot optimal policy (choice probabilities)
```

## Conclusion
