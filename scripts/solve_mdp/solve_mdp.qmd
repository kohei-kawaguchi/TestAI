---
title: "Solving Markov Decision Processes"
author: ""
format:
  html:
    code-fold: false
    toc: true
jupyter: python3
---

## Introduction

## Problem Setup

We consider a Markov Decision Process with the following components:

**State Space**: The state variable $s \in \mathbb{R}$ represents a continuous real-valued state.

**Action Space**: The action variable $a \in \{0, 1\}$ is binary.

**Reward Function**: The mean reward function is given by:
$$\bar{r}(s, a) = \beta s - a$$
where $\beta$ is a parameter that determines the weight on the state value.

The realized reward includes an action-specific shock:
$$r(s, a) = \bar{r}(s, a) + \epsilon(a) = \beta s - a + \epsilon(a)$$
where $\epsilon(a)$ represents the shock associated with action $a$.

**Reward Shocks**: The shocks $\epsilon(0)$ and $\epsilon(1)$ are:
- Independent of each other
- Independent across time periods
- Each follows a Type-I Extreme Value distribution (Gumbel distribution)

This specification leads to the discrete choice model with tractable choice probabilities.

**State Transition**: The state evolves according to the deterministic transition function:
$$s' = (1 - \gamma) s + a$$
where $\gamma \in (0, 1)$ is a depreciation parameter that determines how much the current state decays, and the action directly contributes to the next state.

**Interpretation**: In this model:
- The state $s$ can be interpreted as a resource or capital stock
- The action $a$ represents whether to invest/add to the stock ($a=1$) or not ($a=0$)
- The reward incentivizes having a high state value (through $\beta s$) but penalizes taking action ($-a$)
- The state transition shows that without action, the state decays at rate $\gamma$, while action adds to the state

## Optimal Markov Decision Policy

**Discount Factor**: Let $\delta \in (0, 1)$ denote the discount factor that determines how much the agent values future rewards relative to current rewards.

**Value Function**: The value function $V(s, \epsilon)$ represents the expected present discounted value of rewards from state $s$ with shock vector $\epsilon = (\epsilon(0), \epsilon(1))$, assuming optimal decision-making from this period onward.

**Bellman Equation**: The optimal value function satisfies the recursive Bellman equation:
$$V(s, \epsilon) = \max_{a \in \{0,1\}} \left\{ \bar{r}(s, a) + \epsilon(a) + \delta \mathbb{E}_{\epsilon'} \left[ V(s', \epsilon') \right] \right\}$$
where:
- $s' = (1 - \gamma) s + a$ is the next period state
- $\epsilon' = (\epsilon'(0), \epsilon'(1))$ represents the next period shocks
- The expectation is taken over the distribution of future shocks

**Choice-Specific Value Function**: It is useful to define the choice-specific value function:
$$v(s, a) = \bar{r}(s, a) + \delta \mathbb{E}_{\epsilon'} \left[ V(s', \epsilon') \right]$$
which represents the expected value of choosing action $a$ in state $s$, before observing the shock $\epsilon(a)$.

The Bellman equation can then be rewritten as:
$$V(s, \epsilon) = \max_{a \in \{0,1\}} \left\{ v(s, a) + \epsilon(a) \right\}$$

**Optimal Policy**: The optimal policy is the decision rule that maximizes the value function:
$$a^*(s, \epsilon) = \arg\max_{a \in \{0,1\}} \left\{ v(s, a) + \epsilon(a) \right\}$$

Given the Type-I Extreme Value distribution of the shocks, the probability of choosing action $a$ follows the logit formula:
$$P(a | s) = \frac{\exp(v(s, a))}{\sum_{a' \in \{0,1\}} \exp(v(s, a'))}$$

## Value Iteration

### Neural Network Architecture for Choice-Specific Value Functions

Since the action space is binary, we represent the choice-specific value function using **two separate neural networks**:
- $v_\theta^{(0)}(s)$ for action $a=0$
- $v_\theta^{(1)}(s)$ for action $a=1$

Each network maps the continuous state $s \in \mathbb{R}$ to a scalar value.

**Network Design Requirements:**

1. **Smoothness in $s$**: We use smooth activation functions (tanh or softplus) to ensure the value function is differentiable with respect to the state.

2. **Monotonicity in $s$**: Since higher state values should lead to higher rewards (due to the $\beta s$ term), we enforce monotonicity by constraining the network weights to be non-negative. This is achieved by applying the softplus transformation:
   $$w = \text{softplus}(\tilde{w}) = \log(1 + \exp(\tilde{w}))$$
   where $\tilde{w}$ are unconstrained parameters. The softplus function is used instead of exponential to avoid weight explosion.

3. **Shallow Architecture**: Given that the reward function is linear in $s$ and the state space is one-dimensional, we use a shallow network (2-3 hidden layers) with moderate width (e.g., 32-64 units).

**Network Structure:**
$$v_\theta^{(a)}(s) = W_L^{(a)} h_{L-1} + b_L^{(a)}$$
where:
- $h_0 = s$ (input)
- $h_\ell = \sigma(W_\ell^{(a)} h_{\ell-1} + b_\ell^{(a)})$ for $\ell = 1, \ldots, L-1$
- $W_\ell^{(a)} = \text{softplus}(\tilde{W}_\ell^{(a)})$ (non-negative weights)
- $\sigma$ is a smooth activation function (e.g., tanh)

### Value Function Iteration Algorithm

The value iteration algorithm solves for the optimal choice-specific value functions by iteratively applying the Bellman operator.

**Bellman Update for Choice-Specific Value Function:**

Given the current value function approximations $v_\theta^{(0)}$ and $v_\theta^{(1)}$, we can compute the expected value of the next state:
$$\mathbb{E}_{\epsilon'} [V(s', \epsilon')] = \log\left(\sum_{a' \in \{0,1\}} \exp(v_\theta^{(a')}(s'))\right) + \gamma_E$$
where $\gamma_E \approx 0.5772$ is the Euler-Mascheroni constant (this formula uses the property of the Type-I Extreme Value distribution).

The Bellman update for the choice-specific value function is:
$$v_{\text{new}}(s, a) = \bar{r}(s, a) + \delta \mathbb{E}_{\epsilon'} [V(s', \epsilon')]$$
$$= \beta s - a + \delta \log\left(\sum_{a' \in \{0,1\}} \exp(v_\theta^{(a')}((1-\gamma)s + a))\right) + \delta \gamma_E$$

**Algorithm:**

1. **Initialize**: Set $v_\theta^{(0)}$ and $v_\theta^{(1)}$ to initial values (e.g., zeros or small random values)

2. **Sample states**: Generate a set of state values $\{s_i\}_{i=1}^N$ covering the relevant state space

3. **Iterate** until convergence:

   a. For each sampled state $s_i$ and action $a \in \{0,1\}$:
      - Compute next state: $s'_i = (1-\gamma)s_i + a$
      - Compute expected continuation value:
        $$\text{EV}_i = \log\left(\exp(v_\theta^{(0)}(s'_i)) + \exp(v_\theta^{(1)}(s'_i))\right) + \gamma_E$$
      - Compute target value:
        $$y_i^{(a)} = \beta s_i - a + \delta \cdot \text{EV}_i$$

   b. Update network parameters $\theta$ by minimizing the loss:
      $$L(\theta) = \sum_{i=1}^N \sum_{a \in \{0,1\}} \left(v_\theta^{(a)}(s_i) - y_i^{(a)}\right)^2$$

   c. Check convergence: If $\max_{i,a} |v_\theta^{(a)}(s_i) - y_i^{(a)}| < \epsilon_{\text{tol}}$, stop

4. **Output**: The converged networks $v_\theta^{(0)}$ and $v_\theta^{(1)}$ approximate the optimal choice-specific value functions

**Convergence**: The value iteration algorithm is guaranteed to converge to the unique fixed point by the Contraction Mapping Theorem, since the Bellman operator is a contraction with modulus $\delta < 1$.

### Pseudo Code

::: {.callout-note appearance="simple"}

## Algorithm: Value Function Iteration with Neural Networks

**Input:**

- Parameters: $\beta$, $\gamma$, $\delta$, $\gamma_E$ (Euler-Mascheroni constant)
- Network hyperparameters: `num_layers`, `hidden_size`, `learning_rate`
- Training hyperparameters: $N$ (`num_states`), `max_iter`, $\epsilon_{\text{tol}}$, `num_epochs`

**Output:**

- Trained networks $v_\theta^{(0)}$ and $v_\theta^{(1)}$

---

**Step 1: Initialize**

- Create two neural networks $v_\theta^{(0)}$ and $v_\theta^{(1)}$ with monotonic constraints
- Initialize network parameters $\theta$ randomly

**Step 2: Generate state samples**

- $S = \{s_1, s_2, \ldots, s_N\}$ covering the relevant state space

**Step 3: Iterate until convergence**

For `iteration` = 1 to `max_iter`:

&nbsp;&nbsp;&nbsp;&nbsp;**3a. Compute targets for all states and actions:**

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;For each $s_i \in S$:

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;For each action $a \in \{0, 1\}$:

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$s'_i = (1 - \gamma) \cdot s_i + a$

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$v_0 = v_\theta^{(0)}(s'_i)$

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$v_1 = v_\theta^{(1)}(s'_i)$

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\text{EV}_i = \log(\exp(v_0) + \exp(v_1)) + \gamma_E$

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$y_i^{(a)} = \beta \cdot s_i - a + \delta \cdot \text{EV}_i$

&nbsp;&nbsp;&nbsp;&nbsp;**3b. Update networks (gradient descent):**

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;For `epoch` = 1 to `num_epochs`:

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Compute loss: $L = \sum_i \sum_a (v_\theta^{(a)}(s_i) - y_i^{(a)})^2$

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Compute gradients: $\nabla_\theta L$

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Update parameters: $\theta \leftarrow \theta - \text{learning\_rate} \cdot \nabla_\theta L$

&nbsp;&nbsp;&nbsp;&nbsp;**3c. Check convergence:**

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Compute $\text{max\_error} = \max_{i,a} |v_\theta^{(a)}(s_i) - y_i^{(a)}|$

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**If** $\text{max\_error} < \epsilon_{\text{tol}}$:

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Break (converged)

&nbsp;&nbsp;&nbsp;&nbsp;**3d. (Optional) Print progress:**

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Print iteration number and max_error

**Step 4: Return**

- Return $v_\theta^{(0)}$ and $v_\theta^{(1)}$

:::

**Key Implementation Details:**

- **Monotonic network constraint**: Apply softplus transformation to weight matrices during forward pass
- **Numerical stability**: Use log-sum-exp trick: $\log(\exp(a) + \exp(b)) = \max(a,b) + \log(1 + \exp(-|a-b|))$
- **State sampling**: Use uniform grid or random sampling over a reasonable state range (e.g., [0, 10])
- **Learning rate**: Use adaptive optimizers (e.g., Adam) for faster convergence
- **Batch updates**: Can process all states in parallel for efficiency

```{python}
# Value iteration implementation
```

## Policy Iteration

```{python}
# Policy iteration implementation
```

## Example: Grid World

```{python}
# Grid world example
```

## Results

## Visualization

```{python}
# Visualization code
```

## Conclusion
