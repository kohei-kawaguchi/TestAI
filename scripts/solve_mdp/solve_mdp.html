<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.550">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="">

<title>Solving Markov Decision Processes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="solve_mdp_files/libs/clipboard/clipboard.min.js"></script>
<script src="solve_mdp_files/libs/quarto-html/quarto.js"></script>
<script src="solve_mdp_files/libs/quarto-html/popper.min.js"></script>
<script src="solve_mdp_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="solve_mdp_files/libs/quarto-html/anchor.min.js"></script>
<link href="solve_mdp_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="solve_mdp_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="solve_mdp_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="solve_mdp_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="solve_mdp_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#problem-setup" id="toc-problem-setup" class="nav-link" data-scroll-target="#problem-setup">Problem Setup</a></li>
  <li><a href="#optimal-markov-decision-policy" id="toc-optimal-markov-decision-policy" class="nav-link" data-scroll-target="#optimal-markov-decision-policy">Optimal Markov Decision Policy</a></li>
  <li><a href="#value-iteration" id="toc-value-iteration" class="nav-link" data-scroll-target="#value-iteration">Value Iteration</a>
  <ul class="collapse">
  <li><a href="#neural-network-architecture-for-choice-specific-value-functions" id="toc-neural-network-architecture-for-choice-specific-value-functions" class="nav-link" data-scroll-target="#neural-network-architecture-for-choice-specific-value-functions">Neural Network Architecture for Choice-Specific Value Functions</a></li>
  <li><a href="#value-function-iteration-algorithm" id="toc-value-function-iteration-algorithm" class="nav-link" data-scroll-target="#value-function-iteration-algorithm">Value Function Iteration Algorithm</a></li>
  <li><a href="#pseudo-code" id="toc-pseudo-code" class="nav-link" data-scroll-target="#pseudo-code">Pseudo Code</a></li>
  </ul></li>
  <li><a href="#results" id="toc-results" class="nav-link" data-scroll-target="#results">Results</a>
  <ul class="collapse">
  <li><a href="#convergence-plot" id="toc-convergence-plot" class="nav-link" data-scroll-target="#convergence-plot">Convergence Plot</a></li>
  </ul></li>
  <li><a href="#visualization" id="toc-visualization" class="nav-link" data-scroll-target="#visualization">Visualization</a>
  <ul class="collapse">
  <li><a href="#choice-specific-value-functions" id="toc-choice-specific-value-functions" class="nav-link" data-scroll-target="#choice-specific-value-functions">Choice-Specific Value Functions</a></li>
  <li><a href="#optimal-policy" id="toc-optimal-policy" class="nav-link" data-scroll-target="#optimal-policy">Optimal Policy</a></li>
  </ul></li>
  <li><a href="#comparative-statics" id="toc-comparative-statics" class="nav-link" data-scroll-target="#comparative-statics">Comparative Statics</a>
  <ul class="collapse">
  <li><a href="#value-functions-across-β" id="toc-value-functions-across-β" class="nav-link" data-scroll-target="#value-functions-across-β">Value Functions Across β</a></li>
  <li><a href="#optimal-policy-across-β" id="toc-optimal-policy-across-β" class="nav-link" data-scroll-target="#optimal-policy-across-β">Optimal Policy Across β</a></li>
  <li><a href="#comparative-statics-for-γ" id="toc-comparative-statics-for-γ" class="nav-link" data-scroll-target="#comparative-statics-for-γ">Comparative Statics for γ</a></li>
  <li><a href="#value-functions-across-γ" id="toc-value-functions-across-γ" class="nav-link" data-scroll-target="#value-functions-across-γ">Value Functions Across γ</a></li>
  <li><a href="#optimal-policy-across-γ" id="toc-optimal-policy-across-γ" class="nav-link" data-scroll-target="#optimal-policy-across-γ">Optimal Policy Across γ</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a>
  <ul class="collapse">
  <li><a href="#interpretation-of-comparative-statics" id="toc-interpretation-of-comparative-statics" class="nav-link" data-scroll-target="#interpretation-of-comparative-statics">Interpretation of Comparative Statics</a></li>
  </ul></li>
  </ul>
</nav>
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Solving Markov Decision Processes</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>This document demonstrates solving a Markov Decision Process using neural network function approximation and value iteration.</p>
</section>
<section id="problem-setup" class="level2">
<h2 class="anchored" data-anchor-id="problem-setup">Problem Setup</h2>
<p>We consider a Markov Decision Process with the following components:</p>
<p><strong>State Space</strong>: The state variable <span class="math inline">\(s \in \mathbb{R}\)</span> represents a continuous real-valued state.</p>
<p><strong>Action Space</strong>: The action variable <span class="math inline">\(a \in \{0, 1\}\)</span> is binary.</p>
<p><strong>Reward Function</strong>: The mean reward function is given by: <span class="math display">\[\bar{r}(s, a) = \beta \log(1 + s) - a\]</span> where <span class="math inline">\(\beta\)</span> is a parameter that determines the weight on the state value, and the logarithmic form captures diminishing marginal returns to the state.</p>
<p>The realized reward includes an action-specific shock: <span class="math display">\[r(s, a) = \bar{r}(s, a) + \epsilon(a) = \beta \log(1 + s) - a + \epsilon(a)\]</span> where <span class="math inline">\(\epsilon(a)\)</span> represents the shock associated with action <span class="math inline">\(a\)</span>.</p>
<p><strong>Reward Shocks</strong>: The shocks <span class="math inline">\(\epsilon(0)\)</span> and <span class="math inline">\(\epsilon(1)\)</span> are: - Independent of each other - Independent across time periods - Each follows a Type-I Extreme Value distribution (Gumbel distribution)</p>
<p>This specification leads to the discrete choice model with tractable choice probabilities.</p>
<p><strong>State Transition</strong>: The state evolves according to the deterministic transition function: <span class="math display">\[s' = (1 - \gamma) s + a\]</span> where <span class="math inline">\(\gamma \in (0, 1)\)</span> is a depreciation parameter that determines how much the current state decays, and the action directly contributes to the next state.</p>
<p><strong>Interpretation</strong>: In this model: - The state <span class="math inline">\(s\)</span> can be interpreted as a resource or capital stock - The action <span class="math inline">\(a\)</span> represents whether to invest/add to the stock (<span class="math inline">\(a=1\)</span>) or not (<span class="math inline">\(a=0\)</span>) - The reward incentivizes having a high state value (through <span class="math inline">\(\beta \log(1 + s)\)</span>) with diminishing marginal returns, but penalizes taking action (<span class="math inline">\(-a\)</span>) - The logarithmic reward captures the idea that additional units of state become less valuable as the state increases - The state transition shows that without action, the state decays at rate <span class="math inline">\(\gamma\)</span>, while action adds to the state</p>
</section>
<section id="optimal-markov-decision-policy" class="level2">
<h2 class="anchored" data-anchor-id="optimal-markov-decision-policy">Optimal Markov Decision Policy</h2>
<p><strong>Discount Factor</strong>: Let <span class="math inline">\(\delta \in (0, 1)\)</span> denote the discount factor that determines how much the agent values future rewards relative to current rewards.</p>
<p><strong>Value Function</strong>: The value function <span class="math inline">\(V(s, \epsilon)\)</span> represents the expected present discounted value of rewards from state <span class="math inline">\(s\)</span> with shock vector <span class="math inline">\(\epsilon = (\epsilon(0), \epsilon(1))\)</span>, assuming optimal decision-making from this period onward.</p>
<p><strong>Bellman Equation</strong>: The optimal value function satisfies the recursive Bellman equation: <span class="math display">\[V(s, \epsilon) = \max_{a \in \{0,1\}} \left\{ \bar{r}(s, a) + \epsilon(a) + \delta \mathbb{E}_{\epsilon'} \left[ V(s', \epsilon') \right] \right\}\]</span> where: - <span class="math inline">\(s' = (1 - \gamma) s + a\)</span> is the next period state - <span class="math inline">\(\epsilon' = (\epsilon'(0), \epsilon'(1))\)</span> represents the next period shocks - The expectation is taken over the distribution of future shocks</p>
<p><strong>Choice-Specific Value Function</strong>: It is useful to define the choice-specific value function: <span class="math display">\[v(s, a) = \bar{r}(s, a) + \delta \mathbb{E}_{\epsilon'} \left[ V(s', \epsilon') \right]\]</span> which represents the expected value of choosing action <span class="math inline">\(a\)</span> in state <span class="math inline">\(s\)</span>, before observing the shock <span class="math inline">\(\epsilon(a)\)</span>.</p>
<p>The Bellman equation can then be rewritten as: <span class="math display">\[V(s, \epsilon) = \max_{a \in \{0,1\}} \left\{ v(s, a) + \epsilon(a) \right\}\]</span></p>
<p><strong>Optimal Policy</strong>: The optimal policy is the decision rule that maximizes the value function: <span class="math display">\[a^*(s, \epsilon) = \arg\max_{a \in \{0,1\}} \left\{ v(s, a) + \epsilon(a) \right\}\]</span></p>
<p>Given the Type-I Extreme Value distribution of the shocks, the probability of choosing action <span class="math inline">\(a\)</span> follows the logit formula: <span class="math display">\[P(a | s) = \frac{\exp(v(s, a))}{\sum_{a' \in \{0,1\}} \exp(v(s, a'))}\]</span></p>
</section>
<section id="value-iteration" class="level2">
<h2 class="anchored" data-anchor-id="value-iteration">Value Iteration</h2>
<section id="neural-network-architecture-for-choice-specific-value-functions" class="level3">
<h3 class="anchored" data-anchor-id="neural-network-architecture-for-choice-specific-value-functions">Neural Network Architecture for Choice-Specific Value Functions</h3>
<p>Since the action space is binary, we represent the choice-specific value function using <strong>two separate neural networks</strong>: - <span class="math inline">\(v_\theta^{(0)}(s)\)</span> for action <span class="math inline">\(a=0\)</span> - <span class="math inline">\(v_\theta^{(1)}(s)\)</span> for action <span class="math inline">\(a=1\)</span></p>
<p>Each network maps the continuous state <span class="math inline">\(s \in \mathbb{R}\)</span> to a scalar value.</p>
<p><strong>Network Design Requirements:</strong></p>
<ol type="1">
<li><p><strong>Smoothness in <span class="math inline">\(s\)</span></strong>: We use smooth activation functions (tanh or softplus) to ensure the value function is differentiable with respect to the state.</p></li>
<li><p><strong>Monotonicity in <span class="math inline">\(s\)</span></strong>: Since higher state values should lead to higher rewards (due to the <span class="math inline">\(\beta \log(1 + s)\)</span> term), we enforce monotonicity by constraining the network weights to be non-negative. This is achieved by applying the softplus transformation: <span class="math display">\[w = \text{softplus}(\tilde{w}) = \log(1 + \exp(\tilde{w}))\]</span> where <span class="math inline">\(\tilde{w}\)</span> are unconstrained parameters. The softplus function is used instead of exponential to avoid weight explosion.</p></li>
<li><p><strong>Shallow Architecture</strong>: Given that the reward function is concave and logarithmic in <span class="math inline">\(s\)</span> and the state space is one-dimensional, we use a shallow network (2-3 hidden layers) with moderate width (e.g., 32-64 units).</p></li>
</ol>
<p><strong>Network Structure:</strong> <span class="math display">\[v_\theta^{(a)}(s) = W_L^{(a)} h_{L-1} + b_L^{(a)}\]</span> where: - <span class="math inline">\(h_0 = s\)</span> (input) - <span class="math inline">\(h_\ell = \sigma(W_\ell^{(a)} h_{\ell-1} + b_\ell^{(a)})\)</span> for <span class="math inline">\(\ell = 1, \ldots, L-1\)</span> - <span class="math inline">\(W_\ell^{(a)} = \text{softplus}(\tilde{W}_\ell^{(a)})\)</span> (non-negative weights) - <span class="math inline">\(\sigma\)</span> is a smooth activation function (e.g., tanh)</p>
</section>
<section id="value-function-iteration-algorithm" class="level3">
<h3 class="anchored" data-anchor-id="value-function-iteration-algorithm">Value Function Iteration Algorithm</h3>
<p>The value iteration algorithm solves for the optimal choice-specific value functions by iteratively applying the Bellman operator.</p>
<p><strong>Bellman Update for Choice-Specific Value Function:</strong></p>
<p>Given the current value function approximations <span class="math inline">\(v_\theta^{(0)}\)</span> and <span class="math inline">\(v_\theta^{(1)}\)</span>, we can compute the expected value of the next state: <span class="math display">\[\mathbb{E}_{\epsilon'} [V(s', \epsilon')] = \log\left(\sum_{a' \in \{0,1\}} \exp(v_\theta^{(a')}(s'))\right) + \gamma_E\]</span> where <span class="math inline">\(\gamma_E \approx 0.5772\)</span> is the Euler-Mascheroni constant (this formula uses the property of the Type-I Extreme Value distribution).</p>
<p>The Bellman update for the choice-specific value function is: <span class="math display">\[v_{\text{new}}(s, a) = \bar{r}(s, a) + \delta \mathbb{E}_{\epsilon'} [V(s', \epsilon')]\]</span> <span class="math display">\[= \beta \log(1 + s) - a + \delta \log\left(\sum_{a' \in \{0,1\}} \exp(v_\theta^{(a')}((1-\gamma)s + a))\right) + \delta \gamma_E\]</span></p>
<p><strong>Algorithm:</strong></p>
<ol type="1">
<li><p><strong>Initialize</strong>: Set <span class="math inline">\(v_\theta^{(0)}\)</span> and <span class="math inline">\(v_\theta^{(1)}\)</span> to initial values (e.g., zeros or small random values)</p></li>
<li><p><strong>Sample states</strong>: Generate a set of state values <span class="math inline">\(\{s_i\}_{i=1}^N\)</span> covering the relevant state space</p></li>
<li><p><strong>Iterate</strong> until convergence:</p>
<ol type="a">
<li><p>For each sampled state <span class="math inline">\(s_i\)</span> and action <span class="math inline">\(a \in \{0,1\}\)</span>:</p>
<ul>
<li>Compute next state: <span class="math inline">\(s'_i = (1-\gamma)s_i + a\)</span></li>
<li>Compute expected continuation value: <span class="math display">\[\text{EV}_i = \log\left(\exp(v_\theta^{(0)}(s'_i)) + \exp(v_\theta^{(1)}(s'_i))\right) + \gamma_E\]</span></li>
<li>Compute target value: <span class="math display">\[y_i^{(a)} = \beta \log(1 + s_i) - a + \delta \cdot \text{EV}_i\]</span></li>
</ul></li>
<li><p>Update network parameters <span class="math inline">\(\theta\)</span> by minimizing the loss: <span class="math display">\[L(\theta) = \sum_{i=1}^N \sum_{a \in \{0,1\}} \left(v_\theta^{(a)}(s_i) - y_i^{(a)}\right)^2\]</span></p></li>
<li><p>Check convergence: If <span class="math inline">\(\max_{i,a} |v_\theta^{(a)}(s_i) - y_i^{(a)}| &lt; \epsilon_{\text{tol}}\)</span>, stop</p></li>
</ol></li>
<li><p><strong>Output</strong>: The converged networks <span class="math inline">\(v_\theta^{(0)}\)</span> and <span class="math inline">\(v_\theta^{(1)}\)</span> approximate the optimal choice-specific value functions</p></li>
</ol>
<p><strong>Convergence</strong>: The value iteration algorithm is guaranteed to converge to the unique fixed point by the Contraction Mapping Theorem, since the Bellman operator is a contraction with modulus <span class="math inline">\(\delta &lt; 1\)</span>.</p>
</section>
<section id="pseudo-code" class="level3">
<h3 class="anchored" data-anchor-id="pseudo-code">Pseudo Code</h3>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Algorithm: Value Function Iteration with Neural Networks
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Design Principles:</strong></p>
<p>This pseudo code is designed for direct implementation:</p>
<ul>
<li><strong>No placeholders</strong>: Every operation corresponds to a concrete function or method call</li>
<li><strong>Explicit parameters</strong>: All inputs are passed as parameters, no implicit/global state</li>
<li><strong>Framework-specific</strong>: Uses PyTorch-style operations (optimizer.zero_grad(), backward(), step())</li>
<li><strong>No hard-coding</strong>: All values are parameterized</li>
</ul>
<p><strong>Input:</strong></p>
<ul>
<li>MDP parameters: <span class="math inline">\(\beta\)</span>: float, <span class="math inline">\(\gamma\)</span>: float, <span class="math inline">\(\delta\)</span>: float, <span class="math inline">\(\gamma_E\)</span>: float</li>
<li>Network hyperparameters: hyperparameters: dict (containing <code>hidden_sizes</code>: list[int])</li>
<li>State grid parameters: <span class="math inline">\(N\)</span>: int, state_range: tuple[float, float]</li>
<li>Training parameters: max_iter: int, <span class="math inline">\(\epsilon_{\text{tol}}\)</span>: float, num_epochs: int</li>
<li>Optimization: learning_rate: float, optimizer: Optimizer</li>
</ul>
<p><strong>Output:</strong></p>
<ul>
<li>Trained networks <span class="math inline">\(v_\theta^{(0)}\)</span>: Network and <span class="math inline">\(v_\theta^{(1)}\)</span>: Network</li>
</ul>
<hr>
<section id="main-algorithm" class="level3">
<h3 class="anchored" data-anchor-id="main-algorithm">Main Algorithm</h3>
<p><strong>Procedure</strong> <code>SolveValueIteration</code>(<span class="math inline">\(\beta\)</span>: float, <span class="math inline">\(\gamma\)</span>: float, <span class="math inline">\(\delta\)</span>: float, <span class="math inline">\(\gamma_E\)</span>: float, hyperparameters: dict, <span class="math inline">\(N\)</span>: int, state_range: tuple[float, float], max_iter: int, <span class="math inline">\(\epsilon_{\text{tol}}\)</span>: float, num_epochs: int, learning_rate: float, optimizer: Optimizer) <span class="math inline">\(\to\)</span> (Network, Network)</p>
<ol type="1">
<li><p><span class="math inline">\(v_\theta^{(0)}\)</span>: Network, <span class="math inline">\(v_\theta^{(1)}\)</span>: Network <span class="math inline">\(\leftarrow\)</span> <code>InitializeNetworks</code>(hyperparameters)</p></li>
<li><p><span class="math inline">\(S\)</span>: Tensor[<span class="math inline">\(N \times 1\)</span>] <span class="math inline">\(\leftarrow\)</span> <code>GenerateStateGrid</code>(<span class="math inline">\(N\)</span>, state_range)</p></li>
<li><p>For <code>iteration</code>: int = 1 to max_iter:</p>
<ol type="a">
<li><p><span class="math inline">\(\{y_i^{(a)}\}\)</span>: Tensor[<span class="math inline">\(N \times 2\)</span>] <span class="math inline">\(\leftarrow\)</span> <code>ComputeBellmanTargets</code>(<span class="math inline">\(S\)</span>, <span class="math inline">\(v_\theta^{(0)}\)</span>, <span class="math inline">\(v_\theta^{(1)}\)</span>, <span class="math inline">\(\beta\)</span>, <span class="math inline">\(\gamma\)</span>, <span class="math inline">\(\delta\)</span>, <span class="math inline">\(\gamma_E\)</span>)</p></li>
<li><p><span class="math inline">\(v_\theta^{(0)}\)</span>, <span class="math inline">\(v_\theta^{(1)}\)</span> <span class="math inline">\(\leftarrow\)</span> <code>UpdateNetworks</code>(<span class="math inline">\(S\)</span>, <span class="math inline">\(\{y_i^{(a)}\}\)</span>, <span class="math inline">\(v_\theta^{(0)}\)</span>, <span class="math inline">\(v_\theta^{(1)}\)</span>, num_epochs, optimizer)</p></li>
<li><p>max_error: float <span class="math inline">\(\leftarrow\)</span> <code>CheckConvergence</code>(<span class="math inline">\(S\)</span>, <span class="math inline">\(\{y_i^{(a)}\}\)</span>, <span class="math inline">\(v_\theta^{(0)}\)</span>, <span class="math inline">\(v_\theta^{(1)}\)</span>)</p></li>
<li><p><strong>If</strong> max_error <span class="math inline">\(&lt; \epsilon_{\text{tol}}\)</span>: <strong>Break</strong></p></li>
</ol></li>
<li><p><strong>Return</strong> <span class="math inline">\(v_\theta^{(0)}\)</span>, <span class="math inline">\(v_\theta^{(1)}\)</span></p></li>
</ol>
<hr>
</section>
<section id="subroutines" class="level3">
<h3 class="anchored" data-anchor-id="subroutines">Subroutines</h3>
<p><strong>Procedure</strong> <code>InitializeNetworks</code>(hyperparameters: dict) <span class="math inline">\(\to\)</span> (Network, Network)</p>
<ul>
<li>Create networks <span class="math inline">\(v_\theta^{(0)}\)</span>: Network and <span class="math inline">\(v_\theta^{(1)}\)</span>: Network with monotonic weight constraints</li>
<li>Initialize parameters <span class="math inline">\(\theta\)</span> randomly</li>
<li><strong>Return</strong> <span class="math inline">\(v_\theta^{(0)}\)</span>, <span class="math inline">\(v_\theta^{(1)}\)</span></li>
</ul>
<hr>
<p><strong>Procedure</strong> <code>GenerateStateGrid</code>(<span class="math inline">\(N\)</span>: int, state_range: tuple[float, float]) <span class="math inline">\(\to\)</span> Tensor[<span class="math inline">\(N \times 1\)</span>]</p>
<ul>
<li>Create uniform grid: <span class="math inline">\(S\)</span>: Tensor[<span class="math inline">\(N \times 1\)</span>] = <span class="math inline">\(\{s_1, \ldots, s_N\}\)</span> over state_range</li>
<li><strong>Return</strong> <span class="math inline">\(S\)</span></li>
</ul>
<hr>
<p><strong>Procedure</strong> <code>ComputeBellmanTargets</code>(<span class="math inline">\(S\)</span>: Tensor[<span class="math inline">\(N \times 1\)</span>], <span class="math inline">\(v_\theta^{(0)}\)</span>: Network, <span class="math inline">\(v_\theta^{(1)}\)</span>: Network, <span class="math inline">\(\beta\)</span>: float, <span class="math inline">\(\gamma\)</span>: float, <span class="math inline">\(\delta\)</span>: float, <span class="math inline">\(\gamma_E\)</span>: float) <span class="math inline">\(\to\)</span> Tensor[<span class="math inline">\(N \times 2\)</span>]</p>
<p>For each <span class="math inline">\(s_i\)</span>: float <span class="math inline">\(\in S\)</span> and <span class="math inline">\(a\)</span>: int <span class="math inline">\(\in \{0, 1\}\)</span>:</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;<span class="math inline">\(s'_i\)</span>: Tensor[<span class="math inline">\(N \times 1\)</span>] <span class="math inline">\(\leftarrow\)</span> <code>ComputeNextState</code>(<span class="math inline">\(s_i\)</span>, <span class="math inline">\(a\)</span>, <span class="math inline">\(\gamma\)</span>)</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;<span class="math inline">\(\text{EV}_i\)</span>: Tensor[<span class="math inline">\(N \times 1\)</span>] <span class="math inline">\(\leftarrow\)</span> <code>ComputeExpectedValue</code>(<span class="math inline">\(s'_i\)</span>, <span class="math inline">\(v_\theta^{(0)}\)</span>, <span class="math inline">\(v_\theta^{(1)}\)</span>, <span class="math inline">\(\gamma_E\)</span>)</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;<span class="math inline">\(y_i^{(a)}\)</span>: Tensor[<span class="math inline">\(N \times 1\)</span>] <span class="math inline">\(\leftarrow\)</span> <code>ComputeMeanReward</code>(<span class="math inline">\(s_i\)</span>, <span class="math inline">\(a\)</span>, <span class="math inline">\(\beta\)</span>) <span class="math inline">\(+ \delta \cdot \text{EV}_i\)</span></p>
<p><strong>Return</strong> <span class="math inline">\(\{y_i^{(a)}\}\)</span>: Tensor[<span class="math inline">\(N \times 2\)</span>] for all <span class="math inline">\(i\)</span>, <span class="math inline">\(a\)</span></p>
<hr>
<p><strong>Procedure</strong> <code>ComputeNextState</code>(<span class="math inline">\(s\)</span>: Tensor[<span class="math inline">\(N \times 1\)</span>], <span class="math inline">\(a\)</span>: int, <span class="math inline">\(\gamma\)</span>: float) <span class="math inline">\(\to\)</span> Tensor[<span class="math inline">\(N \times 1\)</span>]</p>
<ul>
<li><strong>Return</strong> <span class="math inline">\((1 - \gamma) \cdot s + a\)</span></li>
</ul>
<hr>
<p><strong>Procedure</strong> <code>ComputeMeanReward</code>(<span class="math inline">\(s\)</span>: Tensor[<span class="math inline">\(N \times 1\)</span>], <span class="math inline">\(a\)</span>: int, <span class="math inline">\(\beta\)</span>: float) <span class="math inline">\(\to\)</span> Tensor[<span class="math inline">\(N \times 1\)</span>]</p>
<ul>
<li><strong>Return</strong> <span class="math inline">\(\beta \cdot \log(1 + s) - a\)</span></li>
</ul>
<hr>
<p><strong>Procedure</strong> <code>ComputeExpectedValue</code>(<span class="math inline">\(s'\)</span>: Tensor[<span class="math inline">\(N \times 1\)</span>], <span class="math inline">\(v_\theta^{(0)}\)</span>: Network, <span class="math inline">\(v_\theta^{(1)}\)</span>: Network, <span class="math inline">\(\gamma_E\)</span>: float) <span class="math inline">\(\to\)</span> Tensor[<span class="math inline">\(N \times 1\)</span>]</p>
<ul>
<li><span class="math inline">\(v_0\)</span>: Tensor[<span class="math inline">\(N \times 1\)</span>] <span class="math inline">\(\leftarrow v_\theta^{(0)}(s')\)</span></li>
<li><span class="math inline">\(v_1\)</span>: Tensor[<span class="math inline">\(N \times 1\)</span>] <span class="math inline">\(\leftarrow v_\theta^{(1)}(s')\)</span></li>
<li><span class="math inline">\(\text{EV}\)</span>: Tensor[<span class="math inline">\(N \times 1\)</span>] <span class="math inline">\(\leftarrow\)</span> <code>LogSumExp</code>(<span class="math inline">\(v_0\)</span>, <span class="math inline">\(v_1\)</span>) <span class="math inline">\(+ \gamma_E\)</span></li>
<li><strong>Return</strong> EV</li>
</ul>
<hr>
<p><strong>Procedure</strong> <code>LogSumExp</code>(<span class="math inline">\(v_0\)</span>: Tensor[<span class="math inline">\(N \times 1\)</span>], <span class="math inline">\(v_1\)</span>: Tensor[<span class="math inline">\(N \times 1\)</span>]) <span class="math inline">\(\to\)</span> Tensor[<span class="math inline">\(N \times 1\)</span>]</p>
<ul>
<li><span class="math inline">\(\text{max}_v\)</span>: Tensor[<span class="math inline">\(N \times 1\)</span>] <span class="math inline">\(\leftarrow \max(v_0, v_1)\)</span></li>
<li><strong>Return</strong> <span class="math inline">\(\text{max}_v + \log(\exp(v_0 - \text{max}_v) + \exp(v_1 - \text{max}_v))\)</span></li>
</ul>
<hr>
<p><strong>Procedure</strong> <code>UpdateNetworks</code>(<span class="math inline">\(S\)</span>: Tensor[<span class="math inline">\(N \times 1\)</span>], <span class="math inline">\(\{y_i^{(a)}\}\)</span>: Tensor[<span class="math inline">\(N \times 2\)</span>], <span class="math inline">\(v_\theta^{(0)}\)</span>: Network, <span class="math inline">\(v_\theta^{(1)}\)</span>: Network, num_epochs: int, optimizer: Optimizer) <span class="math inline">\(\to\)</span> (Network, Network)</p>
<p>For <code>epoch</code>: int = 1 to num_epochs:</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;optimizer.zero_grad() &nbsp;&nbsp;&nbsp;&nbsp;// Clear previous gradients</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;<span class="math inline">\(L\)</span>: float <span class="math inline">\(\leftarrow\)</span> <code>ComputeLoss</code>(<span class="math inline">\(S\)</span>, <span class="math inline">\(\{y_i^{(a)}\}\)</span>, <span class="math inline">\(v_\theta^{(0)}\)</span>, <span class="math inline">\(v_\theta^{(1)}\)</span>)</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;<span class="math inline">\(L\)</span>.backward() &nbsp;&nbsp;&nbsp;&nbsp;// Compute gradients via automatic differentiation</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;optimizer.step() &nbsp;&nbsp;&nbsp;&nbsp;// Update parameters using computed gradients</p>
<p><strong>Return</strong> <span class="math inline">\(v_\theta^{(0)}\)</span>, <span class="math inline">\(v_\theta^{(1)}\)</span></p>
<hr>
<p><strong>Procedure</strong> <code>ComputeLoss</code>(<span class="math inline">\(S\)</span>: Tensor[<span class="math inline">\(N \times 1\)</span>], <span class="math inline">\(\{y_i^{(a)}\}\)</span>: Tensor[<span class="math inline">\(N \times 2\)</span>], <span class="math inline">\(v_\theta^{(0)}\)</span>: Network, <span class="math inline">\(v_\theta^{(1)}\)</span>: Network) <span class="math inline">\(\to\)</span> float</p>
<ul>
<li><span class="math inline">\(L\)</span>: float <span class="math inline">\(\leftarrow \sum_{i=1}^N \sum_{a \in \{0,1\}} (v_\theta^{(a)}(s_i) - y_i^{(a)})^2\)</span></li>
<li><strong>Return</strong> <span class="math inline">\(L\)</span></li>
</ul>
<hr>
<p><strong>Procedure</strong> <code>CheckConvergence</code>(<span class="math inline">\(S\)</span>: Tensor[<span class="math inline">\(N \times 1\)</span>], <span class="math inline">\(\{y_i^{(a)}\}\)</span>: Tensor[<span class="math inline">\(N \times 2\)</span>], <span class="math inline">\(v_\theta^{(0)}\)</span>: Network, <span class="math inline">\(v_\theta^{(1)}\)</span>: Network) <span class="math inline">\(\to\)</span> float</p>
<ul>
<li>max_error: float <span class="math inline">\(\leftarrow \max_{i,a} |v_\theta^{(a)}(s_i) - y_i^{(a)}|\)</span></li>
<li><strong>Return</strong> max_error</li>
</ul>
<hr>
<p><strong>Procedure</strong> <code>ComputeChoiceProbability</code>(<span class="math inline">\(s\)</span>: float, <span class="math inline">\(v_\theta^{(0)}\)</span>: Network, <span class="math inline">\(v_\theta^{(1)}\)</span>: Network) <span class="math inline">\(\to\)</span> (float, float)</p>
<ul>
<li><span class="math inline">\(v_0\)</span>: float <span class="math inline">\(\leftarrow v_\theta^{(0)}(s)\)</span></li>
<li><span class="math inline">\(v_1\)</span>: float <span class="math inline">\(\leftarrow v_\theta^{(1)}(s)\)</span></li>
<li><span class="math inline">\(\text{denom}\)</span>: float <span class="math inline">\(\leftarrow \exp(v_0) + \exp(v_1)\)</span></li>
<li><span class="math inline">\(P(a=0|s)\)</span>: float <span class="math inline">\(\leftarrow \exp(v_0) / \text{denom}\)</span></li>
<li><span class="math inline">\(P(a=1|s)\)</span>: float <span class="math inline">\(\leftarrow \exp(v_1) / \text{denom}\)</span></li>
<li><strong>Return</strong> <span class="math inline">\(P(a=0|s)\)</span>, <span class="math inline">\(P(a=1|s)\)</span></li>
</ul>
<p><strong>Note</strong>: This implements the logit formula for choice probabilities under Type-I Extreme Value distributed shocks: <span class="math display">\[P(a | s) = \frac{\exp(v(s, a))}{\sum_{a' \in \{0,1\}} \exp(v(s, a'))}\]</span></p>
</section>
</div>
</div>
<p><strong>Key Implementation Details:</strong></p>
<ul>
<li><strong>Monotonic network constraint</strong>: Apply softplus transformation to weight matrices during forward pass</li>
<li><strong>Numerical stability</strong>: Use log-sum-exp trick: <span class="math inline">\(\log(\exp(a) + \exp(b)) = \max(a,b) + \log(1 + \exp(-|a-b|))\)</span></li>
<li><strong>State sampling</strong>: Use uniform grid over state_range parameter</li>
<li><strong>Optimizer</strong>: Use adaptive optimizers such as Adam for faster convergence</li>
<li><strong>Batch updates</strong>: Process all states in parallel for efficiency</li>
</ul>
<div id="f67cd540" class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sys</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>sys.path.insert(<span class="dv">0</span>, <span class="st">'..'</span>)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>sys.path.insert(<span class="dv">0</span>, <span class="st">'../../src'</span>)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> concurrent.futures <span class="im">import</span> ThreadPoolExecutor</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> json</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pathlib <span class="im">import</span> Path</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> mdp_solver <span class="im">import</span> SolveValueIteration, ComputeChoiceProbability</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> config_mdp <span class="im">import</span> get_solver_config, get_comparative_statics</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> mdp_utils <span class="im">import</span> (</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    plot_convergence_history,</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>    plot_choice_value_functions,</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>    plot_policy_probabilities,</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>    plot_comparative_value_functions,</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>    plot_comparative_policies,</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="84980d80" class="cell" data-cache="true">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>solver_config <span class="op">=</span> get_solver_config()</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>beta <span class="op">=</span> solver_config[<span class="st">'beta'</span>]</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>gamma <span class="op">=</span> solver_config[<span class="st">'gamma'</span>]</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>delta <span class="op">=</span> solver_config[<span class="st">'delta'</span>]</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>gamma_E <span class="op">=</span> solver_config[<span class="st">'gamma_E'</span>]</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>hyperparameters <span class="op">=</span> solver_config[<span class="st">'hyperparameters'</span>]</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>N <span class="op">=</span> solver_config[<span class="st">'N'</span>]</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>state_range <span class="op">=</span> <span class="bu">tuple</span>(solver_config[<span class="st">'state_range'</span>])</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>max_iter <span class="op">=</span> solver_config[<span class="st">'max_iter'</span>]</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>epsilon_tol <span class="op">=</span> solver_config[<span class="st">'epsilon_tol'</span>]</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>num_epochs <span class="op">=</span> solver_config[<span class="st">'num_epochs'</span>]</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> solver_config[<span class="st">'learning_rate'</span>]</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Solve the MDP using value iteration</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Solving MDP with value iteration..."</span>)</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>v_theta_0, v_theta_1, history <span class="op">=</span> SolveValueIteration(</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>    beta<span class="op">=</span>beta,</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>    gamma<span class="op">=</span>gamma,</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>    delta<span class="op">=</span>delta,</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>    gamma_E<span class="op">=</span>gamma_E,</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>    hyperparameters<span class="op">=</span>hyperparameters,</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>    N<span class="op">=</span>N,</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>    state_range<span class="op">=</span>state_range,</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>    max_iter<span class="op">=</span>max_iter,</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>    epsilon_tol<span class="op">=</span>epsilon_tol,</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>    num_epochs<span class="op">=</span>num_epochs,</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>    learning_rate<span class="op">=</span>learning_rate,</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>    verbose<span class="op">=</span><span class="va">True</span></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Converged after </span><span class="sc">{</span><span class="bu">len</span>(history[<span class="st">'iterations'</span>])<span class="sc">}</span><span class="ss"> iterations"</span>)</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Final max error: </span><span class="sc">{</span>history[<span class="st">'max_errors'</span>][<span class="op">-</span><span class="dv">1</span>]<span class="sc">:.6f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="results" class="level2">
<h2 class="anchored" data-anchor-id="results">Results</h2>
<section id="convergence-plot" class="level3">
<h3 class="anchored" data-anchor-id="convergence-plot">Convergence Plot</h3>
<div id="86b74ff9" class="cell" data-cache="true">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot convergence history using shared utility</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> plot_convergence_history(</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    iterations<span class="op">=</span>history[<span class="st">'iterations'</span>],</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    max_errors<span class="op">=</span>history[<span class="st">'max_errors'</span>],</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    epsilon_tol<span class="op">=</span>epsilon_tol,</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>ax.figure.tight_layout()</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Converged in </span><span class="sc">{</span><span class="bu">len</span>(history[<span class="st">'iterations'</span>])<span class="sc">}</span><span class="ss"> iterations"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="visualization" class="level2">
<h2 class="anchored" data-anchor-id="visualization">Visualization</h2>
<section id="choice-specific-value-functions" class="level3">
<h3 class="anchored" data-anchor-id="choice-specific-value-functions">Choice-Specific Value Functions</h3>
<div id="53ca3cd0" class="cell" data-cache="true">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate dense grid for visualization</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>s_plot <span class="op">=</span> torch.linspace(state_range[<span class="dv">0</span>], state_range[<span class="dv">1</span>], <span class="dv">200</span>).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute value functions for both actions</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    v0_values <span class="op">=</span> v_theta_0(s_plot).numpy().flatten()</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    v1_values <span class="op">=</span> v_theta_1(s_plot).numpy().flatten()</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>s_plot_np <span class="op">=</span> s_plot.numpy().flatten()</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot value functions with shared utility</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> plot_choice_value_functions(</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>    state_grid<span class="op">=</span>s_plot_np,</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>    v0_values<span class="op">=</span>v0_values,</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>    v1_values<span class="op">=</span>v1_values,</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>ax.figure.tight_layout()</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Print some statistics</span></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Value Function Statistics:"</span>)</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"v(s, a=0): min=</span><span class="sc">{</span>v0_values<span class="sc">.</span><span class="bu">min</span>()<span class="sc">:.3f}</span><span class="ss">, max=</span><span class="sc">{</span>v0_values<span class="sc">.</span><span class="bu">max</span>()<span class="sc">:.3f}</span><span class="ss">, mean=</span><span class="sc">{</span>v0_values<span class="sc">.</span>mean()<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"v(s, a=1): min=</span><span class="sc">{</span>v1_values<span class="sc">.</span><span class="bu">min</span>()<span class="sc">:.3f}</span><span class="ss">, max=</span><span class="sc">{</span>v1_values<span class="sc">.</span><span class="bu">max</span>()<span class="sc">:.3f}</span><span class="ss">, mean=</span><span class="sc">{</span>v1_values<span class="sc">.</span>mean()<span class="sc">:.3f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="optimal-policy" class="level3">
<h3 class="anchored" data-anchor-id="optimal-policy">Optimal Policy</h3>
<div id="98189dee" class="cell" data-cache="true">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute optimal policy probabilities</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>prob_a0 <span class="op">=</span> []</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>prob_a1 <span class="op">=</span> []</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> s_val <span class="kw">in</span> s_plot_np:</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    p0, p1 <span class="op">=</span> ComputeChoiceProbability(s<span class="op">=</span><span class="bu">float</span>(s_val), v_theta_0<span class="op">=</span>v_theta_0, v_theta_1<span class="op">=</span>v_theta_1)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    prob_a0.append(p0)</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    prob_a1.append(p1)</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>prob_a0 <span class="op">=</span> np.array(prob_a0)</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>prob_a1 <span class="op">=</span> np.array(prob_a1)</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot optimal policy with shared utility</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> plot_policy_probabilities(</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>    state_grid<span class="op">=</span>s_plot_np,</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>    prob_a0<span class="op">=</span>prob_a0,</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>    prob_a1<span class="op">=</span>prob_a1,</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>ax.figure.tight_layout()</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Find approximate threshold where policy switches</span></span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>diff <span class="op">=</span> np.<span class="bu">abs</span>(prob_a0 <span class="op">-</span> <span class="fl">0.5</span>)</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>threshold_idx <span class="op">=</span> np.argmin(diff)</span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>threshold_s <span class="op">=</span> s_plot_np[threshold_idx]</span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Policy switches near s ≈ </span><span class="sc">{</span>threshold_s<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"At this state: P(a=0) = </span><span class="sc">{</span>prob_a0[threshold_idx]<span class="sc">:.3f}</span><span class="ss">, P(a=1) = </span><span class="sc">{</span>prob_a1[threshold_idx]<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Persist trained policy artifacts for downstream simulation</span></span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>output_dir <span class="op">=</span> Path(<span class="st">'../../output/sovle_mdp'</span>)</span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a>output_dir.mkdir(parents<span class="op">=</span><span class="va">True</span>, exist_ok<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a>statics <span class="op">=</span> get_comparative_statics()</span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a>config_payload <span class="op">=</span> solver_config.copy()</span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a>config_payload[<span class="st">'state_range'</span>] <span class="op">=</span> <span class="bu">list</span>(state_range)</span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a>config_payload[<span class="st">'hyperparameters'</span>] <span class="op">=</span> hyperparameters</span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a>config_payload[<span class="st">'comparative_statics'</span>] <span class="op">=</span> statics</span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(output_dir <span class="op">/</span> <span class="st">'config.json'</span>, <span class="st">'w'</span>, encoding<span class="op">=</span><span class="st">'utf-8'</span>) <span class="im">as</span> config_file:</span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true" tabindex="-1"></a>    json.dump(config_payload, config_file, indent<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb5-43"><a href="#cb5-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-44"><a href="#cb5-44" aria-hidden="true" tabindex="-1"></a>torch.save(v_theta_0.state_dict(), output_dir <span class="op">/</span> <span class="st">'v_theta_0.pt'</span>)</span>
<span id="cb5-45"><a href="#cb5-45" aria-hidden="true" tabindex="-1"></a>torch.save(v_theta_1.state_dict(), output_dir <span class="op">/</span> <span class="st">'v_theta_1.pt'</span>)</span>
<span id="cb5-46"><a href="#cb5-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-47"><a href="#cb5-47" aria-hidden="true" tabindex="-1"></a>np.savez(</span>
<span id="cb5-48"><a href="#cb5-48" aria-hidden="true" tabindex="-1"></a>    output_dir <span class="op">/</span> <span class="st">'policy_data.npz'</span>,</span>
<span id="cb5-49"><a href="#cb5-49" aria-hidden="true" tabindex="-1"></a>    state_grid<span class="op">=</span>s_plot_np,</span>
<span id="cb5-50"><a href="#cb5-50" aria-hidden="true" tabindex="-1"></a>    v0<span class="op">=</span>v0_values,</span>
<span id="cb5-51"><a href="#cb5-51" aria-hidden="true" tabindex="-1"></a>    v1<span class="op">=</span>v1_values,</span>
<span id="cb5-52"><a href="#cb5-52" aria-hidden="true" tabindex="-1"></a>    prob_a0<span class="op">=</span>prob_a0,</span>
<span id="cb5-53"><a href="#cb5-53" aria-hidden="true" tabindex="-1"></a>    prob_a1<span class="op">=</span>prob_a1,</span>
<span id="cb5-54"><a href="#cb5-54" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb5-55"><a href="#cb5-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-56"><a href="#cb5-56" aria-hidden="true" tabindex="-1"></a>np.savez(</span>
<span id="cb5-57"><a href="#cb5-57" aria-hidden="true" tabindex="-1"></a>    output_dir <span class="op">/</span> <span class="st">'convergence_history.npz'</span>,</span>
<span id="cb5-58"><a href="#cb5-58" aria-hidden="true" tabindex="-1"></a>    iterations<span class="op">=</span>np.asarray(history[<span class="st">'iterations'</span>], dtype<span class="op">=</span>np.int64),</span>
<span id="cb5-59"><a href="#cb5-59" aria-hidden="true" tabindex="-1"></a>    max_errors<span class="op">=</span>np.asarray(history[<span class="st">'max_errors'</span>], dtype<span class="op">=</span>np.float64),</span>
<span id="cb5-60"><a href="#cb5-60" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb5-61"><a href="#cb5-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-62"><a href="#cb5-62" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Saved trained artifacts to </span><span class="sc">{</span>output_dir<span class="sc">.</span>resolve()<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="comparative-statics" class="level2">
<h2 class="anchored" data-anchor-id="comparative-statics">Comparative Statics</h2>
<p>We examine how the optimal policy and value functions change with the reward parameter <span class="math inline">\(\beta\)</span>, which controls the weight on the logarithmic state value.</p>
<div id="df849264" class="cell" data-cache="true">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define beta values to test</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>statics <span class="op">=</span> get_comparative_statics()</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>beta_values <span class="op">=</span> np.array(statics[<span class="st">'beta_values'</span>])</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> solve_for_beta(beta_val):</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Solve MDP for a given beta value."""</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>    v0, v1, hist <span class="op">=</span> SolveValueIteration(</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>        beta<span class="op">=</span>beta_val,</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>        gamma<span class="op">=</span>gamma,</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>        delta<span class="op">=</span>delta,</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>        gamma_E<span class="op">=</span>gamma_E,</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>        hyperparameters<span class="op">=</span>hyperparameters,</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>        N<span class="op">=</span>N,</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>        state_range<span class="op">=</span>state_range,</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>        max_iter<span class="op">=</span>max_iter,</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>        epsilon_tol<span class="op">=</span>epsilon_tol,</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>        num_epochs<span class="op">=</span>num_epochs,</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>        learning_rate<span class="op">=</span>learning_rate,</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>        verbose<span class="op">=</span><span class="va">False</span></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute value functions on plot grid</span></span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>        v0_vals <span class="op">=</span> v0(s_plot).numpy().flatten()</span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>        v1_vals <span class="op">=</span> v1(s_plot).numpy().flatten()</span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute policy probabilities</span></span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>    policy_probs <span class="op">=</span> []</span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> s_val <span class="kw">in</span> s_plot_np:</span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a>        p0, p1 <span class="op">=</span> ComputeChoiceProbability(s<span class="op">=</span><span class="bu">float</span>(s_val), v_theta_0<span class="op">=</span>v0, v_theta_1<span class="op">=</span>v1)</span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a>        policy_probs.append((p0, p1))</span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a>    policy_probs <span class="op">=</span> np.array(policy_probs)</span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> {</span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a>        <span class="st">'beta'</span>: beta_val,</span>
<span id="cb6-37"><a href="#cb6-37" aria-hidden="true" tabindex="-1"></a>        <span class="st">'v0'</span>: v0_vals,</span>
<span id="cb6-38"><a href="#cb6-38" aria-hidden="true" tabindex="-1"></a>        <span class="st">'v1'</span>: v1_vals,</span>
<span id="cb6-39"><a href="#cb6-39" aria-hidden="true" tabindex="-1"></a>        <span class="st">'prob_a0'</span>: policy_probs[:, <span class="dv">0</span>],</span>
<span id="cb6-40"><a href="#cb6-40" aria-hidden="true" tabindex="-1"></a>        <span class="st">'prob_a1'</span>: policy_probs[:, <span class="dv">1</span>],</span>
<span id="cb6-41"><a href="#cb6-41" aria-hidden="true" tabindex="-1"></a>        <span class="st">'iterations'</span>: <span class="bu">len</span>(hist[<span class="st">'iterations'</span>])</span>
<span id="cb6-42"><a href="#cb6-42" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb6-43"><a href="#cb6-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-44"><a href="#cb6-44" aria-hidden="true" tabindex="-1"></a><span class="co"># Solve in parallel using threads</span></span>
<span id="cb6-45"><a href="#cb6-45" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Solving MDP for </span><span class="sc">{</span><span class="bu">len</span>(beta_values)<span class="sc">}</span><span class="ss"> different beta values in parallel..."</span>)</span>
<span id="cb6-46"><a href="#cb6-46" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> ThreadPoolExecutor(max_workers<span class="op">=</span>os.cpu_count()) <span class="im">as</span> executor:</span>
<span id="cb6-47"><a href="#cb6-47" aria-hidden="true" tabindex="-1"></a>    results <span class="op">=</span> <span class="bu">list</span>(executor.<span class="bu">map</span>(solve_for_beta, beta_values))</span>
<span id="cb6-48"><a href="#cb6-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-49"><a href="#cb6-49" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Completed all </span><span class="sc">{</span><span class="bu">len</span>(results)<span class="sc">}</span><span class="ss"> solutions"</span>)</span>
<span id="cb6-50"><a href="#cb6-50" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> r <span class="kw">in</span> results:</span>
<span id="cb6-51"><a href="#cb6-51" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  β = </span><span class="sc">{</span>r[<span class="st">'beta'</span>]<span class="sc">:.2f}</span><span class="ss">: converged in </span><span class="sc">{</span>r[<span class="st">'iterations'</span>]<span class="sc">}</span><span class="ss"> iterations"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="value-functions-across-β" class="level3">
<h3 class="anchored" data-anchor-id="value-functions-across-β">Value Functions Across β</h3>
<div id="e08effcd" class="cell" data-cache="true">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>fig, _ <span class="op">=</span> plot_comparative_value_functions(</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    state_grid<span class="op">=</span>s_plot_np,</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    results<span class="op">=</span>results,</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    parameter_key<span class="op">=</span><span class="st">'beta'</span>,</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="optimal-policy-across-β" class="level3">
<h3 class="anchored" data-anchor-id="optimal-policy-across-β">Optimal Policy Across β</h3>
<div id="055923ae" class="cell" data-cache="true">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>fig, _ <span class="op">=</span> plot_comparative_policies(</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    state_grid<span class="op">=</span>s_plot_np,</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>    results<span class="op">=</span>results,</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>    parameter_key<span class="op">=</span><span class="st">'beta'</span>,</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="comparative-statics-for-γ" class="level3">
<h3 class="anchored" data-anchor-id="comparative-statics-for-γ">Comparative Statics for γ</h3>
<p>We examine how the optimal policy and value functions change with the persistence parameter <span class="math inline">\(\gamma\)</span>, which controls the state transition dynamics.</p>
<div id="804e6f2f" class="cell" data-cache="true">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define gamma values to test</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>gamma_values <span class="op">=</span> np.array(statics[<span class="st">'gamma_values'</span>])</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> solve_for_gamma(gamma_val):</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Solve MDP for a given gamma value."""</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>    v0, v1, hist <span class="op">=</span> SolveValueIteration(</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>        beta<span class="op">=</span>beta,</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>        gamma<span class="op">=</span>gamma_val,</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>        delta<span class="op">=</span>delta,</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>        gamma_E<span class="op">=</span>gamma_E,</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>        hyperparameters<span class="op">=</span>hyperparameters,</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>        N<span class="op">=</span>N,</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>        state_range<span class="op">=</span>state_range,</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>        max_iter<span class="op">=</span>max_iter,</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>        epsilon_tol<span class="op">=</span>epsilon_tol,</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>        num_epochs<span class="op">=</span>num_epochs,</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>        learning_rate<span class="op">=</span>learning_rate,</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>        verbose<span class="op">=</span><span class="va">False</span></span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute value functions on plot grid</span></span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>        v0_vals <span class="op">=</span> v0(s_plot).numpy().flatten()</span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>        v1_vals <span class="op">=</span> v1(s_plot).numpy().flatten()</span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute policy probabilities</span></span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a>    policy_probs <span class="op">=</span> []</span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> s_val <span class="kw">in</span> s_plot_np:</span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a>        p0, p1 <span class="op">=</span> ComputeChoiceProbability(s<span class="op">=</span><span class="bu">float</span>(s_val), v_theta_0<span class="op">=</span>v0, v_theta_1<span class="op">=</span>v1)</span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a>        policy_probs.append((p0, p1))</span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a>    policy_probs <span class="op">=</span> np.array(policy_probs)</span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> {</span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a>        <span class="st">'gamma'</span>: gamma_val,</span>
<span id="cb9-36"><a href="#cb9-36" aria-hidden="true" tabindex="-1"></a>        <span class="st">'v0'</span>: v0_vals,</span>
<span id="cb9-37"><a href="#cb9-37" aria-hidden="true" tabindex="-1"></a>        <span class="st">'v1'</span>: v1_vals,</span>
<span id="cb9-38"><a href="#cb9-38" aria-hidden="true" tabindex="-1"></a>        <span class="st">'prob_a0'</span>: policy_probs[:, <span class="dv">0</span>],</span>
<span id="cb9-39"><a href="#cb9-39" aria-hidden="true" tabindex="-1"></a>        <span class="st">'prob_a1'</span>: policy_probs[:, <span class="dv">1</span>],</span>
<span id="cb9-40"><a href="#cb9-40" aria-hidden="true" tabindex="-1"></a>        <span class="st">'iterations'</span>: <span class="bu">len</span>(hist[<span class="st">'iterations'</span>])</span>
<span id="cb9-41"><a href="#cb9-41" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb9-42"><a href="#cb9-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-43"><a href="#cb9-43" aria-hidden="true" tabindex="-1"></a><span class="co"># Solve in parallel using threads</span></span>
<span id="cb9-44"><a href="#cb9-44" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Solving MDP for </span><span class="sc">{</span><span class="bu">len</span>(gamma_values)<span class="sc">}</span><span class="ss"> different gamma values in parallel..."</span>)</span>
<span id="cb9-45"><a href="#cb9-45" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> ThreadPoolExecutor(max_workers<span class="op">=</span>os.cpu_count()) <span class="im">as</span> executor:</span>
<span id="cb9-46"><a href="#cb9-46" aria-hidden="true" tabindex="-1"></a>    gamma_results <span class="op">=</span> <span class="bu">list</span>(executor.<span class="bu">map</span>(solve_for_gamma, gamma_values))</span>
<span id="cb9-47"><a href="#cb9-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-48"><a href="#cb9-48" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Completed all </span><span class="sc">{</span><span class="bu">len</span>(gamma_results)<span class="sc">}</span><span class="ss"> solutions"</span>)</span>
<span id="cb9-49"><a href="#cb9-49" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> r <span class="kw">in</span> gamma_results:</span>
<span id="cb9-50"><a href="#cb9-50" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  γ = </span><span class="sc">{</span>r[<span class="st">'gamma'</span>]<span class="sc">:.2f}</span><span class="ss">: converged in </span><span class="sc">{</span>r[<span class="st">'iterations'</span>]<span class="sc">}</span><span class="ss"> iterations"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="value-functions-across-γ" class="level3">
<h3 class="anchored" data-anchor-id="value-functions-across-γ">Value Functions Across γ</h3>
<div id="e34c93b0" class="cell" data-cache="true">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>fig, _ <span class="op">=</span> plot_comparative_value_functions(</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>    state_grid<span class="op">=</span>s_plot_np,</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>    results<span class="op">=</span>gamma_results,</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>    parameter_key<span class="op">=</span><span class="st">'gamma'</span>,</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="optimal-policy-across-γ" class="level3">
<h3 class="anchored" data-anchor-id="optimal-policy-across-γ">Optimal Policy Across γ</h3>
<div id="8146ff54" class="cell" data-cache="true">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>fig, _ <span class="op">=</span> plot_comparative_policies(</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    state_grid<span class="op">=</span>s_plot_np,</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>    results<span class="op">=</span>gamma_results,</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    parameter_key<span class="op">=</span><span class="st">'gamma'</span>,</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<section id="interpretation-of-comparative-statics" class="level3">
<h3 class="anchored" data-anchor-id="interpretation-of-comparative-statics">Interpretation of Comparative Statics</h3>
<section id="effect-of-β-reward-weight-on-state" class="level4">
<h4 class="anchored" data-anchor-id="effect-of-β-reward-weight-on-state">Effect of β (Reward Weight on State)</h4>
<p>The parameter <span class="math inline">\(\beta\)</span> controls the weight on the logarithmic state value in the mean reward function <span class="math inline">\(r(s,a) = \beta \log(1+s) - a\)</span>. Our comparative statics reveal:</p>
<p><strong>Value Functions</strong>: As <span class="math inline">\(\beta\)</span> increases from 0 to 2, both value functions <span class="math inline">\(v(s, a=0)\)</span> and <span class="math inline">\(v(s, a=1)\)</span> increase substantially. This is intuitive - higher <span class="math inline">\(\beta\)</span> means greater returns from accumulating state, making both the patient action (<span class="math inline">\(a=0\)</span>, which preserves more state) and the active action (<span class="math inline">\(a=1\)</span>, which depletes state but provides immediate consumption) more valuable in present value terms.</p>
<p><strong>Optimal Policy</strong>: The policy exhibits a clear state-dependent threshold structure. For low values of <span class="math inline">\(\beta\)</span> (close to 0), the agent is nearly indifferent between actions across all states since state accumulation provides little reward. As <span class="math inline">\(\beta\)</span> increases: - At low states, <span class="math inline">\(P(a=1|s)\)</span> increases (agent becomes more willing to take the costly action) - At high states, <span class="math inline">\(P(a=0|s)\)</span> increases (agent preserves the valuable state) - The threshold state where the policy switches shifts rightward</p>
<p>This pattern reflects that when investment is more valuable (<span class="math inline">\(\beta\)</span> high), it becomes worthwhile to take the costly action <span class="math inline">\(a=1\)</span> to reach valuable states, but once state improves, the agent optimally preserves that state by choosing <span class="math inline">\(a=0\)</span> because the martinal return of invenstment mitigates.</p>
</section>
<section id="effect-of-γ-state-persistence" class="level4">
<h4 class="anchored" data-anchor-id="effect-of-γ-state-persistence">Effect of γ (State Persistence)</h4>
<p>The parameter <span class="math inline">\(\gamma\)</span> controls state persistence in the transition equation <span class="math inline">\(s' = (1-\gamma)s + a\)</span>. Our comparative statics show:</p>
<p><strong>Value Functions</strong>: As <span class="math inline">\(\gamma\)</span> increases from 0 to 1, the value functions decrease significantly. This occurs because higher <span class="math inline">\(\gamma\)</span> means faster state decay - the state depreciates more quickly, making it harder to maintain high state values over time. With faster depreciation, the future returns from state accumulation diminish, reducing the present value of both action choices.</p>
<p><strong>Optimal Policy</strong>: The effect of <span class="math inline">\(\gamma\)</span> on policy is nuanced: - For <span class="math inline">\(\gamma = 0\)</span> (no depreciation), states persist indefinitely, so the agent can afford to be patient and primarily chooses <span class="math inline">\(a=0\)</span> to preserve state - As <span class="math inline">\(\gamma\)</span> increases (faster depreciation), the relative value of taking action <span class="math inline">\(a=1\)</span> increases at low and medium states, since preserving state becomes less valuable when it depreciates quickly - At very high <span class="math inline">\(\gamma\)</span> (close to 1), state depreciation is so rapid that the agent must constantly choose <span class="math inline">\(a=1\)</span> to replenish the state, fundamentally changing the nature of the optimal policy</p>
</section>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>