# AIをRAとして活用する：モデルの均衡計算の実装事例

## はじめに

前回の記事では、因果推論の実装をAIに依頼する「口頭試問方式」を紹介しました。今回は、経済学研究におけるもう一つの重要なタスク——**モデルの均衡計算**——にこのアプローチを適用します。

具体的には、**個人のマルコフ意思決定過程（Markov Decision Process; MDP）の最適方策の導出**をAIに実装させます。これは強化学習や動的最適化の基本問題であり、経済学では資産蓄積、労働供給、企業の投資決定など多様な応用があります。

ここで重要なのは、この基本的な枠組みを理解すれば、より複雑な問題——ゲーム理論的な戦略的相互作用、不完備情報下での意思決定、異質なエージェント間の均衡——にも同じ方法論で対処できるということです。コンセプチュアルには同じプロセス：モデルの数理的定式化、解法アルゴリズムの設計、実装、検証を経ることになります。

## 「数式」をSingle Source of Truthとする

前回の記事で強調した口頭試問方式を、今回はさらに進化させます。単にAIの理解を確認するだけでなく、**モデルの設定と解法を数式で明示的に記述してから、それをsingle source of truth（唯一の真実の源）として、すべての実装を進めていく**という方法論です。

具体的な手順：

1. **数理モデルの明確な定式化**：状態空間、行動空間、報酬関数、遷移確率、割引因子をすべて数式で記述
2. **解法アルゴリズムの疑似コードを作成**：Bellman方程式、価値反復法、ニューラルネットワーク近似を手続き型の疑似コードで表現
3. **疑似コードを実装の仕様書とする**：関数名、引数、型、処理手順をすべて疑似コードから導出
4. **単体テストを疑似コードから生成**：各サブルーチンの入出力仕様を疑似コードから抽出してテストケースを作成
5. **実装→テスト→文書化の順守**：疑似コードという共通言語があるため、齟齬が生じない

この方法の利点は、**数式と疑似コードが変更管理の基準になる**ことです。報酬関数を β·s - a から β·log(1+s) - a に変更する場合、まず数式と疑似コードを更新し、その後実装とテストを更新します。これにより、文書と実装の乖離を防げます。

## マルコフ意思決定過程の実装

今回実装したのは、連続状態・離散行動のMDP問題です：

**問題設定**：
- 状態：s ∈ ℝ（連続）
- 行動：a ∈ {0, 1}（離散）
- 報酬関数：r(s,a) = β·log(1+s) - a（収穫逓減）
- 状態遷移：s' = (1-γ)s + a（減価と蓄積）
- 行動ショック：Type-I極値分布（ロジット型選択確率を導出するため）

**解法**：
- ニューラルネットワークによる価値関数近似（各行動a∈{0,1}に対してv(s,a)を学習）
- 単調性制約（softplus活性化関数と正の重みで∂v/∂s > 0を保証）
- 価値反復法（Bellman作用素の反復適用）
- 最適方策の導出（ロジット型の確率的方策）

## 疑似コードの設計原則

今回、AIに指示する上で特に重要だったのが、**疑似コードの品質管理**です。以下の原則を厳格に適用しました：

### 1. プレースホルダーの完全排除

AIが最初に生成した疑似コードには、`// Implementation details` や `// TODO: Compute targets` といった曖昧な記述がありました。これらをすべて排除し、**具体的な計算手順のみを記述**するよう指示しました。

悪い例：
```
Procedure ComputeBellmanTargets(...)
    // Compute next state
    // Compute reward
    // Implementation details
```

良い例：
```
Procedure ComputeBellmanTargets(s: Tensor[N×1], a: int, ...) -> Tensor[N×1]
    s_next ← ComputeNextState(s, a, gamma)
    r_mean ← ComputeMeanReward(s, a, beta)
    log_sum_exp ← log(Σ_{a'} exp(v_{a'}(s_next)))
    Return r_mean + delta * (log_sum_exp + gamma_E)
```

### 2. 型注釈の徹底

すべての変数と関数シグネチャに型注釈を追加しました。これにより、sが単一の状態値なのか、N個の状態のバッチなのかが明確になります：

```
Procedure ComputeMeanReward(
    s: Tensor[N×1],      # 状態のバッチ
    a: int,              # 単一の行動
    beta: float          # スカラーパラメータ
) -> Tensor[N×1]         # 報酬のバッチ
```

### 3. モジュール化と動詞ベースの命名

各手続きは単一の責任を持ち、動詞ベースで命名しました：
- `ComputeMeanReward` - 平均報酬の計算
- `ComputeNextState` - 状態遷移の計算
- `ComputeBellmanTargets` - Bellman目標値の計算
- `FitNeuralNetwork` - ニューラルネットワークの学習
- `SolveValueIteration` - 価値反復法の全体制御

この粒度により、各関数を独立してテストできます。

### 4. 名前付き引数の強制

パラメータの順序間違いを防ぐため、すべての関数呼び出しで名前付き引数を使用するルールを設定しました：

```python
# 悪い例
SolveValueIteration(0.5, 0.1, 0.95, gamma_E, ...)

# 良い例
SolveValueIteration(
    beta=0.5,
    gamma=0.1,
    delta=0.95,
    gamma_E=gamma_E,
    ...
)
```

これをCLAUDE.mdに明記し、AIがすべての呼び出しでこのルールに従うようにしました。

## 実装とテストの一対一対応

疑似コードの各手続きが、Pythonの関数とテストケースに一対一で対応します：

**疑似コード** → **実装** → **テスト**
- `ComputeMeanReward` → `src/mdp_solver/mdp_solver.py:ComputeMeanReward()` → `test/mdp_solver/test_subroutines.py:TestComputeMeanReward`
- `ComputeNextState` → `src/mdp_solver/mdp_solver.py:ComputeNextState()` → `test/mdp_solver/test_subroutines.py:TestComputeNextState`

テストケースは疑似コードの仕様から直接導出されます。例えば、`ComputeMeanReward`の仕様が「r(s,a) = β·log(1+s) - aを返す」なら：

```python
def test_formula(self):
    """Should compute r = beta*log(1+s) - a."""
    s = torch.tensor([[4.0]])
    a = 1
    beta = 0.5
    r = ComputeMeanReward(s=s, a=a, beta=beta)
    expected = beta * torch.log(1 + s) - a
    assert torch.allclose(r, expected)
```

最終的に**45個の単体テスト**がすべてパスし、各サブルーチンの正しさを保証しました。

## パラメータチューニングと関数形の変更

実装後、収束性能を改善するためパラメータを調整しました：
- β: 0.5 → 0.1 → 3.0 → 1.0（最終的に1.0でバランス）
- max_iter: デフォルト → 10000 → 1000（収束と計算時間のバランス）

重要なのは、報酬関数の関数形を変更した際のワークフローです：

1. **文書の更新**：β·s - a → β·log(1+s) - a（収穫逓減を表現）
2. **経済学的解釈の追加**：なぜ対数形を使うか（限界効用逓減）を説明
3. **Bellman方程式の更新**：新しい報酬関数を反映
4. **疑似コードの更新**：ComputeMeanRewardの仕様を変更
5. **実装の更新**：疑似コードに従ってPythonコードを修正
6. **テストの更新**：新しい関数形に基づいてテストケースを修正

ユーザーが明示的に「まだ実装を変更しないでください」と指示したことで、文書と実装の乖離を防げました。**段階的な変更管理**が重要です。

## 比較静学によるValidation

モデル実装の検証において、**比較静学（comparative statics）分析は不可欠**です。単体テストが個々の関数の正しさを保証するのに対し、比較静学はモデル全体の経済学的妥当性を検証します。

今回は2つのパラメータについて比較静学を実施しました：

### βの効果（報酬の重み）

β ∈ {0, 0.25, 0.5, ..., 2.0} の9値について並列計算を実施：

**観察された結果**：
- βが増加すると価値関数v(s,0), v(s,1)がともに大幅に増加
- 最適方策に閾値構造が出現：低い状態では a=1（蓄積）を選び、高い状態では a=0（維持）を選ぶ
- βが小さいときは行動間の差がほとんどなく、βが大きいと明確な状態依存方策が出現

**経済学的解釈**：
βは状態蓄積の価値を決定します。βが大きいとき、低い状態から高い状態に移行するコスト（a=1を選ぶコスト）を払う価値があります。高い状態に到達すると、その価値ある状態を維持する（a=0を選ぶ）ことが最適になります。

### γの効果（状態の減価率）

γ ∈ {0, 0.25, 0.5, 0.75, 1.0} の5値について並列計算を実施：

**観察された結果**：
- γが増加すると価値関数が大幅に減少（状態が速く減価するため）
- γ=0（減価なし）では維持行動（a=0）が支配的
- γが大きくなると補充行動（a=1）の相対的価値が増加
- γ→1では常に補充が必要になり、方策の性質が根本的に変化

**経済学的解釈**：
γは状態の持続性を決定します。減価が速い（γ大）と、状態を維持しても価値が失われるため、積極的な補充（a=1）が必要になります。βとγの相互作用が、「蓄積・維持」型の方策か「補充」型の方策かを決定します。

### 比較静学の実装：並列計算とキャッシング

計算効率のため、以下の工夫を実施：

1. **ThreadPoolExecutor による並列化**：複数のパラメータ値について同時にMDPを解く（注：ProcessPoolExecutorはWindows上のJupyterで問題があるためThreadPoolExecutorを使用）

2. **Quartoのキャッシング**：`#| cache: true` ディレクティブにより、コードブロックの計算結果を保存

3. **モジュールインポートの整理**：すべてのインポートを冒頭の1ブロックに集約

4. **freeze設定**：`execute: freeze: auto` により、プロジェクトレンダリング時に実行結果を再利用（ただし単一ファイルレンダリングでは常に実行される）

比較静学の結果が経済学的に妥当（価値関数の単調性、方策の閾値構造、パラメータ効果の符号）であることを確認して初めて、実装が正しいと判断できます。単体テストだけでは不十分です。

## 可視化による検証

比較静学の結果を効果的に可視化するため、カラーグラデーションを使用しました：

- **a=0の価値関数**：黒→青のグラデーション（パラメータ値の増加に応じて）
- **a=1の価値関数**：黒→赤のグラデーション
- 複数のパラメータ値の結果を1つのグラフに重ね描き

これにより、パラメータの連続的な変化が価値関数と方策に与える影響を直感的に理解できます。

## プロジェクト共有のための.gitignore設計

研究プロジェクトを他者と共有する際、どのファイルをgit追跡対象とすべきかは重要な判断です。今回の方針：

**追跡すべき**：
- ソースコード（.qmd, .py）
- 最終的なHTML レポート（solve_mdp.html）
- HTMLが参照する図（figure-html/ディレクトリ）

**追跡すべきでない**：
- Pythonキャッシュ（`__pycache__/`）
- Quarto中間ファイル（`*.quarto_ipynb`, `_freeze/`）
- 自動生成されるライブラリファイル（`*_files/libs/`）

この方針により、リポジトリをクローンした他の研究者は：
1. 計算コストの高いMDPソルバーを実行せずに結果を即座に閲覧できる
2. 必要なら自分でパラメータを変更して再実行できる
3. リポジトリサイズは適切に管理される

## 会話履歴の完全な記録

前回同様、AIとの会話履歴を完全に記録し、`docs/conversation/mdp_solver_conversation_transcript.md`として公開しています。

今回の会話履歴の特徴：

1. **11のフェーズに構造化**：問題定式化→疑似コード設計→環境構築→実装→テスト→Quarto文書化→パラメータチューニング→関数形変更→比較静学→解釈→共有

2. **gitコミット履歴との統合**：各会話段階で何がコミットされたかを明示

3. **Key Learningセクション**：各決定点での学習事項を注釈
   - なぜType-I極値分布を使うか（計算の tractability）
   - なぜsoftplus活性化を使うか（単調性の保証）
   - なぜ名前付き引数を強制するか（エラー防止）
   - なぜThreadPoolExecutorを使うか（WindowsのJupyter互換性）

4. **技術的根拠の文書化**：各設計選択の理由を明記

## まとめ

本実装事例は、前回の因果推論からさらに進んで、**モデルの均衡計算**をAIに実装させる方法論を示しました。

核心的な原則：

1. **数式をsingle source of truthとする**：モデル定式化→疑似コード→実装→テストの一貫性を保つ

2. **疑似コードの品質管理**：プレースホルダー排除、型注釈、モジュール化、動詞ベース命名

3. **段階的変更管理**：文書更新→疑似コード更新→実装更新→テスト更新の順序を守る

4. **比較静学によるvalidation**：単体テストだけでなく、経済学的に妥当な結果が得られるかを検証

5. **名前付き引数の強制**：パラメータの順序間違いを防ぐ

6. **会話履歴の記録と共有**：再現性と学習のために完全な履歴を公開

この方法論は、個人の意思決定問題だけでなく、より複雑な均衡問題——ゲーム理論、不完備情報、異質エージェント——にも適用可能です。コンセプチュアルには同じプロセスを踏むことで、AIを効果的なRAとして活用できます。

詳細な会話履歴、すべてのコード、レンダリングされたHTML レポートは[GitHubリポジトリ](https://github.com/kohei-kawaguchi/TestAI)で公開しています。
