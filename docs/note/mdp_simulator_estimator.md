# AIをRAとして活用する：パラメータ回復による最も包括的な検証

## はじめに

前回の記事では、マルコフ意思決定過程（MDP）の均衡計算をAIに実装させました。今回はその続編として、**シミュレーションと構造推定(逆強化学習)**を加えた完全なパイプラインを構築します。

重要なのは、**solve（解く）→ simulate（シミュレート）→ estimate（推定する）→ recover（回復する）**という一連の流れを完成させることです。真のパラメータでデータを生成し、それを推定し、元のパラメータを正確に回復できることを確認する。これは**コア機能の実装が正しいことの最も包括的な検証**になります。

## パラメータ回復は最も包括的な検証

### なぜパラメータ回復が重要か

単体テストは「この関数は正しく動くか」を検証します。比較静学は「パラメータを変えたとき結果が理論と整合するか」を検証します。しかし、これらだけでは不十分です。

パラメータ回復は、**システム全体のend-to-end検証**です。真のパラメータでモデルを解き、データを生成し、そのデータから元のパラメータを推定する——この一連の流れが正しく機能して初めて、パラメータが正確に回復されます。

**どこか1つでもバグがあれば、パラメータは回復されません。**

- ソルバーのバグ → 誤った均衡 → 誤ったデータ → 誤った推定値
- シミュレーターのバグ → モデルと整合しないデータ → バイアスのある推定値
- 推定器のバグ → 正しいデータからでも誤った推定値

つまり、パラメータ回復の成功は、**ソルバー、シミュレーター、推定器すべてが正しく実装され、整合的に動いている**ことの証明です。

### 一度では成功しない

重要な点は、**パラメータ回復は最初の試行では成功しない**ことです。

今回の実装でも、いくつかのバグを発見し修正しました。各コンポーネントを独立に検証してから統合する、という段階的アプローチが不可欠です。最終的に推定器の目的関数が真の値周りで最適化されていることを確認できた時、実装が正しいという確信を得ることができました。

## データ・設定・関数のパイプライン管理

複数のステップを連携させるパイプラインでは、**データ、設定、関数を一貫して管理する**仕組みが不可欠です。これは、パラメータ回復を成功させるための土台です。

### 設定をデータとして扱う

本プロジェクトの最も重要な設計原則は、**「設定をコードではなくデータとして扱う」**ことです。

従来のアプローチでは、パラメータをコード内にハードコードしていました。ソルバーで使ったパラメータを、シミュレーターでも、推定器でも、それぞれ書き直す——これでは、設定の不一致によるバグが頻発します。

本プロジェクトでは、以下のルールを徹底しました：

1. **すべてのパラメータを一箇所で定義**
   設定モジュールが唯一の真実の源（single source of truth）

2. **設定を結果と一緒に保存**
   ソルバーが設定をJSONファイルとして保存し、次のステップがそれを読み込む

3. **設定の整合性を自動検証**
   読み込んだ設定が現在のコードの設定と一致するかを確認。不一致があればエラー

この方法により、「シミュレーターが違うパラメータで動いていた」「推定器が間違った設定を使っていた」といったミスは原理的に発生しません。

### DRY原則：同じコードを二度書かない

**DRY（Don't Repeat Yourself）原則**も重要です。

例えば、報酬関数をソルバー、シミュレーター、推定器でそれぞれ実装すると、3箇所で同じコードが存在することになります。報酬関数を変更するとき、3箇所すべてを修正しなければならず、修正漏れがバグの原因になります。

本プロジェクトでは、**すべての関数を共有モジュールに集約**しました。報酬関数も、状態遷移も、可視化関数も、すべて1箇所にしか存在しません。変更は1箇所で済み、すべてのステップに自動的に反映されます。

この設計により、「ソルバーと推定器で報酬関数の定義が違っていた」というバグは原理的に発生しません。

### パイプラインの設計

データの流れは直線的です：

- **ソルバー** → 訓練済みモデル＋設定を保存
- **シミュレーター** → モデルと設定を読み込み、データ＋設定を保存
- **推定器** → データと設定を読み込み、パラメータを推定

各ステップは独立して実行可能で、前のステップの出力だけに依存します。この設計により、任意のステップを再実行でき、デバッグが容易になります。

## 診断的分析によるバグの切り分け

パラメータ回復が失敗したとき、**どこにバグがあるのかを特定する**ことが最大の課題です。

### 問題の切り分け

パラメータが回復されない原因は3つのどこかにあります：

1. **ソルバーのバグ** → 誤った均衡を計算している
2. **シミュレーターのバグ** → モデルと整合しないデータを生成している
3. **推定器のバグ** → 正しいデータからでも誤った推定値を出している

しかし、推定値を見ただけでは、どこに問題があるのか分かりません。

### 診断的分析の挿入

そこで、**診断的な分析をパイプラインに挿入**します。

最終的に確認すべきことは、**真のパラメータ周辺で尤度プロファイルを描く**ことです。真のパラメータでモデルを解き、その価値関数を使ってシミュレーションデータの尤度を計算します。

これがうまくいかない時は、ソルバーに問題があるのか、シミュレーターに問題があるのか、推定器に問題があるのかを特定する必要がありますが、推定器の実装に入る前にソルバーとシミュレーターの妥当性確認が終わっていれば推定器のデバッグに注力できます。それが済んでいないと、膨大な領域を手当たり次第に調査することになり、非効率です。

推定器のデバッグでは、データや設定をロードしたところから、アルゴリズムレベルの問題、実装レベルの問題を順番に見ていきます。このとき、意味のある各ルーチンを関数としてモデュール化しておくと、再利用性が高まり、デバッグが容易になります。そうなっていない場合、問題の切り分けや診断がうまくいかないので、まずはコードの整理から始める必要があります。

## まとめ：パラメータ回復による最も包括的な検証

本実装事例は、**solve → simulate → estimate → recover** という完全な計算パイプラインをAIに実装させる方法論を示しました。パラメータ回復は最初の試行では成功せず、バグの切り分けと修正を繰り返すことで、最終的に実装の正しさを確認できました。

### 核心的な原則

1. **パラメータ回復は最も包括的な検証**
   単体テストや比較静学だけでは不十分。真のパラメータでデータを生成し、推定し、パラメータを回復できて初めて、パイプライン全体が正しいと言える。

2. **設定をデータとして扱う**
   コードにハードコードせず、設定をJSONで保存し、各ステップで読み込んで検証する。設定の不一致を自動検出し、バグを防ぐ。これがパラメータ回復成功の土台。

3. **DRY原則の徹底**
   同じコードを二度書かない。報酬関数、状態遷移、可視化——すべて共有モジュールに集約。一箇所の変更がすべてのステップに反映され、実装の不整合を原理的に防ぐ。

4. **診断的分析によるバグの切り分け**
   パラメータ回復が失敗したとき、どこにバグがあるのかを特定する必要がある。関数をモジュール化し、診断的な分析を挿入し、真のパラメータで尤度プロファイルを描くことで、ソルバー、シミュレーター、推定器のどこに問題があるかを切り分ける。

### より複雑な問題への拡張

同じ方法論は、より複雑な問題にも適用できます。多次元パラメータ、異質性のあるエージェント、不完備情報、ゲーム理論的均衡——コンセプトは同じです。

solve → simulate → estimate → recoverのパイプラインを構築し、各ステップで設定と関数を共有し、最終的にパラメータ回復で検証する。この原則は、どのような計算経済学のプロジェクトにも適用できます。

### 実装の全公開

詳細な会話履歴、すべてのコード、レンダリングされたHTMLレポートは[GitHubリポジトリ](https://github.com/kohei-kawaguchi/TestAI)で公開しています。特に、[`docs/conversation/mdp_simulator_estimator_conversation_transcript.md`](../../conversation/mdp_simulator_estimator_conversation_transcript.md)には、会話履歴と各段階での判断基準が記録されています。
